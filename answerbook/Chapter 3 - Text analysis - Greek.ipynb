{
 "metadata": {
  "name": "",
  "signature": "sha256:9cfa9a99ca63fb344c9ea280c568fb154c99dd76b6fa4f5d437aae93ba30dc85"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Chapter 3: Text Analysis"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "-- *A Python Course for the Humanities by Folgert Karsdorp and Maarten van Gompel*"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In this chapter we will introduce you to the task of text analysis in Python. You will learn how to read an entire corpus into Python, clean it and how to perform certain data analyses on those texts. We will also briefly introduce you to using Python's plotting library *matplotlib*, with which you can visualize your data.\n",
      "\n",
      "Before we delve into the main subject of this chapter, text analysis, we will first write a couple of utility functions that build upon the things you learnt in the previous chapter. Often we don't work with a single text file stored at our computer, but with multiple text files or entire corpora. We would like to have a way to load a corpus into Python.\n",
      "\n",
      "Remember how to read files? Each time we had to open a file, read the contents and then close the file. Since this is a series of steps we will often need to do, we can write a single function that does all that for us. We write a small utility function `read_file(filename)` that reads the specified file and simply returns all contents as a single string."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def read_file(filename):\n",
      "    \"Read the contents of FILENAME and return as a string.\"\n",
      "    infile = open(filename) # windows users should add the encoding='utf-8' option\n",
      "    contents = infile.read()\n",
      "    infile.close()\n",
      "    return contents"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now, instead of having to open a file, read the contents and close the file, we can just call the function `read_file` to do all that:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "text = read_file(\"data/romans_1:14-23_gk.txt\")\n",
      "print(text)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \u1f1d\u03bb\u03bb\u03b7\u03c3\u03af\u03bd \u03c4\u03b5 \u03ba\u03b1\u1f76 \u03b2\u03b1\u03c1\u03b2\u03ac\u03c1\u03bf\u03b9\u03c2, \u03c3\u03bf\u03c6\u03bf\u1fd6\u03c2 \u03c4\u03b5 \u03ba\u03b1\u1f76 \u1f00\u03bd\u03bf\u03ae\u03c4\u03bf\u03b9\u03c2 \u1f40\u03c6\u03b5\u03b9\u03bb\u03ad\u03c4\u03b7\u03c2 \u03b5\u1f30\u03bc\u03af\u00b7 \u03bf\u1f55\u03c4\u03c9\u03c2 \u03c4\u1f78 \u03ba\u03b1\u03c4\u2019 \u1f10\u03bc\u1f72 \u03c0\u03c1\u03cc\u03b8\u03c5\u03bc\u03bf\u03bd \u03ba\u03b1\u1f76 \u1f51\u03bc\u1fd6\u03bd \u03c4\u03bf\u1fd6\u03c2 \u1f10\u03bd \u1fec\u03ce\u03bc\u1fc3 \u03b5\u1f50\u03b1\u03b3\u03b3\u03b5\u03bb\u03af\u03c3\u03b1\u03c3\u03b8\u03b1\u03b9. \u039f\u1f50 \u03b3\u1f70\u03c1 \u1f10\u03c0\u03b1\u03b9\u03c3\u03c7\u03cd\u03bd\u03bf\u03bc\u03b1\u03b9 \u03c4\u1f78 \u2e00\u03b5\u1f50\u03b1\u03b3\u03b3\u03ad\u03bb\u03b9\u03bf\u03bd, \u03b4\u03cd\u03bd\u03b1\u03bc\u03b9\u03c2 \u03b3\u1f70\u03c1 \u03b8\u03b5\u03bf\u1fe6 \u1f10\u03c3\u03c4\u03b9\u03bd \u03b5\u1f30\u03c2 \u03c3\u03c9\u03c4\u03b7\u03c1\u03af\u03b1\u03bd \u03c0\u03b1\u03bd\u03c4\u1f76 \u03c4\u1ff7 \u03c0\u03b9\u03c3\u03c4\u03b5\u03cd\u03bf\u03bd\u03c4\u03b9, \u1f38\u03bf\u03c5\u03b4\u03b1\u03af\u1ff3 \u03c4\u03b5 \u03c0\u03c1\u1ff6\u03c4\u03bf\u03bd \u03ba\u03b1\u1f76 \u1f1d\u03bb\u03bb\u03b7\u03bd\u03b9\u00b7 \u03b4\u03b9\u03ba\u03b1\u03b9\u03bf\u03c3\u03cd\u03bd\u03b7 \u03b3\u1f70\u03c1 \u03b8\u03b5\u03bf\u1fe6 \u1f10\u03bd \u03b1\u1f50\u03c4\u1ff7 \u1f00\u03c0\u03bf\u03ba\u03b1\u03bb\u03cd\u03c0\u03c4\u03b5\u03c4\u03b1\u03b9 \u1f10\u03ba \u03c0\u03af\u03c3\u03c4\u03b5\u03c9\u03c2 \u03b5\u1f30\u03c2 \u03c0\u03af\u03c3\u03c4\u03b9\u03bd, \u03ba\u03b1\u03b8\u1f7c\u03c2 \u03b3\u03ad\u03b3\u03c1\u03b1\u03c0\u03c4\u03b1\u03b9\u00b7 \u1f49 \u03b4\u1f72 \u03b4\u03af\u03ba\u03b1\u03b9\u03bf\u03c2 \u1f10\u03ba \u03c0\u03af\u03c3\u03c4\u03b5\u03c9\u03c2 \u03b6\u03ae\u03c3\u03b5\u03c4\u03b1\u03b9. \u1f08\u03c0\u03bf\u03ba\u03b1\u03bb\u03cd\u03c0\u03c4\u03b5\u03c4\u03b1\u03b9 \u03b3\u1f70\u03c1 \u1f40\u03c1\u03b3\u1f74 \u03b8\u03b5\u03bf\u1fe6 \u1f00\u03c0\u2019 \u03bf\u1f50\u03c1\u03b1\u03bd\u03bf\u1fe6 \u1f10\u03c0\u1f76 \u03c0\u1fb6\u03c3\u03b1\u03bd \u1f00\u03c3\u03ad\u03b2\u03b5\u03b9\u03b1\u03bd \u03ba\u03b1\u1f76 \u1f00\u03b4\u03b9\u03ba\u03af\u03b1\u03bd \u1f00\u03bd\u03b8\u03c1\u03ce\u03c0\u03c9\u03bd \u03c4\u1ff6\u03bd \u03c4\u1f74\u03bd \u1f00\u03bb\u03ae\u03b8\u03b5\u03b9\u03b1\u03bd \u1f10\u03bd \u1f00\u03b4\u03b9\u03ba\u03af\u1fb3 \u03ba\u03b1\u03c4\u03b5\u03c7\u03cc\u03bd\u03c4\u03c9\u03bd, \u03b4\u03b9\u03cc\u03c4\u03b9 \u03c4\u1f78 \u03b3\u03bd\u03c9\u03c3\u03c4\u1f78\u03bd \u03c4\u03bf\u1fe6 \u03b8\u03b5\u03bf\u1fe6 \u03c6\u03b1\u03bd\u03b5\u03c1\u03cc\u03bd \u1f10\u03c3\u03c4\u03b9\u03bd \u1f10\u03bd \u03b1\u1f50\u03c4\u03bf\u1fd6\u03c2, \u1f41 \u2e02\u03b8\u03b5\u1f78\u03c2 \u03b3\u1f70\u03c1\u2e03 \u03b1\u1f50\u03c4\u03bf\u1fd6\u03c2 \u1f10\u03c6\u03b1\u03bd\u03ad\u03c1\u03c9\u03c3\u03b5\u03bd. \u03c4\u1f70 \u03b3\u1f70\u03c1 \u1f00\u03cc\u03c1\u03b1\u03c4\u03b1 \u03b1\u1f50\u03c4\u03bf\u1fe6 \u1f00\u03c0\u1f78 \u03ba\u03c4\u03af\u03c3\u03b5\u03c9\u03c2 \u03ba\u03cc\u03c3\u03bc\u03bf\u03c5 \u03c4\u03bf\u1fd6\u03c2 \u03c0\u03bf\u03b9\u03ae\u03bc\u03b1\u03c3\u03b9\u03bd \u03bd\u03bf\u03bf\u03cd\u03bc\u03b5\u03bd\u03b1 \u03ba\u03b1\u03b8\u03bf\u03c1\u1fb6\u03c4\u03b1\u03b9, \u1f25 \u03c4\u03b5 \u1f00\u0390\u03b4\u03b9\u03bf\u03c2 \u03b1\u1f50\u03c4\u03bf\u1fe6 \u03b4\u03cd\u03bd\u03b1\u03bc\u03b9\u03c2 \u03ba\u03b1\u1f76 \u03b8\u03b5\u03b9\u03cc\u03c4\u03b7\u03c2, \u03b5\u1f30\u03c2 \u03c4\u1f78 \u03b5\u1f36\u03bd\u03b1\u03b9 \u03b1\u1f50\u03c4\u03bf\u1f7a\u03c2 \u1f00\u03bd\u03b1\u03c0\u03bf\u03bb\u03bf\u03b3\u03ae\u03c4\u03bf\u03c5\u03c2, \u03b4\u03b9\u03cc\u03c4\u03b9 \u03b3\u03bd\u03cc\u03bd\u03c4\u03b5\u03c2 \u03c4\u1f78\u03bd \u03b8\u03b5\u1f78\u03bd \u03bf\u1f50\u03c7 \u1f61\u03c2 \u03b8\u03b5\u1f78\u03bd \u1f10\u03b4\u03cc\u03be\u03b1\u03c3\u03b1\u03bd \u1f22 \u03b7\u1f50\u03c7\u03b1\u03c1\u03af\u03c3\u03c4\u03b7\u03c3\u03b1\u03bd, \u1f00\u03bb\u03bb\u1f70 \u1f10\u03bc\u03b1\u03c4\u03b1\u03b9\u03ce\u03b8\u03b7\u03c3\u03b1\u03bd \u1f10\u03bd \u03c4\u03bf\u1fd6\u03c2 \u03b4\u03b9\u03b1\u03bb\u03bf\u03b3\u03b9\u03c3\u03bc\u03bf\u1fd6\u03c2 \u03b1\u1f50\u03c4\u1ff6\u03bd \u03ba\u03b1\u1f76 \u1f10\u03c3\u03ba\u03bf\u03c4\u03af\u03c3\u03b8\u03b7 \u1f21 \u1f00\u03c3\u03cd\u03bd\u03b5\u03c4\u03bf\u03c2 \u03b1\u1f50\u03c4\u1ff6\u03bd \u03ba\u03b1\u03c1\u03b4\u03af\u03b1\u00b7 \u03c6\u03ac\u03c3\u03ba\u03bf\u03bd\u03c4\u03b5\u03c2 \u03b5\u1f36\u03bd\u03b1\u03b9 \u03c3\u03bf\u03c6\u03bf\u1f76 \u1f10\u03bc\u03c9\u03c1\u03ac\u03bd\u03b8\u03b7\u03c3\u03b1\u03bd, \u03ba\u03b1\u1f76 \u1f24\u03bb\u03bb\u03b1\u03be\u03b1\u03bd \u03c4\u1f74\u03bd \u03b4\u03cc\u03be\u03b1\u03bd \u03c4\u03bf\u1fe6 \u1f00\u03c6\u03b8\u03ac\u03c1\u03c4\u03bf\u03c5 \u03b8\u03b5\u03bf\u1fe6 \u1f10\u03bd \u1f41\u03bc\u03bf\u03b9\u03ce\u03bc\u03b1\u03c4\u03b9 \u03b5\u1f30\u03ba\u03cc\u03bd\u03bf\u03c2 \u03c6\u03b8\u03b1\u03c1\u03c4\u03bf\u1fe6 \u1f00\u03bd\u03b8\u03c1\u03ce\u03c0\u03bf\u03c5 \u03ba\u03b1\u1f76 \u03c0\u03b5\u03c4\u03b5\u03b9\u03bd\u1ff6\u03bd \u03ba\u03b1\u1f76 \u03c4\u03b5\u03c4\u03c1\u03b1\u03c0\u03cc\u03b4\u03c9\u03bd \u03ba\u03b1\u1f76 \u1f11\u03c1\u03c0\u03b5\u03c4\u1ff6\u03bd.\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In the directory `data/NT/Raw` we have a corpus consisting of multiple files with the extension `.txt`. This corpus contains all the books of the New Testament in the raw text format from https://github.com/morphgnt/sblgnt. We want to iterate over all these files. You can do this using the `listdir` function from the `os` module. We import this function as follows:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from os import listdir"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "After that, the `listdir` function is available to use. This function takes as argument the path to a directory and returns all the files and subdirectories present in that directory:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "listdir(\"data\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 4,
       "text": [
        "['arabian_nights',\n",
        " 'austen-emma-excerpt-tokenised.txt',\n",
        " 'austen-emma-excerpt.txt',\n",
        " 'austen-emma.txt',\n",
        " 'british-novels',\n",
        " 'gutenberg',\n",
        " 'haggard',\n",
        " 'names',\n",
        " 'NT',\n",
        " 'romans_1:14-23_gk.txt',\n",
        " 'romans_1_gk.txt',\n",
        " 'romans_gk.txt',\n",
        " 'romans_greeting_gk.txt',\n",
        " 'romans_punc.txt',\n",
        " 'twitter.txt']"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Notice that `listdir` returns a list and we can iterate over that list. Now, consider the following function:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def list_textfiles(directory):\n",
      "    \"Return a list of filenames ending in '.txt' in DIRECTORY.\"\n",
      "    textfiles = []\n",
      "    for filename in listdir(directory):\n",
      "        if filename.endswith(\".txt\"):\n",
      "            textfiles.append(directory + \"/\" + filename)\n",
      "    return textfiles"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The function `listdir` takes as argument the name of a directory and lists all filenames in that directory. We iterate over this list and append each filename that ends with the extension, `.txt` to a new list of `textfiles`. Using the `list_textfiles` function, the following code will read all text files in the directory `data/NT/Raw` and outputs the length (in characters) of each:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for filepath in list_textfiles(\"data/NT/Raw\"):\n",
      "    text = read_file(filepath)\n",
      "    print(filepath +  \" has \" + str(len(text)) + \" characters.\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "data/NT/Raw/61-Mt-morphgnt.txt has 1188419 characters.\n",
        "data/NT/Raw/62-Mk-morphgnt.txt has 742730 characters.\n",
        "data/NT/Raw/63-Lk-morphgnt.txt has 1265393 characters.\n",
        "data/NT/Raw/64-Jn-morphgnt.txt has 964834 characters.\n",
        "data/NT/Raw/65-Ac-morphgnt.txt has 1229183 characters.\n",
        "data/NT/Raw/66-Ro-morphgnt.txt has 452321 characters.\n",
        "data/NT/Raw/67-1Co-morphgnt.txt has 435080 characters.\n",
        "data/NT/Raw/68-2Co-morphgnt.txt has 291598 characters.\n",
        "data/NT/Raw/69-Ga-morphgnt.txt has 145250 characters.\n",
        "data/NT/Raw/70-Eph-morphgnt.txt has 156660 characters.\n",
        "data/NT/Raw/71-Php-morphgnt.txt has 105206 characters.\n",
        "data/NT/Raw/72-Col-morphgnt.txt has 102852 characters.\n",
        "data/NT/Raw/73-1Th-morphgnt.txt has 96316 characters.\n",
        "data/NT/Raw/74-2Th-morphgnt.txt has 53242 characters.\n",
        "data/NT/Raw/75-1Ti-morphgnt.txt has 110709 characters.\n",
        "data/NT/Raw/76-2Ti-morphgnt.txt has 83503 characters.\n",
        "data/NT/Raw/77-Tit-morphgnt.txt has 46385 characters.\n",
        "data/NT/Raw/78-Phm-morphgnt.txt has 21101 characters.\n",
        "data/NT/Raw/79-Heb-morphgnt.txt has 333385 characters.\n",
        "data/NT/Raw/80-Jas-morphgnt.txt has 114611 characters.\n",
        "data/NT/Raw/81-1Pe-morphgnt.txt has 114390 characters.\n",
        "data/NT/Raw/82-2Pe-morphgnt.txt has 76001 characters.\n",
        "data/NT/Raw/83-1Jn-morphgnt.txt has 130820 characters.\n",
        "data/NT/Raw/84-2Jn-morphgnt.txt has 15328 characters.\n",
        "data/NT/Raw/85-3Jn-morphgnt.txt has 14475 characters.\n",
        "data/NT/Raw/86-Jud-morphgnt.txt has 31988 characters.\n",
        "data/NT/Raw/87-Re-morphgnt.txt has 616219 characters.\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Sentence tokenization"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In the previous chapter we wrote a function to tokenize or split a text string into a list of words. However, using this function we lose information about where sentences end and start in the text. We will develop a function `split_sentences` that performs some very simple sentence splitting when passed a text string. Each sentence will be represented as a new string, so the function as a whole returns a list of sentence strings. We assume that any occurrence of either `.` or `!` or `?` marks the end of a sentence. In reality, this is more ambiguous of course. Consider for example the use of periods as end-of-sentence marker as well as in abbreviations and initials!\n",
      "\n",
      "How should we tackle this problem? Have a look at the following picture:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "![caption](files/images/indexing.png)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The first sentence *Hello there!* spans from index 0 to index 11. The second sentence from 13 to 26. If we come up with a way to extract those indexes, we could slice the text into separate sentences. First we define a utility function `end_of_sentence` that takes as argument a character and returns `True` if it is an end-of-sentence marker, otherwise it returns `False`. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Quiz!"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Write the function `end_of_sentence_marker` below:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def end_of_sentence_marker(character):\n",
      "    # insert your code here\n",
      "    return character in ('.', '?', '!')\n",
      "\n",
      "# these tests should return True if your code is correct\n",
      "print(end_of_sentence_marker(\"?\") == True)\n",
      "print(end_of_sentence_marker(\"a\") == False)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "True\n",
        "True\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "An important function we will use is the built in `enumerate`. `enumerate` takes as argument any iterable (a string a list etc.). Let's see it in action:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for element in enumerate(\"Python\"):\n",
      "    print(element)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(0, 'P')\n",
        "(1, 'y')\n",
        "(2, 't')\n",
        "(3, 'h')\n",
        "(4, 'o')\n",
        "(5, 'n')\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As you can see, enumerate allows you to iterate over an iterable and for each element in that iterable, it gives you its corresponding index. A slightly more convenient way of iterating over `enumerate` is the following:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for index, character in enumerate(\"Python\"):\n",
      "    print(index)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0\n",
        "1\n",
        "2\n",
        "3\n",
        "4\n",
        "5\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This way, we have easy access to both the index and the original item in the iterable. Now we know enough to write our `split_sentences` function. We will walk you through it, step by step, but first try to read the function and think about what it possibly does at each step:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def split_sentences(text):\n",
      "    \"Split a text string into a list of sentences.\"\n",
      "    sentences = []\n",
      "    start = 0\n",
      "    for end, character in enumerate(text):\n",
      "        if end_of_sentence_marker(character):\n",
      "            sentence = text[start: end + 1]\n",
      "            sentences.append(sentence)\n",
      "            start = end + 1\n",
      "    return sentences"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The function `split_sentences` takes as argument a text represented by a simple string. Within the function we define a variable `sentences` in which we will store the individual sentences. We need to extract both the start position and the end position of each sentence. We know that the first sentence will always start at position 0. Therefore we define a variable start and set it to zero.\n",
      "\n",
      "Next we will use `enumerate` to loop over all individual characters in the text. Remember that enumerate returns pairs of indexes and their corresponding elements (here characters). For each character we check whether it is an end-of-sentence marker. If it is, the variable `end` marks the position in `text` where a sentence ends. We can now slice the text from the starting position to the end position and obtain our sentence. Notice that we add 1 to the end position. Why would that be? This is because, as you might remember from the first chapter, slices are non-inclusive, so `text[start:end]` would return the text starting at `start` and ending one position before `end`. Since we have reached the end of a sentence, we know that the next sentence will start at least one position later than our last end point. Therefore, we update the start variable to `end + 1`. Let's check whether our function works as promised:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(split_sentences(\"This is a sentence. Should we seperate it from this one?\"))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['This is a sentence.', ' Should we seperate it from this one?']\n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "It does! "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Quiz!"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To conclude this section, you will write a wrapper function `tokenize`, that takes as input a text represented by a string and tokenizes this string into sentences. After that, we clean each sentence, by lowercasing all words and removing punctuation. The final step is to tokenize each sentence into a list of words. The file `preprocessing.py` contains a function called `clean_gk` which removes all punctuation from a Greek text and turns all characters to lowercase. We import that function using the following line:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from pyhum.preprocessing import clean_gk"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "SyntaxError",
       "evalue": "Non-ASCII character '\\xe2' in file pyhum/preprocessing.py on line 14, but no encoding declared; see http://www.python.org/peps/pep-0263.html for details (preprocessing.py, line 14)",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;36m  File \u001b[0;32m\"pyhum/preprocessing.py\"\u001b[0;36m, line \u001b[0;32m14\u001b[0m\n\u001b[0;31m    return ''.join(ch for ch in s if ch not in \"\u2019(.;)\u2014\u2e02\u2e00\u2e01,\u00b7\u2e03\")\u001b[0m\n\u001b[0m                                                              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m Non-ASCII character '\\xe2' in file pyhum/preprocessing.py on line 14, but no encoding declared; see http://www.python.org/peps/pep-0263.html for details\n"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "First, you will need to rewrite your `end_of_sentence_marker` function to recognize the two end-of-sentence markers in our Greek New Testament text: `.` and `;`."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def gk_end_of_sentence_marker(character):\n",
      "    # insert your code here\n",
      "    return character in ('.', ';')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "And then a small change to the `split_sentences` function to make it use the `gk_end_of_sentence_marker` function instead of the regular `end_of_sentence_marker` function."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def gk_split_sentences(text):\n",
      "    \"Split a text string into a list of sentences.\"\n",
      "    sentences = []\n",
      "    start = 0\n",
      "    for end, character in enumerate(text):\n",
      "        if gk_end_of_sentence_marker(character):\n",
      "            sentence = text[start: end + 1]\n",
      "            sentences.append(sentence)\n",
      "            start = end + 1\n",
      "    return sentences"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def tokenize(text):\n",
      "    \"\"\"Transform text into a list of sentences. Lowercase \n",
      "    each sentence and remove all punctuation. Finally split each\n",
      "    sentence into a list of words.\"\"\"\n",
      "    # insert your code here\n",
      "    stripped_sents = []\n",
      "    sents = gk_split_sentences(text)\n",
      "    for sent in sents:\n",
      "        stripped_sents.append(clean_gk(sent).split())\n",
      "    return stripped_sents\n",
      "    \n",
      "\n",
      "# these tests should return True if your code is correct\n",
      "print(tokenize(\"\u03a4\u03af \u03b4\u1f72 \u1f51\u03bc\u1fd6\u03bd \u03b4\u03bf\u03ba\u03b5\u1fd6; \u1f04\u03bd\u03b8\u03c1\u03c9\u03c0\u03bf\u03c2 \u03b5\u1f36\u03c7\u03b5\u03bd \u03c4\u03ad\u03ba\u03bd\u03b1 \u03b4\u03cd\u03bf.\") == \n",
      "      [['\u03c4\u03af', '\u03b4\u1f72', '\u1f51\u03bc\u1fd6\u03bd', '\u03b4\u03bf\u03ba\u03b5\u1fd6'], ['\u1f04\u03bd\u03b8\u03c1\u03c9\u03c0\u03bf\u03c2', '\u03b5\u1f36\u03c7\u03b5\u03bd', '\u03c4\u03ad\u03ba\u03bd\u03b1', '\u03b4\u03cd\u03bf']])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "NameError",
       "evalue": "global name 'clean_gk' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-15-b34a1798d7eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# these tests should return True if your code is correct\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m print(tokenize(\"\u03a4\u03af \u03b4\u1f72 \u1f51\u03bc\u1fd6\u03bd \u03b4\u03bf\u03ba\u03b5\u1fd6; \u1f04\u03bd\u03b8\u03c1\u03c9\u03c0\u03bf\u03c2 \u03b5\u1f36\u03c7\u03b5\u03bd \u03c4\u03ad\u03ba\u03bd\u03b1 \u03b4\u03cd\u03bf.\") == \n\u001b[0m\u001b[1;32m     15\u001b[0m       [['\u03c4\u03af', '\u03b4\u1f72', '\u1f51\u03bc\u1fd6\u03bd', '\u03b4\u03bf\u03ba\u03b5\u1fd6'], ['\u1f04\u03bd\u03b8\u03c1\u03c9\u03c0\u03bf\u03c2', '\u03b5\u1f36\u03c7\u03b5\u03bd', '\u03c4\u03ad\u03ba\u03bd\u03b1', '\u03b4\u03cd\u03bf']])\n",
        "\u001b[0;32m<ipython-input-15-b34a1798d7eb>\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0msents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgk_split_sentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mstripped_sents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean_gk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mstripped_sents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mNameError\u001b[0m: global name 'clean_gk' is not defined"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "General Text Statistics"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "> \u1f10\u03ac\u03bd \u03c4\u03b9\u03c2 \u1f10\u03c0\u03b9\u03b8\u1fc7 \u1f10\u03c0\u2019 \u03b1\u1f50\u03c4\u03ac, \u1f10\u03c0\u03b9\u03b8\u03ae\u03c3\u03b5\u03b9 \u1f41 \u03b8\u03b5\u1f78\u03c2 \u1f10\u03c0\u2019 \u03b1\u1f50\u03c4\u1f78\u03bd \u03c4\u1f70\u03c2 \u03c0\u03bb\u03b7\u03b3\u1f70\u03c2 \u03c4\u1f70\u03c2 \u03b3\u03b5\u03b3\u03c1\u03b1\u03bc\u03bc\u03ad\u03bd\u03b1\u03c2 \u1f10\u03bd \u03c4\u1ff7 \u03b2\u03b9\u03b2\u03bb\u03af\u1ff3 \u03c4\u03bf\u03cd\u03c4\u1ff3\u00b7"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "One of the great advantages of being a student of the Bible in the digital age is the great amount of information that is available, one might even say that has been _added_, to the plain text of the Bible.  Besides the innumerable amateur, professional, and semi-professional commentaries on words, passages, and whole books, there are also many very useful sites that contain more information about the very words themselves, whether this information is lexical, syntactic, or textual.  Besides well-known sites such as <a href='http://www.perseus.tufts.edu/hopper/'>Perseus</a> or the German Bible Society's <a href='http://www.academic-bible.com/en/home/'>Academic Bible Portal</a>, there are also older or lesser known projects, like the <a href='http://ccat.sas.upenn.edu/rak/catss.html'>Computer Assisted Tools for Septuagint/Scriptural Study</a>.\n",
      "\n",
      "The texts that we are going to work with for the rest of this lesson come from a very current project to analyze every word in the New Testament, the <a href='http://morphgnt.org/'>MorphGNT</a> project, which provides \"linguistic databases and Python tools for the Greek New Testament.\"  (If you're interested in the Hebrew Bible, check out <a href='https://github.com/openscriptures/morphhb'>MorphHB</a>.)  If we take a look at one line of one of their files, we will see what a wealth of information they have.\n",
      "\n",
      "> 010405 V- 3AAI-S-- \u2e00\u1f14\u03c3\u03c4\u03b7\u03c3\u03b5\u03bd \u1f14\u03c3\u03c4\u03b7\u03c3\u03b5\u03bd \u1f14\u03c3\u03c4\u03b7\u03c3\u03b5(\u03bd) \u1f35\u03c3\u03c4\u03b7\u03bc\u03b9\n",
      "\n",
      "As you can see, each line is divided by spaces into 7 parts separated.  The first part, the six digits, consists of three 2-digit groups that tell us the book, chapter and verse in which this word occurs, in this case Matthew (book 01) 4:5.  The second part represents the part of speech, here a verb.  The next part is an 8-part parsing code.  This one tells us that the verb is a `3`rd person `A`orist `A`ctive `I`ndicative `S`ingular verb form.  The final four parts represent the text of the word in different states: 1) exactly as it appears in the text, 2) without punctuation, 3) normalized, and 4) lemma.  And every word in the New Testament is represented by such a line.  Perhaps you can imagine some of the possibilities!"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Quiz!"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We want to recreate the biblical text of the SBLGNT, the text used by MorphGNT.  We could use the `read_file` function above but, as we just saw, the MorphGNT files are not set up as a running text but, instead, as lines that each represent a single word.  So the first thing we should do is to rewrite this function to split a text file into lines instead of reading it in as one string, as we did above.  Take a look at our new function below."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def read_file_lines(filename):\n",
      "    \"Read the contents of FILENAME and return as a string.\"\n",
      "    infile = open(filename) # windows users should add the encoding='utf-8' option\n",
      "    contents = infile.read()\n",
      "    infile.close()\n",
      "    contents = contents.split('\\n') # \\n is the line separator\n",
      "    return contents"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The only difference between the two functions (besides the name) is that we have added the line\n",
      "\n",
      "    contents = contents.split('\\n')\n",
      "    \n",
      "If we pass the string `'\\n'` to the `split` method, it will split the string on the line separator '\\n' and return a list where each member is a separate line in the original file."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lines = read_file_lines('data/NT/Raw/85-3Jn-morphgnt.txt')\n",
      "lines[:5]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 17,
       "text": [
        "['250101 RA ----NSM- \\xe1\\xbd\\x89 \\xe1\\xbd\\x89 \\xe1\\xbd\\x81 \\xe1\\xbd\\x81',\n",
        " '250101 A- ----NSM- \\xcf\\x80\\xcf\\x81\\xce\\xb5\\xcf\\x83\\xce\\xb2\\xcf\\x8d\\xcf\\x84\\xce\\xb5\\xcf\\x81\\xce\\xbf\\xcf\\x82 \\xcf\\x80\\xcf\\x81\\xce\\xb5\\xcf\\x83\\xce\\xb2\\xcf\\x8d\\xcf\\x84\\xce\\xb5\\xcf\\x81\\xce\\xbf\\xcf\\x82 \\xcf\\x80\\xcf\\x81\\xce\\xb5\\xcf\\x83\\xce\\xb2\\xcf\\x8d\\xcf\\x84\\xce\\xb5\\xcf\\x81\\xce\\xbf\\xcf\\x82 \\xcf\\x80\\xcf\\x81\\xce\\xb5\\xcf\\x83\\xce\\xb2\\xcf\\x8d\\xcf\\x84\\xce\\xb5\\xcf\\x81\\xce\\xbf\\xcf\\x82',\n",
        " '250101 N- ----DSM- \\xce\\x93\\xce\\xb1\\xce\\x90\\xe1\\xbf\\xb3 \\xce\\x93\\xce\\xb1\\xce\\x90\\xe1\\xbf\\xb3 \\xce\\x93\\xce\\xb1\\xce\\x90\\xe1\\xbf\\xb3 \\xce\\x93\\xce\\xac\\xcf\\x8a\\xce\\xbf\\xcf\\x82',\n",
        " '250101 RA ----DSM- \\xcf\\x84\\xe1\\xbf\\xb7 \\xcf\\x84\\xe1\\xbf\\xb7 \\xcf\\x84\\xe1\\xbf\\xb7 \\xe1\\xbd\\x81',\n",
        " '250101 A- ----DSM- \\xe1\\xbc\\x80\\xce\\xb3\\xce\\xb1\\xcf\\x80\\xce\\xb7\\xcf\\x84\\xe1\\xbf\\xb7, \\xe1\\xbc\\x80\\xce\\xb3\\xce\\xb1\\xcf\\x80\\xce\\xb7\\xcf\\x84\\xe1\\xbf\\xb7 \\xe1\\xbc\\x80\\xce\\xb3\\xce\\xb1\\xcf\\x80\\xce\\xb7\\xcf\\x84\\xe1\\xbf\\xb7 \\xe1\\xbc\\x80\\xce\\xb3\\xce\\xb1\\xcf\\x80\\xce\\xb7\\xcf\\x84\\xcf\\x8c\\xcf\\x82']"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Our list `lines` now has the information for every word in 3 John separated into one line per word.  Your task is to write code below that will loop through each line of the `lines` list, extract the word as it appears in the text from each line, and add that word to a string object (called `Third_Jn`) that will then be the whole text of 3 John as it appears in the SLBGNT."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Third_Jn = ''\n",
      "# insert your code here\n",
      "for line in lines:\n",
      "    Third_Jn = ' '.join([Third_Jn, line.split()[3]])\n",
      "Third_Jn = Third_Jn.strip()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 18
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We now have the whole book 3 John in the string variable `Third_Jn`.  Let's see how it looks."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(Third_Jn)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\u1f49 \u03c0\u03c1\u03b5\u03c3\u03b2\u03cd\u03c4\u03b5\u03c1\u03bf\u03c2 \u0393\u03b1\u0390\u1ff3 \u03c4\u1ff7 \u1f00\u03b3\u03b1\u03c0\u03b7\u03c4\u1ff7, \u1f43\u03bd \u1f10\u03b3\u1f7c \u1f00\u03b3\u03b1\u03c0\u1ff6 \u1f10\u03bd \u1f00\u03bb\u03b7\u03b8\u03b5\u03af\u1fb3. \u1f08\u03b3\u03b1\u03c0\u03b7\u03c4\u03ad, \u03c0\u03b5\u03c1\u1f76 \u03c0\u03ac\u03bd\u03c4\u03c9\u03bd \u03b5\u1f54\u03c7\u03bf\u03bc\u03b1\u03af \u03c3\u03b5 \u03b5\u1f50\u03bf\u03b4\u03bf\u1fe6\u03c3\u03b8\u03b1\u03b9 \u03ba\u03b1\u1f76 \u1f51\u03b3\u03b9\u03b1\u03af\u03bd\u03b5\u03b9\u03bd, \u03ba\u03b1\u03b8\u1f7c\u03c2 \u03b5\u1f50\u03bf\u03b4\u03bf\u1fe6\u03c4\u03b1\u03af \u03c3\u03bf\u03c5 \u1f21 \u03c8\u03c5\u03c7\u03ae. \u1f10\u03c7\u03ac\u03c1\u03b7\u03bd \u03b3\u1f70\u03c1 \u03bb\u03af\u03b1\u03bd \u1f10\u03c1\u03c7\u03bf\u03bc\u03ad\u03bd\u03c9\u03bd \u1f00\u03b4\u03b5\u03bb\u03c6\u1ff6\u03bd \u03ba\u03b1\u1f76 \u03bc\u03b1\u03c1\u03c4\u03c5\u03c1\u03bf\u03cd\u03bd\u03c4\u03c9\u03bd \u03c3\u03bf\u03c5 \u03c4\u1fc7 \u1f00\u03bb\u03b7\u03b8\u03b5\u03af\u1fb3, \u03ba\u03b1\u03b8\u1f7c\u03c2 \u03c3\u1f7a \u1f10\u03bd \u1f00\u03bb\u03b7\u03b8\u03b5\u03af\u1fb3 \u03c0\u03b5\u03c1\u03b9\u03c0\u03b1\u03c4\u03b5\u1fd6\u03c2. \u03bc\u03b5\u03b9\u03b6\u03bf\u03c4\u03ad\u03c1\u03b1\u03bd \u03c4\u03bf\u03cd\u03c4\u03c9\u03bd \u03bf\u1f50\u03ba \u1f14\u03c7\u03c9 \u2e00\u03c7\u03b1\u03c1\u03ac\u03bd, \u1f35\u03bd\u03b1 \u1f00\u03ba\u03bf\u03cd\u03c9 \u03c4\u1f70 \u1f10\u03bc\u1f70 \u03c4\u03ad\u03ba\u03bd\u03b1 \u1f10\u03bd \u2e00\u03c4\u1fc7 \u1f00\u03bb\u03b7\u03b8\u03b5\u03af\u1fb3 \u03c0\u03b5\u03c1\u03b9\u03c0\u03b1\u03c4\u03bf\u1fe6\u03bd\u03c4\u03b1. \u1f08\u03b3\u03b1\u03c0\u03b7\u03c4\u03ad, \u03c0\u03b9\u03c3\u03c4\u1f78\u03bd \u03c0\u03bf\u03b9\u03b5\u1fd6\u03c2 \u1f43 \u1f10\u1f70\u03bd \u1f10\u03c1\u03b3\u03ac\u03c3\u1fc3 \u03b5\u1f30\u03c2 \u03c4\u03bf\u1f7a\u03c2 \u1f00\u03b4\u03b5\u03bb\u03c6\u03bf\u1f7a\u03c2 \u03ba\u03b1\u1f76 \u2e00\u03c4\u03bf\u1fe6\u03c4\u03bf \u03be\u03ad\u03bd\u03bf\u03c5\u03c2, \u03bf\u1f33 \u1f10\u03bc\u03b1\u03c1\u03c4\u03cd\u03c1\u03b7\u03c3\u03ac\u03bd \u03c3\u03bf\u03c5 \u03c4\u1fc7 \u1f00\u03b3\u03ac\u03c0\u1fc3 \u1f10\u03bd\u03ce\u03c0\u03b9\u03bf\u03bd \u1f10\u03ba\u03ba\u03bb\u03b7\u03c3\u03af\u03b1\u03c2, \u03bf\u1f53\u03c2 \u03ba\u03b1\u03bb\u1ff6\u03c2 \u03c0\u03bf\u03b9\u03ae\u03c3\u03b5\u03b9\u03c2 \u03c0\u03c1\u03bf\u03c0\u03ad\u03bc\u03c8\u03b1\u03c2 \u1f00\u03be\u03af\u03c9\u03c2 \u03c4\u03bf\u1fe6 \u03b8\u03b5\u03bf\u1fe6\u00b7 \u1f51\u03c0\u1f72\u03c1 \u03b3\u1f70\u03c1 \u03c4\u03bf\u1fe6 \u1f40\u03bd\u03cc\u03bc\u03b1\u03c4\u03bf\u03c2 \u1f10\u03be\u1fc6\u03bb\u03b8\u03bf\u03bd \u03bc\u03b7\u03b4\u1f72\u03bd \u03bb\u03b1\u03bc\u03b2\u03ac\u03bd\u03bf\u03bd\u03c4\u03b5\u03c2 \u1f00\u03c0\u1f78 \u03c4\u1ff6\u03bd \u2e00\u1f10\u03b8\u03bd\u03b9\u03ba\u1ff6\u03bd. \u1f21\u03bc\u03b5\u1fd6\u03c2 \u03bf\u1f56\u03bd \u1f40\u03c6\u03b5\u03af\u03bb\u03bf\u03bc\u03b5\u03bd \u2e00\u1f51\u03c0\u03bf\u03bb\u03b1\u03bc\u03b2\u03ac\u03bd\u03b5\u03b9\u03bd \u03c4\u03bf\u1f7a\u03c2 \u03c4\u03bf\u03b9\u03bf\u03cd\u03c4\u03bf\u03c5\u03c2, \u1f35\u03bd\u03b1 \u03c3\u03c5\u03bd\u03b5\u03c1\u03b3\u03bf\u1f76 \u03b3\u03b9\u03bd\u03ce\u03bc\u03b5\u03b8\u03b1 \u03c4\u1fc7 \u1f00\u03bb\u03b7\u03b8\u03b5\u03af\u1fb3. \u1f1c\u03b3\u03c1\u03b1\u03c8\u03ac \u2e00\u03c4\u03b9 \u03c4\u1fc7 \u1f10\u03ba\u03ba\u03bb\u03b7\u03c3\u03af\u1fb3\u00b7 \u1f00\u03bb\u03bb\u2019 \u1f41 \u03c6\u03b9\u03bb\u03bf\u03c0\u03c1\u03c9\u03c4\u03b5\u03cd\u03c9\u03bd \u03b1\u1f50\u03c4\u1ff6\u03bd \u0394\u03b9\u03bf\u03c4\u03c1\u03ad\u03c6\u03b7\u03c2 \u03bf\u1f50\u03ba \u1f10\u03c0\u03b9\u03b4\u03ad\u03c7\u03b5\u03c4\u03b1\u03b9 \u1f21\u03bc\u1fb6\u03c2. \u03b4\u03b9\u1f70 \u03c4\u03bf\u1fe6\u03c4\u03bf, \u1f10\u1f70\u03bd \u1f14\u03bb\u03b8\u03c9, \u1f51\u03c0\u03bf\u03bc\u03bd\u03ae\u03c3\u03c9 \u03b1\u1f50\u03c4\u03bf\u1fe6 \u03c4\u1f70 \u1f14\u03c1\u03b3\u03b1 \u1f03 \u03c0\u03bf\u03b9\u03b5\u1fd6, \u03bb\u03cc\u03b3\u03bf\u03b9\u03c2 \u03c0\u03bf\u03bd\u03b7\u03c1\u03bf\u1fd6\u03c2 \u03c6\u03bb\u03c5\u03b1\u03c1\u1ff6\u03bd \u1f21\u03bc\u1fb6\u03c2, \u03ba\u03b1\u1f76 \u03bc\u1f74 \u1f00\u03c1\u03ba\u03bf\u03cd\u03bc\u03b5\u03bd\u03bf\u03c2 \u1f10\u03c0\u1f76 \u03c4\u03bf\u03cd\u03c4\u03bf\u03b9\u03c2 \u03bf\u1f54\u03c4\u03b5 \u03b1\u1f50\u03c4\u1f78\u03c2 \u1f10\u03c0\u03b9\u03b4\u03ad\u03c7\u03b5\u03c4\u03b1\u03b9 \u03c4\u03bf\u1f7a\u03c2 \u1f00\u03b4\u03b5\u03bb\u03c6\u03bf\u1f7a\u03c2 \u03ba\u03b1\u1f76 \u03c4\u03bf\u1f7a\u03c2 \u03b2\u03bf\u03c5\u03bb\u03bf\u03bc\u03ad\u03bd\u03bf\u03c5\u03c2 \u03ba\u03c9\u03bb\u03cd\u03b5\u03b9 \u03ba\u03b1\u1f76 \u1f10\u03ba \u03c4\u1fc6\u03c2 \u1f10\u03ba\u03ba\u03bb\u03b7\u03c3\u03af\u03b1\u03c2 \u1f10\u03ba\u03b2\u03ac\u03bb\u03bb\u03b5\u03b9. \u1f08\u03b3\u03b1\u03c0\u03b7\u03c4\u03ad, \u03bc\u1f74 \u03bc\u03b9\u03bc\u03bf\u1fe6 \u03c4\u1f78 \u03ba\u03b1\u03ba\u1f78\u03bd \u1f00\u03bb\u03bb\u1f70 \u03c4\u1f78 \u1f00\u03b3\u03b1\u03b8\u03cc\u03bd. \u1f41 \u1f00\u03b3\u03b1\u03b8\u03bf\u03c0\u03bf\u03b9\u1ff6\u03bd \u1f10\u03ba \u03c4\u03bf\u1fe6 \u03b8\u03b5\u03bf\u1fe6 \u1f10\u03c3\u03c4\u03b9\u03bd\u00b7 \u1f41 \u03ba\u03b1\u03ba\u03bf\u03c0\u03bf\u03b9\u1ff6\u03bd \u03bf\u1f50\u03c7 \u1f11\u03ce\u03c1\u03b1\u03ba\u03b5\u03bd \u03c4\u1f78\u03bd \u03b8\u03b5\u03cc\u03bd. \u0394\u03b7\u03bc\u03b7\u03c4\u03c1\u03af\u1ff3 \u03bc\u03b5\u03bc\u03b1\u03c1\u03c4\u03cd\u03c1\u03b7\u03c4\u03b1\u03b9 \u1f51\u03c0\u1f78 \u03c0\u03ac\u03bd\u03c4\u03c9\u03bd \u03ba\u03b1\u1f76 \u1f51\u03c0\u1f78 \u03b1\u1f50\u03c4\u1fc6\u03c2 \u03c4\u1fc6\u03c2 \u1f00\u03bb\u03b7\u03b8\u03b5\u03af\u03b1\u03c2\u00b7 \u03ba\u03b1\u1f76 \u1f21\u03bc\u03b5\u1fd6\u03c2 \u03b4\u1f72 \u03bc\u03b1\u03c1\u03c4\u03c5\u03c1\u03bf\u1fe6\u03bc\u03b5\u03bd, \u03ba\u03b1\u1f76 \u2e00\u03bf\u1f36\u03b4\u03b1\u03c2 \u1f45\u03c4\u03b9 \u1f21 \u03bc\u03b1\u03c1\u03c4\u03c5\u03c1\u03af\u03b1 \u1f21\u03bc\u1ff6\u03bd \u1f00\u03bb\u03b7\u03b8\u03ae\u03c2 \u1f10\u03c3\u03c4\u03b9\u03bd. \u03a0\u03bf\u03bb\u03bb\u1f70 \u03b5\u1f36\u03c7\u03bf\u03bd \u2e02\u03b3\u03c1\u03ac\u03c8\u03b1\u03b9 \u03c3\u03bf\u03b9\u2e03, \u1f00\u03bb\u03bb\u2019 \u03bf\u1f50 \u03b8\u03ad\u03bb\u03c9 \u03b4\u03b9\u1f70 \u03bc\u03ad\u03bb\u03b1\u03bd\u03bf\u03c2 \u03ba\u03b1\u1f76 \u03ba\u03b1\u03bb\u03ac\u03bc\u03bf\u03c5 \u03c3\u03bf\u03b9 \u2e00\u03b3\u03c1\u03ac\u03c6\u03b5\u03b9\u03bd\u00b7 \u1f10\u03bb\u03c0\u03af\u03b6\u03c9 \u03b4\u1f72 \u03b5\u1f50\u03b8\u03ad\u03c9\u03c2 \u2e02\u03c3\u03b5 \u1f30\u03b4\u03b5\u1fd6\u03bd\u2e03, \u03ba\u03b1\u1f76 \u03c3\u03c4\u03cc\u03bc\u03b1 \u03c0\u03c1\u1f78\u03c2 \u03c3\u03c4\u03cc\u03bc\u03b1 \u03bb\u03b1\u03bb\u03ae\u03c3\u03bf\u03bc\u03b5\u03bd. \u0395\u1f30\u03c1\u03ae\u03bd\u03b7 \u03c3\u03bf\u03b9. \u1f00\u03c3\u03c0\u03ac\u03b6\u03bf\u03bd\u03c4\u03b1\u03af \u03c3\u03b5 \u03bf\u1f31 \u03c6\u03af\u03bb\u03bf\u03b9. \u1f00\u03c3\u03c0\u03ac\u03b6\u03bf\u03c5 \u03c4\u03bf\u1f7a\u03c2 \u03c6\u03af\u03bb\u03bf\u03c5\u03c2 \u03ba\u03b1\u03c4\u2019 \u1f44\u03bd\u03bf\u03bc\u03b1.\n"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "How does it look?  If you see any problems, go back to your code and correct them.  Now that we have the text of the book, it would be a shame to lose it.  We could recreate it easily, but we could just save it to a text file and wouldn't need to.  Plus, saving it will allow us to use the .txt file with other programs on our system.\n",
      "\n",
      "But before we just blunder into giving the file any name we want, let's take a look at how the files from MorphGNT are named."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "list_textfiles(\"data/NT/Raw\")[:10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 20,
       "text": [
        "['data/NT/Raw/61-Mt-morphgnt.txt',\n",
        " 'data/NT/Raw/62-Mk-morphgnt.txt',\n",
        " 'data/NT/Raw/63-Lk-morphgnt.txt',\n",
        " 'data/NT/Raw/64-Jn-morphgnt.txt',\n",
        " 'data/NT/Raw/65-Ac-morphgnt.txt',\n",
        " 'data/NT/Raw/66-Ro-morphgnt.txt',\n",
        " 'data/NT/Raw/67-1Co-morphgnt.txt',\n",
        " 'data/NT/Raw/68-2Co-morphgnt.txt',\n",
        " 'data/NT/Raw/69-Ga-morphgnt.txt',\n",
        " 'data/NT/Raw/70-Eph-morphgnt.txt']"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As you can see, the files are sorted in the order in which they appear in the NT by the number that begins their file name (61, 62, 63, etc.).  It would be a shame to lose this information, so we would like to keep this number at the beginning of all of the files.  We could simply load each file and give each one its own name.  But we are learning to program to save ourselves the trouble of having to do such things.  So what we will do below is build a series of functions that will allow us to load each file, read its text into a string, and then save the string with a filename appropriate for that book of the Bible."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Quiz!"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**1)** We want to build our `save as` filenames using the original filenames as a guide.  To do this, the first thing we need to do is to remove the file extension from the filename, in our case `.txt`.  Write a function `remove_ext` that takes as argument a string. It should return the string without the file extension. Tip: use the function `splitext` from the `os.path` module. Look up the documentation [here](https://docs.python.org/3.4/library/os.path.html#os.path.splitext)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from os.path import splitext\n",
      "\n",
      "def remove_ext(filename):\n",
      "    # insert your code here\n",
      "    return splitext(filename)[0]\n",
      "    \n",
      "# these tests should return True if your code is correct\n",
      "print(remove_ext(\"data/NT/Raw/61-Mt-morphgnt.txt\") == \"data/NT/Raw/61-Mt-morphgnt\")\n",
      "print(remove_ext(\"ridiculous_selfie.jpg\") == \"ridiculous_selfie\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "True\n",
        "True\n"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**2)** Next, we need to remove the directory from the front of the filename.  Write a function `remove_dir` that takes as argument a filepath and removes the directory from a filepath. Tip: use the function `basename` from the `os.path` module. Look up the document [here](http://docs.python.org/3.4/library/os.path.html#os.path.basename)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from os.path import basename\n",
      "\n",
      "def remove_dir(filepath):\n",
      "    # insert your code here\n",
      "    return basename(filepath)\n",
      "    \n",
      "# these tests should return True if your code is correct\n",
      "print(remove_dir(\"data/NT/Raw/61-Mt-morphgnt.txt\") == \"61-Mt-morphgnt.txt\")\n",
      "print(remove_dir(\"/a/kind/of/funny/filepath/to/file.txt\") == \"file.txt\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "True\n",
        "True\n"
       ]
      }
     ],
     "prompt_number": 22
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**3)** Combine the two functions `remove_ext` and `remove_dir` into one function `get_filename`. This function takes as argument a filepath and returns the name (without the extensions) of the file."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_filename(filepath):\n",
      "    # insert your code here\n",
      "    return remove_ext(basename(filepath))\n",
      "    \n",
      "# these tests should return True if your code is correct\n",
      "print(get_filename(\"data/NT/Raw/61-Mt-morphgnt.txt\") == '61-Mt-morphgnt')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "True\n"
       ]
      }
     ],
     "prompt_number": 23
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The final step is to strip the part of the filename that we don't want in our `save as` filename, i.e., the '-morphgnt' part.  Below we will present two different ways to do this.  \n",
      "\n",
      "The first way is useful if you want to strip the same sequence of characters from the beginning or the end of the filename, or any string.  It takes advantage of the `strip`, `lstrip`, and `rstrip` methods on strings.  Let's take a look at how they work.\n",
      "\n",
      "As the names suggest, they 'strip' something from a string, either on both sides (`strip`), the left side (`lstrip`), or the right side (`rstrip`).  Take a look."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "string = '...wo.rd...'\n",
      "print(string.strip('.'))\n",
      "print(string.lstrip('.'))\n",
      "print(string.rstrip('.'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "wo.rd\n",
        "wo.rd...\n",
        "...wo.rd\n"
       ]
      }
     ],
     "prompt_number": 24
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Notice that it strips __all__ occurrences of the string from the left and/or right side.  Below, right a short bit of code using `strip` that gives us what we want."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "name = '61-Mt-morphgnt'\n",
      "new_name = name.strip('morphgnt').strip('-')#insert your code here\n",
      "\n",
      "print(new_name == '61-Mt')\n",
      "print(new_name)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "True\n",
        "61-Mt\n"
       ]
      }
     ],
     "prompt_number": 25
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Did you have any problems?  If so, why?\n",
      "\n",
      "The second method is a bit more generalizable and will not run into the problem we had above.  It uses the `split` method that we have already seen.  Try to figure out below how to use `split` in the function below to give us what we want."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "name = '61-Mt-morphgnt'\n",
      "def build_filename(f_name):\n",
      "    #insert your code here\n",
      "    return '-'.join(f_name.split('-')[:-1]) + '.txt'\n",
      "\n",
      "print(build_filename(name) == '61-Mt.txt')\n",
      "print(build_filename(name))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "True\n",
        "61-Mt.txt\n"
       ]
      }
     ],
     "prompt_number": 26
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Not quite as easy but, trust me, knowing how to split and reconstitute filenames, and strings in general, will be of enormous use.  And now we have a function that will do it for us."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Quiz!"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now, finally, combine the functions `get_filename` and `build_filename` into the function `saveas_filename` to obtain the filename we want to use to save the resulting text."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def saveas_filename(filepath):\n",
      "    # insert your code here\n",
      "    return build_filename(get_filename(filepath))\n",
      "\n",
      "# these tests should return True if your code is correct\n",
      "print(saveas_filename(\"data/NT/Raw/61-Mt-morphgnt.txt\") == \"61-Mt.txt\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "True\n"
       ]
      }
     ],
     "prompt_number": 27
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "OK, so we now have the tools we need read the text of every NT book into a file of its own.  The problem is that we are losing a lot of information when we do this.  What we want to do below is to construct a function that will capture some of this information for us: the chapter and verse numbers for the text.  And while we could simply insert these into the text at the correct place, instead we will use a dictionary to organize this information in a way that is easily retrievable later.\n",
      "\n",
      "We learned about dictionaries in Chapter 1.  Remember dictionaries are made up of key:value pairs that allow us to look up the values very easily: `dict(key)` will return the value of that key in the dictionary.  But the `values` in a dictionary do not have to be strings, integers, etc.  They can also be other Python objects, like lists or even other dictionaries!  Take a look."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "d = {}\n",
      "d['list'] = [1, 2, 3, 4, 5]\n",
      "d['dict'] = {'\u1f41': 'the', '\u03b8\u03b5\u03cc\u03c2': 'God'}\n",
      "d['Mt'] = {1: {1: ['\u03b2\u03af\u03b2\u03bb\u03bf\u03c2', '\u03b3\u03b5\u03bd\u03ad\u03c3\u03b5\u03c9\u03c2', '\u1f38\u03b7\u03c3\u03bf\u1fe6', '\u03a7\u03c1\u03b9\u03c3\u03c4\u03bf\u1fe6', '\u03c5\u1f31\u03bf\u1fe6', '\u0394\u03b1\u03c5\u1f76\u03b4', '\u03c5\u1f31\u03bf\u1fe6', '\u1f08\u03b2\u03c1\u03b1\u03ac\u03bc']}}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 28
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Do you see what is going on with `d['Mt']`?  This is what we want to end up with.  And look what we can do with it then."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(d['Mt'][1][1]) #print out the words in Matthew chapter 1, verse 1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['\\xce\\xb2\\xce\\xaf\\xce\\xb2\\xce\\xbb\\xce\\xbf\\xcf\\x82', '\\xce\\xb3\\xce\\xb5\\xce\\xbd\\xce\\xad\\xcf\\x83\\xce\\xb5\\xcf\\x89\\xcf\\x82', '\\xe1\\xbc\\xb8\\xce\\xb7\\xcf\\x83\\xce\\xbf\\xe1\\xbf\\xa6', '\\xce\\xa7\\xcf\\x81\\xce\\xb9\\xcf\\x83\\xcf\\x84\\xce\\xbf\\xe1\\xbf\\xa6', '\\xcf\\x85\\xe1\\xbc\\xb1\\xce\\xbf\\xe1\\xbf\\xa6', '\\xce\\x94\\xce\\xb1\\xcf\\x85\\xe1\\xbd\\xb6\\xce\\xb4', '\\xcf\\x85\\xe1\\xbc\\xb1\\xce\\xbf\\xe1\\xbf\\xa6', '\\xe1\\xbc\\x88\\xce\\xb2\\xcf\\x81\\xce\\xb1\\xce\\xac\\xce\\xbc']\n"
       ]
      }
     ],
     "prompt_number": 29
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This may appear to be a difficult task at first, but if we break it down into its component steps, we should be able to accomplish it without too much trouble.\n",
      "\n",
      "The best way to go about it is to build the dictionary from the inside out.  That is, first we will build the list of words in each verse, then put together each verse for each chapter, and then, finally, put together all the chapters in the book.  First, let's take another look at what the lines look like in the original files.\n",
      "\n",
      "> 010405 V- 3AAI-S-- \u2e00\u1f14\u03c3\u03c4\u03b7\u03c3\u03b5\u03bd \u1f14\u03c3\u03c4\u03b7\u03c3\u03b5\u03bd \u1f14\u03c3\u03c4\u03b7\u03c3\u03b5(\u03bd) \u1f35\u03c3\u03c4\u03b7\u03bc\u03b9\n",
      "\n",
      "Our first task will be to write a function that takes such a line as input and extracts the elements that we want, in this case the first element that represents the book, chapter, and verse for each word.  We have 4 different forms of the word to choose from.  For this activity, we'll use the normalized form of the word, i.e., the third word for or the sixth element of each line.  The normalized form has capitalization normalized (NB, not always lowercased), punctuation removed, and accents normalized so that the same morphological form of the lemma will always appear in the same form.  This will make it easier to extract word statistics, as we saw above."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def bcv_word_extract(line):\n",
      "    #insert your code here\n",
      "    bcv = line.split()[0]\n",
      "    word = line.split()[5]\n",
      "    \n",
      "    return bcv, word\n",
      "\n",
      "print(bcv_word_extract('010405 V- 3AAI-S-- \u2e00\u1f14\u03c3\u03c4\u03b7\u03c3\u03b5\u03bd \u1f14\u03c3\u03c4\u03b7\u03c3\u03b5\u03bd \u1f14\u03c3\u03c4\u03b7\u03c3\u03b5(\u03bd) \u1f35\u03c3\u03c4\u03b7\u03bc\u03b9') == ('010405', '\u1f14\u03c3\u03c4\u03b7\u03c3\u03b5(\u03bd)'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "True\n"
       ]
      }
     ],
     "prompt_number": 30
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We now have the information that we need to build a list of words in each verse.  But first we need to figure out how to detect when the verse changes from one verse to another.  One way of doing this is demonstrated below."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "numbers = (1,1,1,1,2,2,2,3,3,3,4,4,4)\n",
      "old_number = 1\n",
      "for number in numbers:\n",
      "    if number != old_number:\n",
      "        print(number, ' != ', old_number)\n",
      "        old_number = number"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(2, ' != ', 1)\n",
        "(3, ' != ', 2)\n",
        "(4, ' != ', 3)\n"
       ]
      }
     ],
     "prompt_number": 31
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Notice that the `if` statement is only executed when the new number changes from what it was before.  And if this is the case, then we need to change `old_number` so that it is the same as the new value of `number`.\n",
      "\n",
      "It is time to start putting our functions together to build the first level of our dictionary.  Below, write a function that will take a list of lines from one of our files as input and then will do the following:\n",
      "\n",
      "-  will initialize an empty dictionary `d`\n",
      "-  will loop over every line in the file\n",
      "-  will use our `bcv_word_extract` function to extract the book-chapter-verse number and the appropriate word from that line\n",
      "-  will check to see if the book-chapter-verse number is the same as that of the previous line\n",
      "-  if it isn't, it will add a new key to the dictionary with the book-chapter-verse number as the key and a new list with that word as its first element as the value for that key\n",
      "-  if it is, it will append the word to the existing list of values for that key\n",
      "-  will finally return the dictionary that has been built\n",
      "\n",
      "I have given you a few snippets of code to get you started"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def build_verses(lines):\n",
      "    d = {}\n",
      "    old_bcv = ''\n",
      "    for line in lines:\n",
      "        #insert your code here\n",
      "        bcv, word = bcv_word_extract(line) #delete this line\n",
      "        if bcv != old_bcv:\n",
      "            #insert your code here\n",
      "            d[bcv] = [word] #delete this line\n",
      "            old_bcv = bcv #delete this line\n",
      "        else:\n",
      "            #insert your code here\n",
      "            d[bcv].append(word) #delete this line\n",
      "    return d\n",
      "\n",
      "Third_Jn_dict = build_verses(read_file_lines('data/NT/Raw/85-3Jn-morphgnt.txt'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 32
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(Third_Jn_dict)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "{'250110': ['\\xce\\xb4\\xce\\xb9\\xce\\xac', '\\xcf\\x84\\xce\\xbf\\xe1\\xbf\\xa6\\xcf\\x84\\xce\\xbf', '\\xe1\\xbc\\x90\\xce\\xac\\xce\\xbd', '\\xe1\\xbc\\x94\\xce\\xbb\\xce\\xb8\\xcf\\x89', '\\xe1\\xbd\\x91\\xcf\\x80\\xce\\xbf\\xce\\xbc\\xce\\xbd\\xce\\xae\\xcf\\x83\\xcf\\x89', '\\xce\\xb1\\xe1\\xbd\\x90\\xcf\\x84\\xce\\xbf\\xe1\\xbf\\xa6', '\\xcf\\x84\\xce\\xac', '\\xe1\\xbc\\x94\\xcf\\x81\\xce\\xb3\\xce\\xb1', '\\xe1\\xbc\\x85', '\\xcf\\x80\\xce\\xbf\\xce\\xb9\\xce\\xb5\\xe1\\xbf\\x96', '\\xce\\xbb\\xcf\\x8c\\xce\\xb3\\xce\\xbf\\xce\\xb9\\xcf\\x82', '\\xcf\\x80\\xce\\xbf\\xce\\xbd\\xce\\xb7\\xcf\\x81\\xce\\xbf\\xe1\\xbf\\x96\\xcf\\x82', '\\xcf\\x86\\xce\\xbb\\xcf\\x85\\xce\\xb1\\xcf\\x81\\xe1\\xbf\\xb6\\xce\\xbd', '\\xe1\\xbc\\xa1\\xce\\xbc\\xe1\\xbe\\xb6\\xcf\\x82', '\\xce\\xba\\xce\\xb1\\xce\\xaf', '\\xce\\xbc\\xce\\xae', '\\xe1\\xbc\\x80\\xcf\\x81\\xce\\xba\\xce\\xbf\\xcf\\x8d\\xce\\xbc\\xce\\xb5\\xce\\xbd\\xce\\xbf\\xcf\\x82', '\\xe1\\xbc\\x90\\xcf\\x80\\xce\\xaf', '\\xcf\\x84\\xce\\xbf\\xcf\\x8d\\xcf\\x84\\xce\\xbf\\xce\\xb9\\xcf\\x82', '\\xce\\xbf\\xe1\\xbd\\x94\\xcf\\x84\\xce\\xb5', '\\xce\\xb1\\xe1\\xbd\\x90\\xcf\\x84\\xcf\\x8c\\xcf\\x82', '\\xe1\\xbc\\x90\\xcf\\x80\\xce\\xb9\\xce\\xb4\\xce\\xad\\xcf\\x87\\xce\\xb5\\xcf\\x84\\xce\\xb1\\xce\\xb9', '\\xcf\\x84\\xce\\xbf\\xcf\\x8d\\xcf\\x82', '\\xe1\\xbc\\x80\\xce\\xb4\\xce\\xb5\\xce\\xbb\\xcf\\x86\\xce\\xbf\\xcf\\x8d\\xcf\\x82', '\\xce\\xba\\xce\\xb1\\xce\\xaf', '\\xcf\\x84\\xce\\xbf\\xcf\\x8d\\xcf\\x82', '\\xce\\xb2\\xce\\xbf\\xcf\\x85\\xce\\xbb\\xce\\xbf\\xce\\xbc\\xce\\xad\\xce\\xbd\\xce\\xbf\\xcf\\x85\\xcf\\x82', '\\xce\\xba\\xcf\\x89\\xce\\xbb\\xcf\\x8d\\xce\\xb5\\xce\\xb9', '\\xce\\xba\\xce\\xb1\\xce\\xaf', '\\xe1\\xbc\\x90\\xce\\xba', '\\xcf\\x84\\xe1\\xbf\\x86\\xcf\\x82', '\\xe1\\xbc\\x90\\xce\\xba\\xce\\xba\\xce\\xbb\\xce\\xb7\\xcf\\x83\\xce\\xaf\\xce\\xb1\\xcf\\x82', '\\xe1\\xbc\\x90\\xce\\xba\\xce\\xb2\\xce\\xac\\xce\\xbb\\xce\\xbb\\xce\\xb5\\xce\\xb9'], '250108': ['\\xe1\\xbc\\xa1\\xce\\xbc\\xce\\xb5\\xe1\\xbf\\x96\\xcf\\x82', '\\xce\\xbf\\xe1\\xbd\\x96\\xce\\xbd', '\\xe1\\xbd\\x80\\xcf\\x86\\xce\\xb5\\xce\\xaf\\xce\\xbb\\xce\\xbf\\xce\\xbc\\xce\\xb5\\xce\\xbd', '\\xe1\\xbd\\x91\\xcf\\x80\\xce\\xbf\\xce\\xbb\\xce\\xb1\\xce\\xbc\\xce\\xb2\\xce\\xac\\xce\\xbd\\xce\\xb5\\xce\\xb9\\xce\\xbd', '\\xcf\\x84\\xce\\xbf\\xcf\\x8d\\xcf\\x82', '\\xcf\\x84\\xce\\xbf\\xce\\xb9\\xce\\xbf\\xcf\\x8d\\xcf\\x84\\xce\\xbf\\xcf\\x85\\xcf\\x82', '\\xe1\\xbc\\xb5\\xce\\xbd\\xce\\xb1', '\\xcf\\x83\\xcf\\x85\\xce\\xbd\\xce\\xb5\\xcf\\x81\\xce\\xb3\\xce\\xbf\\xce\\xaf', '\\xce\\xb3\\xce\\xb9\\xce\\xbd\\xcf\\x8e\\xce\\xbc\\xce\\xb5\\xce\\xb8\\xce\\xb1', '\\xcf\\x84\\xe1\\xbf\\x87', '\\xe1\\xbc\\x80\\xce\\xbb\\xce\\xb7\\xce\\xb8\\xce\\xb5\\xce\\xaf\\xe1\\xbe\\xb3'], '250109': ['\\xe1\\xbc\\x94\\xce\\xb3\\xcf\\x81\\xce\\xb1\\xcf\\x88\\xce\\xb1', '\\xcf\\x84\\xce\\xb9', '\\xcf\\x84\\xe1\\xbf\\x87', '\\xe1\\xbc\\x90\\xce\\xba\\xce\\xba\\xce\\xbb\\xce\\xb7\\xcf\\x83\\xce\\xaf\\xe1\\xbe\\xb3', '\\xe1\\xbc\\x80\\xce\\xbb\\xce\\xbb\\xce\\xac', '\\xe1\\xbd\\x81', '\\xcf\\x86\\xce\\xb9\\xce\\xbb\\xce\\xbf\\xcf\\x80\\xcf\\x81\\xcf\\x89\\xcf\\x84\\xce\\xb5\\xcf\\x8d\\xcf\\x89\\xce\\xbd', '\\xce\\xb1\\xe1\\xbd\\x90\\xcf\\x84\\xe1\\xbf\\xb6\\xce\\xbd', '\\xce\\x94\\xce\\xb9\\xce\\xbf\\xcf\\x84\\xcf\\x81\\xce\\xad\\xcf\\x86\\xce\\xb7\\xcf\\x82', '\\xce\\xbf\\xe1\\xbd\\x90', '\\xe1\\xbc\\x90\\xcf\\x80\\xce\\xb9\\xce\\xb4\\xce\\xad\\xcf\\x87\\xce\\xb5\\xcf\\x84\\xce\\xb1\\xce\\xb9', '\\xe1\\xbc\\xa1\\xce\\xbc\\xe1\\xbe\\xb6\\xcf\\x82'], '250104': ['\\xce\\xbc\\xce\\xb5\\xce\\xb9\\xce\\xb6\\xce\\xbf\\xcf\\x84\\xce\\xad\\xcf\\x81\\xce\\xb1\\xce\\xbd', '\\xcf\\x84\\xce\\xbf\\xcf\\x8d\\xcf\\x84\\xcf\\x89\\xce\\xbd', '\\xce\\xbf\\xe1\\xbd\\x90', '\\xe1\\xbc\\x94\\xcf\\x87\\xcf\\x89', '\\xcf\\x87\\xce\\xb1\\xcf\\x81\\xce\\xac\\xce\\xbd', '\\xe1\\xbc\\xb5\\xce\\xbd\\xce\\xb1', '\\xe1\\xbc\\x80\\xce\\xba\\xce\\xbf\\xcf\\x8d\\xcf\\x89', '\\xcf\\x84\\xce\\xac', '\\xe1\\xbc\\x90\\xce\\xbc\\xce\\xac', '\\xcf\\x84\\xce\\xad\\xce\\xba\\xce\\xbd\\xce\\xb1', '\\xe1\\xbc\\x90\\xce\\xbd', '\\xcf\\x84\\xe1\\xbf\\x87', '\\xe1\\xbc\\x80\\xce\\xbb\\xce\\xb7\\xce\\xb8\\xce\\xb5\\xce\\xaf\\xe1\\xbe\\xb3', '\\xcf\\x80\\xce\\xb5\\xcf\\x81\\xce\\xb9\\xcf\\x80\\xce\\xb1\\xcf\\x84\\xce\\xbf\\xe1\\xbf\\xa6\\xce\\xbd\\xcf\\x84\\xce\\xb1'], '250105': ['\\xe1\\xbc\\x80\\xce\\xb3\\xce\\xb1\\xcf\\x80\\xce\\xb7\\xcf\\x84\\xce\\xad', '\\xcf\\x80\\xce\\xb9\\xcf\\x83\\xcf\\x84\\xcf\\x8c\\xce\\xbd', '\\xcf\\x80\\xce\\xbf\\xce\\xb9\\xce\\xb5\\xe1\\xbf\\x96\\xcf\\x82', '\\xe1\\xbd\\x85', '\\xe1\\xbc\\x90\\xce\\xac\\xce\\xbd', '\\xe1\\xbc\\x90\\xcf\\x81\\xce\\xb3\\xce\\xac\\xcf\\x83\\xe1\\xbf\\x83', '\\xce\\xb5\\xe1\\xbc\\xb0\\xcf\\x82', '\\xcf\\x84\\xce\\xbf\\xcf\\x8d\\xcf\\x82', '\\xe1\\xbc\\x80\\xce\\xb4\\xce\\xb5\\xce\\xbb\\xcf\\x86\\xce\\xbf\\xcf\\x8d\\xcf\\x82', '\\xce\\xba\\xce\\xb1\\xce\\xaf', '\\xcf\\x84\\xce\\xbf\\xe1\\xbf\\xa6\\xcf\\x84\\xce\\xbf', '\\xce\\xbe\\xce\\xad\\xce\\xbd\\xce\\xbf\\xcf\\x85\\xcf\\x82'], '250106': ['\\xce\\xbf\\xe1\\xbc\\xb5', '\\xe1\\xbc\\x90\\xce\\xbc\\xce\\xb1\\xcf\\x81\\xcf\\x84\\xcf\\x8d\\xcf\\x81\\xce\\xb7\\xcf\\x83\\xce\\xb1\\xce\\xbd', '\\xcf\\x83\\xce\\xbf\\xcf\\x85', '\\xcf\\x84\\xe1\\xbf\\x87', '\\xe1\\xbc\\x80\\xce\\xb3\\xce\\xac\\xcf\\x80\\xe1\\xbf\\x83', '\\xe1\\xbc\\x90\\xce\\xbd\\xcf\\x8e\\xcf\\x80\\xce\\xb9\\xce\\xbf\\xce\\xbd', '\\xe1\\xbc\\x90\\xce\\xba\\xce\\xba\\xce\\xbb\\xce\\xb7\\xcf\\x83\\xce\\xaf\\xce\\xb1\\xcf\\x82', '\\xce\\xbf\\xe1\\xbd\\x95\\xcf\\x82', '\\xce\\xba\\xce\\xb1\\xce\\xbb\\xe1\\xbf\\xb6\\xcf\\x82', '\\xcf\\x80\\xce\\xbf\\xce\\xb9\\xce\\xae\\xcf\\x83\\xce\\xb5\\xce\\xb9\\xcf\\x82', '\\xcf\\x80\\xcf\\x81\\xce\\xbf\\xcf\\x80\\xce\\xad\\xce\\xbc\\xcf\\x88\\xce\\xb1\\xcf\\x82', '\\xe1\\xbc\\x80\\xce\\xbe\\xce\\xaf\\xcf\\x89\\xcf\\x82', '\\xcf\\x84\\xce\\xbf\\xe1\\xbf\\xa6', '\\xce\\xb8\\xce\\xb5\\xce\\xbf\\xe1\\xbf\\xa6'], '250107': ['\\xe1\\xbd\\x91\\xcf\\x80\\xce\\xad\\xcf\\x81', '\\xce\\xb3\\xce\\xac\\xcf\\x81', '\\xcf\\x84\\xce\\xbf\\xe1\\xbf\\xa6', '\\xe1\\xbd\\x80\\xce\\xbd\\xcf\\x8c\\xce\\xbc\\xce\\xb1\\xcf\\x84\\xce\\xbf\\xcf\\x82', '\\xe1\\xbc\\x90\\xce\\xbe\\xe1\\xbf\\x86\\xce\\xbb\\xce\\xb8\\xce\\xbf\\xce\\xbd', '\\xce\\xbc\\xce\\xb7\\xce\\xb4\\xce\\xad\\xce\\xbd', '\\xce\\xbb\\xce\\xb1\\xce\\xbc\\xce\\xb2\\xce\\xac\\xce\\xbd\\xce\\xbf\\xce\\xbd\\xcf\\x84\\xce\\xb5\\xcf\\x82', '\\xe1\\xbc\\x80\\xcf\\x80\\xcf\\x8c', '\\xcf\\x84\\xe1\\xbf\\xb6\\xce\\xbd', '\\xe1\\xbc\\x90\\xce\\xb8\\xce\\xbd\\xce\\xb9\\xce\\xba\\xe1\\xbf\\xb6\\xce\\xbd'], '250101': ['\\xe1\\xbd\\x81', '\\xcf\\x80\\xcf\\x81\\xce\\xb5\\xcf\\x83\\xce\\xb2\\xcf\\x8d\\xcf\\x84\\xce\\xb5\\xcf\\x81\\xce\\xbf\\xcf\\x82', '\\xce\\x93\\xce\\xb1\\xce\\x90\\xe1\\xbf\\xb3', '\\xcf\\x84\\xe1\\xbf\\xb7', '\\xe1\\xbc\\x80\\xce\\xb3\\xce\\xb1\\xcf\\x80\\xce\\xb7\\xcf\\x84\\xe1\\xbf\\xb7', '\\xe1\\xbd\\x85\\xce\\xbd', '\\xe1\\xbc\\x90\\xce\\xb3\\xcf\\x8e', '\\xe1\\xbc\\x80\\xce\\xb3\\xce\\xb1\\xcf\\x80\\xe1\\xbf\\xb6', '\\xe1\\xbc\\x90\\xce\\xbd', '\\xe1\\xbc\\x80\\xce\\xbb\\xce\\xb7\\xce\\xb8\\xce\\xb5\\xce\\xaf\\xe1\\xbe\\xb3'], '250102': ['\\xe1\\xbc\\x80\\xce\\xb3\\xce\\xb1\\xcf\\x80\\xce\\xb7\\xcf\\x84\\xce\\xad', '\\xcf\\x80\\xce\\xb5\\xcf\\x81\\xce\\xaf', '\\xcf\\x80\\xce\\xac\\xce\\xbd\\xcf\\x84\\xcf\\x89\\xce\\xbd', '\\xce\\xb5\\xe1\\xbd\\x94\\xcf\\x87\\xce\\xbf\\xce\\xbc\\xce\\xb1\\xce\\xb9', '\\xcf\\x83\\xce\\xb5', '\\xce\\xb5\\xe1\\xbd\\x90\\xce\\xbf\\xce\\xb4\\xce\\xbf\\xe1\\xbf\\xa6\\xcf\\x83\\xce\\xb8\\xce\\xb1\\xce\\xb9', '\\xce\\xba\\xce\\xb1\\xce\\xaf', '\\xe1\\xbd\\x91\\xce\\xb3\\xce\\xb9\\xce\\xb1\\xce\\xaf\\xce\\xbd\\xce\\xb5\\xce\\xb9\\xce\\xbd', '\\xce\\xba\\xce\\xb1\\xce\\xb8\\xcf\\x8e\\xcf\\x82', '\\xce\\xb5\\xe1\\xbd\\x90\\xce\\xbf\\xce\\xb4\\xce\\xbf\\xe1\\xbf\\xa6\\xcf\\x84\\xce\\xb1\\xce\\xb9', '\\xcf\\x83\\xce\\xbf\\xcf\\x85', '\\xe1\\xbc\\xa1', '\\xcf\\x88\\xcf\\x85\\xcf\\x87\\xce\\xae'], '250103': ['\\xe1\\xbc\\x90\\xcf\\x87\\xce\\xac\\xcf\\x81\\xce\\xb7\\xce\\xbd', '\\xce\\xb3\\xce\\xac\\xcf\\x81', '\\xce\\xbb\\xce\\xaf\\xce\\xb1\\xce\\xbd', '\\xe1\\xbc\\x90\\xcf\\x81\\xcf\\x87\\xce\\xbf\\xce\\xbc\\xce\\xad\\xce\\xbd\\xcf\\x89\\xce\\xbd', '\\xe1\\xbc\\x80\\xce\\xb4\\xce\\xb5\\xce\\xbb\\xcf\\x86\\xe1\\xbf\\xb6\\xce\\xbd', '\\xce\\xba\\xce\\xb1\\xce\\xaf', '\\xce\\xbc\\xce\\xb1\\xcf\\x81\\xcf\\x84\\xcf\\x85\\xcf\\x81\\xce\\xbf\\xcf\\x8d\\xce\\xbd\\xcf\\x84\\xcf\\x89\\xce\\xbd', '\\xcf\\x83\\xce\\xbf\\xcf\\x85', '\\xcf\\x84\\xe1\\xbf\\x87', '\\xe1\\xbc\\x80\\xce\\xbb\\xce\\xb7\\xce\\xb8\\xce\\xb5\\xce\\xaf\\xe1\\xbe\\xb3', '\\xce\\xba\\xce\\xb1\\xce\\xb8\\xcf\\x8e\\xcf\\x82', '\\xcf\\x83\\xcf\\x8d', '\\xe1\\xbc\\x90\\xce\\xbd', '\\xe1\\xbc\\x80\\xce\\xbb\\xce\\xb7\\xce\\xb8\\xce\\xb5\\xce\\xaf\\xe1\\xbe\\xb3', '\\xcf\\x80\\xce\\xb5\\xcf\\x81\\xce\\xb9\\xcf\\x80\\xce\\xb1\\xcf\\x84\\xce\\xb5\\xe1\\xbf\\x96\\xcf\\x82'], '250113': ['\\xcf\\x80\\xce\\xbf\\xce\\xbb\\xce\\xbb\\xce\\xac', '\\xce\\xb5\\xe1\\xbc\\xb6\\xcf\\x87\\xce\\xbf\\xce\\xbd', '\\xce\\xb3\\xcf\\x81\\xce\\xac\\xcf\\x88\\xce\\xb1\\xce\\xb9', '\\xcf\\x83\\xce\\xbf\\xce\\xb9', '\\xe1\\xbc\\x80\\xce\\xbb\\xce\\xbb\\xce\\xac', '\\xce\\xbf\\xe1\\xbd\\x90', '\\xce\\xb8\\xce\\xad\\xce\\xbb\\xcf\\x89', '\\xce\\xb4\\xce\\xb9\\xce\\xac', '\\xce\\xbc\\xce\\xad\\xce\\xbb\\xce\\xb1\\xce\\xbd\\xce\\xbf\\xcf\\x82', '\\xce\\xba\\xce\\xb1\\xce\\xaf', '\\xce\\xba\\xce\\xb1\\xce\\xbb\\xce\\xac\\xce\\xbc\\xce\\xbf\\xcf\\x85', '\\xcf\\x83\\xce\\xbf\\xce\\xb9', '\\xce\\xb3\\xcf\\x81\\xce\\xac\\xcf\\x86\\xce\\xb5\\xce\\xb9\\xce\\xbd'], '250115': ['\\xce\\xb5\\xe1\\xbc\\xb0\\xcf\\x81\\xce\\xae\\xce\\xbd\\xce\\xb7', '\\xcf\\x83\\xce\\xbf\\xce\\xb9', '\\xe1\\xbc\\x80\\xcf\\x83\\xcf\\x80\\xce\\xac\\xce\\xb6\\xce\\xbf\\xce\\xbd\\xcf\\x84\\xce\\xb1\\xce\\xb9', '\\xcf\\x83\\xce\\xb5', '\\xce\\xbf\\xe1\\xbc\\xb1', '\\xcf\\x86\\xce\\xaf\\xce\\xbb\\xce\\xbf\\xce\\xb9', '\\xe1\\xbc\\x80\\xcf\\x83\\xcf\\x80\\xce\\xac\\xce\\xb6\\xce\\xbf\\xcf\\x85', '\\xcf\\x84\\xce\\xbf\\xcf\\x8d\\xcf\\x82', '\\xcf\\x86\\xce\\xaf\\xce\\xbb\\xce\\xbf\\xcf\\x85\\xcf\\x82', '\\xce\\xba\\xce\\xb1\\xcf\\x84\\xce\\xac', '\\xe1\\xbd\\x84\\xce\\xbd\\xce\\xbf\\xce\\xbc\\xce\\xb1'], '250112': ['\\xce\\x94\\xce\\xb7\\xce\\xbc\\xce\\xb7\\xcf\\x84\\xcf\\x81\\xce\\xaf\\xe1\\xbf\\xb3', '\\xce\\xbc\\xce\\xb5\\xce\\xbc\\xce\\xb1\\xcf\\x81\\xcf\\x84\\xcf\\x8d\\xcf\\x81\\xce\\xb7\\xcf\\x84\\xce\\xb1\\xce\\xb9', '\\xe1\\xbd\\x91\\xcf\\x80\\xcf\\x8c', '\\xcf\\x80\\xce\\xac\\xce\\xbd\\xcf\\x84\\xcf\\x89\\xce\\xbd', '\\xce\\xba\\xce\\xb1\\xce\\xaf', '\\xe1\\xbd\\x91\\xcf\\x80\\xcf\\x8c', '\\xce\\xb1\\xe1\\xbd\\x90\\xcf\\x84\\xe1\\xbf\\x86\\xcf\\x82', '\\xcf\\x84\\xe1\\xbf\\x86\\xcf\\x82', '\\xe1\\xbc\\x80\\xce\\xbb\\xce\\xb7\\xce\\xb8\\xce\\xb5\\xce\\xaf\\xce\\xb1\\xcf\\x82', '\\xce\\xba\\xce\\xb1\\xce\\xaf', '\\xe1\\xbc\\xa1\\xce\\xbc\\xce\\xb5\\xe1\\xbf\\x96\\xcf\\x82', '\\xce\\xb4\\xce\\xad', '\\xce\\xbc\\xce\\xb1\\xcf\\x81\\xcf\\x84\\xcf\\x85\\xcf\\x81\\xce\\xbf\\xe1\\xbf\\xa6\\xce\\xbc\\xce\\xb5\\xce\\xbd', '\\xce\\xba\\xce\\xb1\\xce\\xaf', '\\xce\\xbf\\xe1\\xbc\\xb6\\xce\\xb4\\xce\\xb1\\xcf\\x82', '\\xe1\\xbd\\x85\\xcf\\x84\\xce\\xb9', '\\xe1\\xbc\\xa1', '\\xce\\xbc\\xce\\xb1\\xcf\\x81\\xcf\\x84\\xcf\\x85\\xcf\\x81\\xce\\xaf\\xce\\xb1', '\\xe1\\xbc\\xa1\\xce\\xbc\\xe1\\xbf\\xb6\\xce\\xbd', '\\xe1\\xbc\\x80\\xce\\xbb\\xce\\xb7\\xce\\xb8\\xce\\xae\\xcf\\x82', '\\xe1\\xbc\\x90\\xcf\\x83\\xcf\\x84\\xce\\xaf(\\xce\\xbd)'], '250114': ['\\xe1\\xbc\\x90\\xce\\xbb\\xcf\\x80\\xce\\xaf\\xce\\xb6\\xcf\\x89', '\\xce\\xb4\\xce\\xad', '\\xce\\xb5\\xe1\\xbd\\x90\\xce\\xb8\\xce\\xad\\xcf\\x89\\xcf\\x82', '\\xcf\\x83\\xce\\xb5', '\\xe1\\xbc\\xb0\\xce\\xb4\\xce\\xb5\\xe1\\xbf\\x96\\xce\\xbd', '\\xce\\xba\\xce\\xb1\\xce\\xaf', '\\xcf\\x83\\xcf\\x84\\xcf\\x8c\\xce\\xbc\\xce\\xb1', '\\xcf\\x80\\xcf\\x81\\xcf\\x8c\\xcf\\x82', '\\xcf\\x83\\xcf\\x84\\xcf\\x8c\\xce\\xbc\\xce\\xb1', '\\xce\\xbb\\xce\\xb1\\xce\\xbb\\xce\\xae\\xcf\\x83\\xce\\xbf\\xce\\xbc\\xce\\xb5\\xce\\xbd'], '250111': ['\\xe1\\xbc\\x80\\xce\\xb3\\xce\\xb1\\xcf\\x80\\xce\\xb7\\xcf\\x84\\xce\\xad', '\\xce\\xbc\\xce\\xae', '\\xce\\xbc\\xce\\xb9\\xce\\xbc\\xce\\xbf\\xe1\\xbf\\xa6', '\\xcf\\x84\\xcf\\x8c', '\\xce\\xba\\xce\\xb1\\xce\\xba\\xcf\\x8c\\xce\\xbd', '\\xe1\\xbc\\x80\\xce\\xbb\\xce\\xbb\\xce\\xac', '\\xcf\\x84\\xcf\\x8c', '\\xe1\\xbc\\x80\\xce\\xb3\\xce\\xb1\\xce\\xb8\\xcf\\x8c\\xce\\xbd', '\\xe1\\xbd\\x81', '\\xe1\\xbc\\x80\\xce\\xb3\\xce\\xb1\\xce\\xb8\\xce\\xbf\\xcf\\x80\\xce\\xbf\\xce\\xb9\\xe1\\xbf\\xb6\\xce\\xbd', '\\xe1\\xbc\\x90\\xce\\xba', '\\xcf\\x84\\xce\\xbf\\xe1\\xbf\\xa6', '\\xce\\xb8\\xce\\xb5\\xce\\xbf\\xe1\\xbf\\xa6', '\\xe1\\xbc\\x90\\xcf\\x83\\xcf\\x84\\xce\\xaf(\\xce\\xbd)', '\\xe1\\xbd\\x81', '\\xce\\xba\\xce\\xb1\\xce\\xba\\xce\\xbf\\xcf\\x80\\xce\\xbf\\xce\\xb9\\xe1\\xbf\\xb6\\xce\\xbd', '\\xce\\xbf\\xe1\\xbd\\x90', '\\xe1\\xbc\\x91\\xcf\\x8e\\xcf\\x81\\xce\\xb1\\xce\\xba\\xce\\xb5(\\xce\\xbd)', '\\xcf\\x84\\xcf\\x8c\\xce\\xbd', '\\xce\\xb8\\xce\\xb5\\xcf\\x8c\\xce\\xbd']}\n"
       ]
      }
     ],
     "prompt_number": 33
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "OK, so now we have a dictionary that has as its keys the `bcv` numbers for every verse in the book and as its values lists of the words in each of the verses.  Now the job is to split the keys into book, chapter, and verse and use this information to build our biblical dictionary that will be in the form that we saw above.\n",
      "\n",
      "First, let's build a function that takes as input the key-value pair from a dictionary and returns three objects: the chapter number (as `int`), the verse number (as `int`), the the list of words."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def chapter_verse(key):\n",
      "    #insert your code here\n",
      "    return int(key[2:4]), int(key[4:])\n",
      "\n",
      "print(chapter_verse('250101') == (1, 1))\n",
      "print(chapter_verse('012025') == (20, 25))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "True\n",
        "True\n"
       ]
      }
     ],
     "prompt_number": 34
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The final step in building the dictionary for each book is to put everything together into a function that takes a filename as input and returns a full dictionary for the biblical book represented by that file with each value in the form {chapter: {verse: [words]}}.\n",
      "\n",
      "N.B.: Use the `read_file_lines`, `build_verses`, and `chapter_verse` functions above.\n",
      "\n",
      "If you are getting a `KeyError` in your code, look closely at the error to see if you can figure out how to fix it.  Remember, you can't add a sub-key to a key if the key itself does not yet exist in your dictionary."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def build_book_dict(file):\n",
      "    #insert your code here\n",
      "    book_dict = {}\n",
      "    lines = read_file_lines(file)\n",
      "    bcv_dict = build_verses(lines)\n",
      "    for key, value in bcv_dict.items():\n",
      "        chapter, verse = chapter_verse(key)\n",
      "        if chapter in book_dict.keys():\n",
      "            book_dict[chapter][verse] = value\n",
      "        else:\n",
      "            book_dict[chapter] = {verse: value}\n",
      "    return book_dict\n",
      "\n",
      "Third_Jn_final = build_book_dict('data/NT/Raw/85-3Jn-morphgnt.txt')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 35
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(Third_Jn_final)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "{1: {1: ['\\xe1\\xbd\\x81', '\\xcf\\x80\\xcf\\x81\\xce\\xb5\\xcf\\x83\\xce\\xb2\\xcf\\x8d\\xcf\\x84\\xce\\xb5\\xcf\\x81\\xce\\xbf\\xcf\\x82', '\\xce\\x93\\xce\\xb1\\xce\\x90\\xe1\\xbf\\xb3', '\\xcf\\x84\\xe1\\xbf\\xb7', '\\xe1\\xbc\\x80\\xce\\xb3\\xce\\xb1\\xcf\\x80\\xce\\xb7\\xcf\\x84\\xe1\\xbf\\xb7', '\\xe1\\xbd\\x85\\xce\\xbd', '\\xe1\\xbc\\x90\\xce\\xb3\\xcf\\x8e', '\\xe1\\xbc\\x80\\xce\\xb3\\xce\\xb1\\xcf\\x80\\xe1\\xbf\\xb6', '\\xe1\\xbc\\x90\\xce\\xbd', '\\xe1\\xbc\\x80\\xce\\xbb\\xce\\xb7\\xce\\xb8\\xce\\xb5\\xce\\xaf\\xe1\\xbe\\xb3'], 2: ['\\xe1\\xbc\\x80\\xce\\xb3\\xce\\xb1\\xcf\\x80\\xce\\xb7\\xcf\\x84\\xce\\xad', '\\xcf\\x80\\xce\\xb5\\xcf\\x81\\xce\\xaf', '\\xcf\\x80\\xce\\xac\\xce\\xbd\\xcf\\x84\\xcf\\x89\\xce\\xbd', '\\xce\\xb5\\xe1\\xbd\\x94\\xcf\\x87\\xce\\xbf\\xce\\xbc\\xce\\xb1\\xce\\xb9', '\\xcf\\x83\\xce\\xb5', '\\xce\\xb5\\xe1\\xbd\\x90\\xce\\xbf\\xce\\xb4\\xce\\xbf\\xe1\\xbf\\xa6\\xcf\\x83\\xce\\xb8\\xce\\xb1\\xce\\xb9', '\\xce\\xba\\xce\\xb1\\xce\\xaf', '\\xe1\\xbd\\x91\\xce\\xb3\\xce\\xb9\\xce\\xb1\\xce\\xaf\\xce\\xbd\\xce\\xb5\\xce\\xb9\\xce\\xbd', '\\xce\\xba\\xce\\xb1\\xce\\xb8\\xcf\\x8e\\xcf\\x82', '\\xce\\xb5\\xe1\\xbd\\x90\\xce\\xbf\\xce\\xb4\\xce\\xbf\\xe1\\xbf\\xa6\\xcf\\x84\\xce\\xb1\\xce\\xb9', '\\xcf\\x83\\xce\\xbf\\xcf\\x85', '\\xe1\\xbc\\xa1', '\\xcf\\x88\\xcf\\x85\\xcf\\x87\\xce\\xae'], 3: ['\\xe1\\xbc\\x90\\xcf\\x87\\xce\\xac\\xcf\\x81\\xce\\xb7\\xce\\xbd', '\\xce\\xb3\\xce\\xac\\xcf\\x81', '\\xce\\xbb\\xce\\xaf\\xce\\xb1\\xce\\xbd', '\\xe1\\xbc\\x90\\xcf\\x81\\xcf\\x87\\xce\\xbf\\xce\\xbc\\xce\\xad\\xce\\xbd\\xcf\\x89\\xce\\xbd', '\\xe1\\xbc\\x80\\xce\\xb4\\xce\\xb5\\xce\\xbb\\xcf\\x86\\xe1\\xbf\\xb6\\xce\\xbd', '\\xce\\xba\\xce\\xb1\\xce\\xaf', '\\xce\\xbc\\xce\\xb1\\xcf\\x81\\xcf\\x84\\xcf\\x85\\xcf\\x81\\xce\\xbf\\xcf\\x8d\\xce\\xbd\\xcf\\x84\\xcf\\x89\\xce\\xbd', '\\xcf\\x83\\xce\\xbf\\xcf\\x85', '\\xcf\\x84\\xe1\\xbf\\x87', '\\xe1\\xbc\\x80\\xce\\xbb\\xce\\xb7\\xce\\xb8\\xce\\xb5\\xce\\xaf\\xe1\\xbe\\xb3', '\\xce\\xba\\xce\\xb1\\xce\\xb8\\xcf\\x8e\\xcf\\x82', '\\xcf\\x83\\xcf\\x8d', '\\xe1\\xbc\\x90\\xce\\xbd', '\\xe1\\xbc\\x80\\xce\\xbb\\xce\\xb7\\xce\\xb8\\xce\\xb5\\xce\\xaf\\xe1\\xbe\\xb3', '\\xcf\\x80\\xce\\xb5\\xcf\\x81\\xce\\xb9\\xcf\\x80\\xce\\xb1\\xcf\\x84\\xce\\xb5\\xe1\\xbf\\x96\\xcf\\x82'], 4: ['\\xce\\xbc\\xce\\xb5\\xce\\xb9\\xce\\xb6\\xce\\xbf\\xcf\\x84\\xce\\xad\\xcf\\x81\\xce\\xb1\\xce\\xbd', '\\xcf\\x84\\xce\\xbf\\xcf\\x8d\\xcf\\x84\\xcf\\x89\\xce\\xbd', '\\xce\\xbf\\xe1\\xbd\\x90', '\\xe1\\xbc\\x94\\xcf\\x87\\xcf\\x89', '\\xcf\\x87\\xce\\xb1\\xcf\\x81\\xce\\xac\\xce\\xbd', '\\xe1\\xbc\\xb5\\xce\\xbd\\xce\\xb1', '\\xe1\\xbc\\x80\\xce\\xba\\xce\\xbf\\xcf\\x8d\\xcf\\x89', '\\xcf\\x84\\xce\\xac', '\\xe1\\xbc\\x90\\xce\\xbc\\xce\\xac', '\\xcf\\x84\\xce\\xad\\xce\\xba\\xce\\xbd\\xce\\xb1', '\\xe1\\xbc\\x90\\xce\\xbd', '\\xcf\\x84\\xe1\\xbf\\x87', '\\xe1\\xbc\\x80\\xce\\xbb\\xce\\xb7\\xce\\xb8\\xce\\xb5\\xce\\xaf\\xe1\\xbe\\xb3', '\\xcf\\x80\\xce\\xb5\\xcf\\x81\\xce\\xb9\\xcf\\x80\\xce\\xb1\\xcf\\x84\\xce\\xbf\\xe1\\xbf\\xa6\\xce\\xbd\\xcf\\x84\\xce\\xb1'], 5: ['\\xe1\\xbc\\x80\\xce\\xb3\\xce\\xb1\\xcf\\x80\\xce\\xb7\\xcf\\x84\\xce\\xad', '\\xcf\\x80\\xce\\xb9\\xcf\\x83\\xcf\\x84\\xcf\\x8c\\xce\\xbd', '\\xcf\\x80\\xce\\xbf\\xce\\xb9\\xce\\xb5\\xe1\\xbf\\x96\\xcf\\x82', '\\xe1\\xbd\\x85', '\\xe1\\xbc\\x90\\xce\\xac\\xce\\xbd', '\\xe1\\xbc\\x90\\xcf\\x81\\xce\\xb3\\xce\\xac\\xcf\\x83\\xe1\\xbf\\x83', '\\xce\\xb5\\xe1\\xbc\\xb0\\xcf\\x82', '\\xcf\\x84\\xce\\xbf\\xcf\\x8d\\xcf\\x82', '\\xe1\\xbc\\x80\\xce\\xb4\\xce\\xb5\\xce\\xbb\\xcf\\x86\\xce\\xbf\\xcf\\x8d\\xcf\\x82', '\\xce\\xba\\xce\\xb1\\xce\\xaf', '\\xcf\\x84\\xce\\xbf\\xe1\\xbf\\xa6\\xcf\\x84\\xce\\xbf', '\\xce\\xbe\\xce\\xad\\xce\\xbd\\xce\\xbf\\xcf\\x85\\xcf\\x82'], 6: ['\\xce\\xbf\\xe1\\xbc\\xb5', '\\xe1\\xbc\\x90\\xce\\xbc\\xce\\xb1\\xcf\\x81\\xcf\\x84\\xcf\\x8d\\xcf\\x81\\xce\\xb7\\xcf\\x83\\xce\\xb1\\xce\\xbd', '\\xcf\\x83\\xce\\xbf\\xcf\\x85', '\\xcf\\x84\\xe1\\xbf\\x87', '\\xe1\\xbc\\x80\\xce\\xb3\\xce\\xac\\xcf\\x80\\xe1\\xbf\\x83', '\\xe1\\xbc\\x90\\xce\\xbd\\xcf\\x8e\\xcf\\x80\\xce\\xb9\\xce\\xbf\\xce\\xbd', '\\xe1\\xbc\\x90\\xce\\xba\\xce\\xba\\xce\\xbb\\xce\\xb7\\xcf\\x83\\xce\\xaf\\xce\\xb1\\xcf\\x82', '\\xce\\xbf\\xe1\\xbd\\x95\\xcf\\x82', '\\xce\\xba\\xce\\xb1\\xce\\xbb\\xe1\\xbf\\xb6\\xcf\\x82', '\\xcf\\x80\\xce\\xbf\\xce\\xb9\\xce\\xae\\xcf\\x83\\xce\\xb5\\xce\\xb9\\xcf\\x82', '\\xcf\\x80\\xcf\\x81\\xce\\xbf\\xcf\\x80\\xce\\xad\\xce\\xbc\\xcf\\x88\\xce\\xb1\\xcf\\x82', '\\xe1\\xbc\\x80\\xce\\xbe\\xce\\xaf\\xcf\\x89\\xcf\\x82', '\\xcf\\x84\\xce\\xbf\\xe1\\xbf\\xa6', '\\xce\\xb8\\xce\\xb5\\xce\\xbf\\xe1\\xbf\\xa6'], 7: ['\\xe1\\xbd\\x91\\xcf\\x80\\xce\\xad\\xcf\\x81', '\\xce\\xb3\\xce\\xac\\xcf\\x81', '\\xcf\\x84\\xce\\xbf\\xe1\\xbf\\xa6', '\\xe1\\xbd\\x80\\xce\\xbd\\xcf\\x8c\\xce\\xbc\\xce\\xb1\\xcf\\x84\\xce\\xbf\\xcf\\x82', '\\xe1\\xbc\\x90\\xce\\xbe\\xe1\\xbf\\x86\\xce\\xbb\\xce\\xb8\\xce\\xbf\\xce\\xbd', '\\xce\\xbc\\xce\\xb7\\xce\\xb4\\xce\\xad\\xce\\xbd', '\\xce\\xbb\\xce\\xb1\\xce\\xbc\\xce\\xb2\\xce\\xac\\xce\\xbd\\xce\\xbf\\xce\\xbd\\xcf\\x84\\xce\\xb5\\xcf\\x82', '\\xe1\\xbc\\x80\\xcf\\x80\\xcf\\x8c', '\\xcf\\x84\\xe1\\xbf\\xb6\\xce\\xbd', '\\xe1\\xbc\\x90\\xce\\xb8\\xce\\xbd\\xce\\xb9\\xce\\xba\\xe1\\xbf\\xb6\\xce\\xbd'], 8: ['\\xe1\\xbc\\xa1\\xce\\xbc\\xce\\xb5\\xe1\\xbf\\x96\\xcf\\x82', '\\xce\\xbf\\xe1\\xbd\\x96\\xce\\xbd', '\\xe1\\xbd\\x80\\xcf\\x86\\xce\\xb5\\xce\\xaf\\xce\\xbb\\xce\\xbf\\xce\\xbc\\xce\\xb5\\xce\\xbd', '\\xe1\\xbd\\x91\\xcf\\x80\\xce\\xbf\\xce\\xbb\\xce\\xb1\\xce\\xbc\\xce\\xb2\\xce\\xac\\xce\\xbd\\xce\\xb5\\xce\\xb9\\xce\\xbd', '\\xcf\\x84\\xce\\xbf\\xcf\\x8d\\xcf\\x82', '\\xcf\\x84\\xce\\xbf\\xce\\xb9\\xce\\xbf\\xcf\\x8d\\xcf\\x84\\xce\\xbf\\xcf\\x85\\xcf\\x82', '\\xe1\\xbc\\xb5\\xce\\xbd\\xce\\xb1', '\\xcf\\x83\\xcf\\x85\\xce\\xbd\\xce\\xb5\\xcf\\x81\\xce\\xb3\\xce\\xbf\\xce\\xaf', '\\xce\\xb3\\xce\\xb9\\xce\\xbd\\xcf\\x8e\\xce\\xbc\\xce\\xb5\\xce\\xb8\\xce\\xb1', '\\xcf\\x84\\xe1\\xbf\\x87', '\\xe1\\xbc\\x80\\xce\\xbb\\xce\\xb7\\xce\\xb8\\xce\\xb5\\xce\\xaf\\xe1\\xbe\\xb3'], 9: ['\\xe1\\xbc\\x94\\xce\\xb3\\xcf\\x81\\xce\\xb1\\xcf\\x88\\xce\\xb1', '\\xcf\\x84\\xce\\xb9', '\\xcf\\x84\\xe1\\xbf\\x87', '\\xe1\\xbc\\x90\\xce\\xba\\xce\\xba\\xce\\xbb\\xce\\xb7\\xcf\\x83\\xce\\xaf\\xe1\\xbe\\xb3', '\\xe1\\xbc\\x80\\xce\\xbb\\xce\\xbb\\xce\\xac', '\\xe1\\xbd\\x81', '\\xcf\\x86\\xce\\xb9\\xce\\xbb\\xce\\xbf\\xcf\\x80\\xcf\\x81\\xcf\\x89\\xcf\\x84\\xce\\xb5\\xcf\\x8d\\xcf\\x89\\xce\\xbd', '\\xce\\xb1\\xe1\\xbd\\x90\\xcf\\x84\\xe1\\xbf\\xb6\\xce\\xbd', '\\xce\\x94\\xce\\xb9\\xce\\xbf\\xcf\\x84\\xcf\\x81\\xce\\xad\\xcf\\x86\\xce\\xb7\\xcf\\x82', '\\xce\\xbf\\xe1\\xbd\\x90', '\\xe1\\xbc\\x90\\xcf\\x80\\xce\\xb9\\xce\\xb4\\xce\\xad\\xcf\\x87\\xce\\xb5\\xcf\\x84\\xce\\xb1\\xce\\xb9', '\\xe1\\xbc\\xa1\\xce\\xbc\\xe1\\xbe\\xb6\\xcf\\x82'], 10: ['\\xce\\xb4\\xce\\xb9\\xce\\xac', '\\xcf\\x84\\xce\\xbf\\xe1\\xbf\\xa6\\xcf\\x84\\xce\\xbf', '\\xe1\\xbc\\x90\\xce\\xac\\xce\\xbd', '\\xe1\\xbc\\x94\\xce\\xbb\\xce\\xb8\\xcf\\x89', '\\xe1\\xbd\\x91\\xcf\\x80\\xce\\xbf\\xce\\xbc\\xce\\xbd\\xce\\xae\\xcf\\x83\\xcf\\x89', '\\xce\\xb1\\xe1\\xbd\\x90\\xcf\\x84\\xce\\xbf\\xe1\\xbf\\xa6', '\\xcf\\x84\\xce\\xac', '\\xe1\\xbc\\x94\\xcf\\x81\\xce\\xb3\\xce\\xb1', '\\xe1\\xbc\\x85', '\\xcf\\x80\\xce\\xbf\\xce\\xb9\\xce\\xb5\\xe1\\xbf\\x96', '\\xce\\xbb\\xcf\\x8c\\xce\\xb3\\xce\\xbf\\xce\\xb9\\xcf\\x82', '\\xcf\\x80\\xce\\xbf\\xce\\xbd\\xce\\xb7\\xcf\\x81\\xce\\xbf\\xe1\\xbf\\x96\\xcf\\x82', '\\xcf\\x86\\xce\\xbb\\xcf\\x85\\xce\\xb1\\xcf\\x81\\xe1\\xbf\\xb6\\xce\\xbd', '\\xe1\\xbc\\xa1\\xce\\xbc\\xe1\\xbe\\xb6\\xcf\\x82', '\\xce\\xba\\xce\\xb1\\xce\\xaf', '\\xce\\xbc\\xce\\xae', '\\xe1\\xbc\\x80\\xcf\\x81\\xce\\xba\\xce\\xbf\\xcf\\x8d\\xce\\xbc\\xce\\xb5\\xce\\xbd\\xce\\xbf\\xcf\\x82', '\\xe1\\xbc\\x90\\xcf\\x80\\xce\\xaf', '\\xcf\\x84\\xce\\xbf\\xcf\\x8d\\xcf\\x84\\xce\\xbf\\xce\\xb9\\xcf\\x82', '\\xce\\xbf\\xe1\\xbd\\x94\\xcf\\x84\\xce\\xb5', '\\xce\\xb1\\xe1\\xbd\\x90\\xcf\\x84\\xcf\\x8c\\xcf\\x82', '\\xe1\\xbc\\x90\\xcf\\x80\\xce\\xb9\\xce\\xb4\\xce\\xad\\xcf\\x87\\xce\\xb5\\xcf\\x84\\xce\\xb1\\xce\\xb9', '\\xcf\\x84\\xce\\xbf\\xcf\\x8d\\xcf\\x82', '\\xe1\\xbc\\x80\\xce\\xb4\\xce\\xb5\\xce\\xbb\\xcf\\x86\\xce\\xbf\\xcf\\x8d\\xcf\\x82', '\\xce\\xba\\xce\\xb1\\xce\\xaf', '\\xcf\\x84\\xce\\xbf\\xcf\\x8d\\xcf\\x82', '\\xce\\xb2\\xce\\xbf\\xcf\\x85\\xce\\xbb\\xce\\xbf\\xce\\xbc\\xce\\xad\\xce\\xbd\\xce\\xbf\\xcf\\x85\\xcf\\x82', '\\xce\\xba\\xcf\\x89\\xce\\xbb\\xcf\\x8d\\xce\\xb5\\xce\\xb9', '\\xce\\xba\\xce\\xb1\\xce\\xaf', '\\xe1\\xbc\\x90\\xce\\xba', '\\xcf\\x84\\xe1\\xbf\\x86\\xcf\\x82', '\\xe1\\xbc\\x90\\xce\\xba\\xce\\xba\\xce\\xbb\\xce\\xb7\\xcf\\x83\\xce\\xaf\\xce\\xb1\\xcf\\x82', '\\xe1\\xbc\\x90\\xce\\xba\\xce\\xb2\\xce\\xac\\xce\\xbb\\xce\\xbb\\xce\\xb5\\xce\\xb9'], 11: ['\\xe1\\xbc\\x80\\xce\\xb3\\xce\\xb1\\xcf\\x80\\xce\\xb7\\xcf\\x84\\xce\\xad', '\\xce\\xbc\\xce\\xae', '\\xce\\xbc\\xce\\xb9\\xce\\xbc\\xce\\xbf\\xe1\\xbf\\xa6', '\\xcf\\x84\\xcf\\x8c', '\\xce\\xba\\xce\\xb1\\xce\\xba\\xcf\\x8c\\xce\\xbd', '\\xe1\\xbc\\x80\\xce\\xbb\\xce\\xbb\\xce\\xac', '\\xcf\\x84\\xcf\\x8c', '\\xe1\\xbc\\x80\\xce\\xb3\\xce\\xb1\\xce\\xb8\\xcf\\x8c\\xce\\xbd', '\\xe1\\xbd\\x81', '\\xe1\\xbc\\x80\\xce\\xb3\\xce\\xb1\\xce\\xb8\\xce\\xbf\\xcf\\x80\\xce\\xbf\\xce\\xb9\\xe1\\xbf\\xb6\\xce\\xbd', '\\xe1\\xbc\\x90\\xce\\xba', '\\xcf\\x84\\xce\\xbf\\xe1\\xbf\\xa6', '\\xce\\xb8\\xce\\xb5\\xce\\xbf\\xe1\\xbf\\xa6', '\\xe1\\xbc\\x90\\xcf\\x83\\xcf\\x84\\xce\\xaf(\\xce\\xbd)', '\\xe1\\xbd\\x81', '\\xce\\xba\\xce\\xb1\\xce\\xba\\xce\\xbf\\xcf\\x80\\xce\\xbf\\xce\\xb9\\xe1\\xbf\\xb6\\xce\\xbd', '\\xce\\xbf\\xe1\\xbd\\x90', '\\xe1\\xbc\\x91\\xcf\\x8e\\xcf\\x81\\xce\\xb1\\xce\\xba\\xce\\xb5(\\xce\\xbd)', '\\xcf\\x84\\xcf\\x8c\\xce\\xbd', '\\xce\\xb8\\xce\\xb5\\xcf\\x8c\\xce\\xbd'], 12: ['\\xce\\x94\\xce\\xb7\\xce\\xbc\\xce\\xb7\\xcf\\x84\\xcf\\x81\\xce\\xaf\\xe1\\xbf\\xb3', '\\xce\\xbc\\xce\\xb5\\xce\\xbc\\xce\\xb1\\xcf\\x81\\xcf\\x84\\xcf\\x8d\\xcf\\x81\\xce\\xb7\\xcf\\x84\\xce\\xb1\\xce\\xb9', '\\xe1\\xbd\\x91\\xcf\\x80\\xcf\\x8c', '\\xcf\\x80\\xce\\xac\\xce\\xbd\\xcf\\x84\\xcf\\x89\\xce\\xbd', '\\xce\\xba\\xce\\xb1\\xce\\xaf', '\\xe1\\xbd\\x91\\xcf\\x80\\xcf\\x8c', '\\xce\\xb1\\xe1\\xbd\\x90\\xcf\\x84\\xe1\\xbf\\x86\\xcf\\x82', '\\xcf\\x84\\xe1\\xbf\\x86\\xcf\\x82', '\\xe1\\xbc\\x80\\xce\\xbb\\xce\\xb7\\xce\\xb8\\xce\\xb5\\xce\\xaf\\xce\\xb1\\xcf\\x82', '\\xce\\xba\\xce\\xb1\\xce\\xaf', '\\xe1\\xbc\\xa1\\xce\\xbc\\xce\\xb5\\xe1\\xbf\\x96\\xcf\\x82', '\\xce\\xb4\\xce\\xad', '\\xce\\xbc\\xce\\xb1\\xcf\\x81\\xcf\\x84\\xcf\\x85\\xcf\\x81\\xce\\xbf\\xe1\\xbf\\xa6\\xce\\xbc\\xce\\xb5\\xce\\xbd', '\\xce\\xba\\xce\\xb1\\xce\\xaf', '\\xce\\xbf\\xe1\\xbc\\xb6\\xce\\xb4\\xce\\xb1\\xcf\\x82', '\\xe1\\xbd\\x85\\xcf\\x84\\xce\\xb9', '\\xe1\\xbc\\xa1', '\\xce\\xbc\\xce\\xb1\\xcf\\x81\\xcf\\x84\\xcf\\x85\\xcf\\x81\\xce\\xaf\\xce\\xb1', '\\xe1\\xbc\\xa1\\xce\\xbc\\xe1\\xbf\\xb6\\xce\\xbd', '\\xe1\\xbc\\x80\\xce\\xbb\\xce\\xb7\\xce\\xb8\\xce\\xae\\xcf\\x82', '\\xe1\\xbc\\x90\\xcf\\x83\\xcf\\x84\\xce\\xaf(\\xce\\xbd)'], 13: ['\\xcf\\x80\\xce\\xbf\\xce\\xbb\\xce\\xbb\\xce\\xac', '\\xce\\xb5\\xe1\\xbc\\xb6\\xcf\\x87\\xce\\xbf\\xce\\xbd', '\\xce\\xb3\\xcf\\x81\\xce\\xac\\xcf\\x88\\xce\\xb1\\xce\\xb9', '\\xcf\\x83\\xce\\xbf\\xce\\xb9', '\\xe1\\xbc\\x80\\xce\\xbb\\xce\\xbb\\xce\\xac', '\\xce\\xbf\\xe1\\xbd\\x90', '\\xce\\xb8\\xce\\xad\\xce\\xbb\\xcf\\x89', '\\xce\\xb4\\xce\\xb9\\xce\\xac', '\\xce\\xbc\\xce\\xad\\xce\\xbb\\xce\\xb1\\xce\\xbd\\xce\\xbf\\xcf\\x82', '\\xce\\xba\\xce\\xb1\\xce\\xaf', '\\xce\\xba\\xce\\xb1\\xce\\xbb\\xce\\xac\\xce\\xbc\\xce\\xbf\\xcf\\x85', '\\xcf\\x83\\xce\\xbf\\xce\\xb9', '\\xce\\xb3\\xcf\\x81\\xce\\xac\\xcf\\x86\\xce\\xb5\\xce\\xb9\\xce\\xbd'], 14: ['\\xe1\\xbc\\x90\\xce\\xbb\\xcf\\x80\\xce\\xaf\\xce\\xb6\\xcf\\x89', '\\xce\\xb4\\xce\\xad', '\\xce\\xb5\\xe1\\xbd\\x90\\xce\\xb8\\xce\\xad\\xcf\\x89\\xcf\\x82', '\\xcf\\x83\\xce\\xb5', '\\xe1\\xbc\\xb0\\xce\\xb4\\xce\\xb5\\xe1\\xbf\\x96\\xce\\xbd', '\\xce\\xba\\xce\\xb1\\xce\\xaf', '\\xcf\\x83\\xcf\\x84\\xcf\\x8c\\xce\\xbc\\xce\\xb1', '\\xcf\\x80\\xcf\\x81\\xcf\\x8c\\xcf\\x82', '\\xcf\\x83\\xcf\\x84\\xcf\\x8c\\xce\\xbc\\xce\\xb1', '\\xce\\xbb\\xce\\xb1\\xce\\xbb\\xce\\xae\\xcf\\x83\\xce\\xbf\\xce\\xbc\\xce\\xb5\\xce\\xbd'], 15: ['\\xce\\xb5\\xe1\\xbc\\xb0\\xcf\\x81\\xce\\xae\\xce\\xbd\\xce\\xb7', '\\xcf\\x83\\xce\\xbf\\xce\\xb9', '\\xe1\\xbc\\x80\\xcf\\x83\\xcf\\x80\\xce\\xac\\xce\\xb6\\xce\\xbf\\xce\\xbd\\xcf\\x84\\xce\\xb1\\xce\\xb9', '\\xcf\\x83\\xce\\xb5', '\\xce\\xbf\\xe1\\xbc\\xb1', '\\xcf\\x86\\xce\\xaf\\xce\\xbb\\xce\\xbf\\xce\\xb9', '\\xe1\\xbc\\x80\\xcf\\x83\\xcf\\x80\\xce\\xac\\xce\\xb6\\xce\\xbf\\xcf\\x85', '\\xcf\\x84\\xce\\xbf\\xcf\\x8d\\xcf\\x82', '\\xcf\\x86\\xce\\xaf\\xce\\xbb\\xce\\xbf\\xcf\\x85\\xcf\\x82', '\\xce\\xba\\xce\\xb1\\xcf\\x84\\xce\\xac', '\\xe1\\xbd\\x84\\xce\\xbd\\xce\\xbf\\xce\\xbc\\xce\\xb1']}}\n"
       ]
      }
     ],
     "prompt_number": 36
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "And now let's add our final wrapper function that will take as input a directory where our text files are located and will use the `list_textfiles` function to get a list of filenames from this directory, will call the `build_book_dict` function on every file, and then will assign the results of this to a new dictionary that has as its keys the book names.  The results will be a complete dictionary object with all the book, chapters, verses, and words from the New Testament."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def testament_dict_builder(directory):\n",
      "    testament_dict = {}\n",
      "    #insert your code here\n",
      "    files = list_textfiles(directory)\n",
      "    for f in files:\n",
      "        testament_dict[f.split('-')[1]] = build_book_dict(f)\n",
      "    return testament_dict\n",
      "\n",
      "NT_dict = testament_dict_builder('data/NT/Raw/')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 37
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(NT_dict.keys())\n",
      "print(NT_dict['3Jn'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['Tit', 'Lk', 'Re', '3Jn', 'Heb', 'Ro', 'Phm', '2Ti', '2Th', '1Pe', 'Jn', '2Pe', 'Php', '1Th', '1Ti', 'Mk', 'Jas', 'Mt', '2Co', 'Eph', '1Co', 'Col', 'Ac', 'Jud', '1Jn', 'Ga', '2Jn']\n",
        "{1: {1: ['\\xe1\\xbd\\x81', '\\xcf\\x80\\xcf\\x81\\xce\\xb5\\xcf\\x83\\xce\\xb2\\xcf\\x8d\\xcf\\x84\\xce\\xb5\\xcf\\x81\\xce\\xbf\\xcf\\x82', '\\xce\\x93\\xce\\xb1\\xce\\x90\\xe1\\xbf\\xb3', '\\xcf\\x84\\xe1\\xbf\\xb7', '\\xe1\\xbc\\x80\\xce\\xb3\\xce\\xb1\\xcf\\x80\\xce\\xb7\\xcf\\x84\\xe1\\xbf\\xb7', '\\xe1\\xbd\\x85\\xce\\xbd', '\\xe1\\xbc\\x90\\xce\\xb3\\xcf\\x8e', '\\xe1\\xbc\\x80\\xce\\xb3\\xce\\xb1\\xcf\\x80\\xe1\\xbf\\xb6', '\\xe1\\xbc\\x90\\xce\\xbd', '\\xe1\\xbc\\x80\\xce\\xbb\\xce\\xb7\\xce\\xb8\\xce\\xb5\\xce\\xaf\\xe1\\xbe\\xb3'], 2: ['\\xe1\\xbc\\x80\\xce\\xb3\\xce\\xb1\\xcf\\x80\\xce\\xb7\\xcf\\x84\\xce\\xad', '\\xcf\\x80\\xce\\xb5\\xcf\\x81\\xce\\xaf', '\\xcf\\x80\\xce\\xac\\xce\\xbd\\xcf\\x84\\xcf\\x89\\xce\\xbd', '\\xce\\xb5\\xe1\\xbd\\x94\\xcf\\x87\\xce\\xbf\\xce\\xbc\\xce\\xb1\\xce\\xb9', '\\xcf\\x83\\xce\\xb5', '\\xce\\xb5\\xe1\\xbd\\x90\\xce\\xbf\\xce\\xb4\\xce\\xbf\\xe1\\xbf\\xa6\\xcf\\x83\\xce\\xb8\\xce\\xb1\\xce\\xb9', '\\xce\\xba\\xce\\xb1\\xce\\xaf', '\\xe1\\xbd\\x91\\xce\\xb3\\xce\\xb9\\xce\\xb1\\xce\\xaf\\xce\\xbd\\xce\\xb5\\xce\\xb9\\xce\\xbd', '\\xce\\xba\\xce\\xb1\\xce\\xb8\\xcf\\x8e\\xcf\\x82', '\\xce\\xb5\\xe1\\xbd\\x90\\xce\\xbf\\xce\\xb4\\xce\\xbf\\xe1\\xbf\\xa6\\xcf\\x84\\xce\\xb1\\xce\\xb9', '\\xcf\\x83\\xce\\xbf\\xcf\\x85', '\\xe1\\xbc\\xa1', '\\xcf\\x88\\xcf\\x85\\xcf\\x87\\xce\\xae'], 3: ['\\xe1\\xbc\\x90\\xcf\\x87\\xce\\xac\\xcf\\x81\\xce\\xb7\\xce\\xbd', '\\xce\\xb3\\xce\\xac\\xcf\\x81', '\\xce\\xbb\\xce\\xaf\\xce\\xb1\\xce\\xbd', '\\xe1\\xbc\\x90\\xcf\\x81\\xcf\\x87\\xce\\xbf\\xce\\xbc\\xce\\xad\\xce\\xbd\\xcf\\x89\\xce\\xbd', '\\xe1\\xbc\\x80\\xce\\xb4\\xce\\xb5\\xce\\xbb\\xcf\\x86\\xe1\\xbf\\xb6\\xce\\xbd', '\\xce\\xba\\xce\\xb1\\xce\\xaf', '\\xce\\xbc\\xce\\xb1\\xcf\\x81\\xcf\\x84\\xcf\\x85\\xcf\\x81\\xce\\xbf\\xcf\\x8d\\xce\\xbd\\xcf\\x84\\xcf\\x89\\xce\\xbd', '\\xcf\\x83\\xce\\xbf\\xcf\\x85', '\\xcf\\x84\\xe1\\xbf\\x87', '\\xe1\\xbc\\x80\\xce\\xbb\\xce\\xb7\\xce\\xb8\\xce\\xb5\\xce\\xaf\\xe1\\xbe\\xb3', '\\xce\\xba\\xce\\xb1\\xce\\xb8\\xcf\\x8e\\xcf\\x82', '\\xcf\\x83\\xcf\\x8d', '\\xe1\\xbc\\x90\\xce\\xbd', '\\xe1\\xbc\\x80\\xce\\xbb\\xce\\xb7\\xce\\xb8\\xce\\xb5\\xce\\xaf\\xe1\\xbe\\xb3', '\\xcf\\x80\\xce\\xb5\\xcf\\x81\\xce\\xb9\\xcf\\x80\\xce\\xb1\\xcf\\x84\\xce\\xb5\\xe1\\xbf\\x96\\xcf\\x82'], 4: ['\\xce\\xbc\\xce\\xb5\\xce\\xb9\\xce\\xb6\\xce\\xbf\\xcf\\x84\\xce\\xad\\xcf\\x81\\xce\\xb1\\xce\\xbd', '\\xcf\\x84\\xce\\xbf\\xcf\\x8d\\xcf\\x84\\xcf\\x89\\xce\\xbd', '\\xce\\xbf\\xe1\\xbd\\x90', '\\xe1\\xbc\\x94\\xcf\\x87\\xcf\\x89', '\\xcf\\x87\\xce\\xb1\\xcf\\x81\\xce\\xac\\xce\\xbd', '\\xe1\\xbc\\xb5\\xce\\xbd\\xce\\xb1', '\\xe1\\xbc\\x80\\xce\\xba\\xce\\xbf\\xcf\\x8d\\xcf\\x89', '\\xcf\\x84\\xce\\xac', '\\xe1\\xbc\\x90\\xce\\xbc\\xce\\xac', '\\xcf\\x84\\xce\\xad\\xce\\xba\\xce\\xbd\\xce\\xb1', '\\xe1\\xbc\\x90\\xce\\xbd', '\\xcf\\x84\\xe1\\xbf\\x87', '\\xe1\\xbc\\x80\\xce\\xbb\\xce\\xb7\\xce\\xb8\\xce\\xb5\\xce\\xaf\\xe1\\xbe\\xb3', '\\xcf\\x80\\xce\\xb5\\xcf\\x81\\xce\\xb9\\xcf\\x80\\xce\\xb1\\xcf\\x84\\xce\\xbf\\xe1\\xbf\\xa6\\xce\\xbd\\xcf\\x84\\xce\\xb1'], 5: ['\\xe1\\xbc\\x80\\xce\\xb3\\xce\\xb1\\xcf\\x80\\xce\\xb7\\xcf\\x84\\xce\\xad', '\\xcf\\x80\\xce\\xb9\\xcf\\x83\\xcf\\x84\\xcf\\x8c\\xce\\xbd', '\\xcf\\x80\\xce\\xbf\\xce\\xb9\\xce\\xb5\\xe1\\xbf\\x96\\xcf\\x82', '\\xe1\\xbd\\x85', '\\xe1\\xbc\\x90\\xce\\xac\\xce\\xbd', '\\xe1\\xbc\\x90\\xcf\\x81\\xce\\xb3\\xce\\xac\\xcf\\x83\\xe1\\xbf\\x83', '\\xce\\xb5\\xe1\\xbc\\xb0\\xcf\\x82', '\\xcf\\x84\\xce\\xbf\\xcf\\x8d\\xcf\\x82', '\\xe1\\xbc\\x80\\xce\\xb4\\xce\\xb5\\xce\\xbb\\xcf\\x86\\xce\\xbf\\xcf\\x8d\\xcf\\x82', '\\xce\\xba\\xce\\xb1\\xce\\xaf', '\\xcf\\x84\\xce\\xbf\\xe1\\xbf\\xa6\\xcf\\x84\\xce\\xbf', '\\xce\\xbe\\xce\\xad\\xce\\xbd\\xce\\xbf\\xcf\\x85\\xcf\\x82'], 6: ['\\xce\\xbf\\xe1\\xbc\\xb5', '\\xe1\\xbc\\x90\\xce\\xbc\\xce\\xb1\\xcf\\x81\\xcf\\x84\\xcf\\x8d\\xcf\\x81\\xce\\xb7\\xcf\\x83\\xce\\xb1\\xce\\xbd', '\\xcf\\x83\\xce\\xbf\\xcf\\x85', '\\xcf\\x84\\xe1\\xbf\\x87', '\\xe1\\xbc\\x80\\xce\\xb3\\xce\\xac\\xcf\\x80\\xe1\\xbf\\x83', '\\xe1\\xbc\\x90\\xce\\xbd\\xcf\\x8e\\xcf\\x80\\xce\\xb9\\xce\\xbf\\xce\\xbd', '\\xe1\\xbc\\x90\\xce\\xba\\xce\\xba\\xce\\xbb\\xce\\xb7\\xcf\\x83\\xce\\xaf\\xce\\xb1\\xcf\\x82', '\\xce\\xbf\\xe1\\xbd\\x95\\xcf\\x82', '\\xce\\xba\\xce\\xb1\\xce\\xbb\\xe1\\xbf\\xb6\\xcf\\x82', '\\xcf\\x80\\xce\\xbf\\xce\\xb9\\xce\\xae\\xcf\\x83\\xce\\xb5\\xce\\xb9\\xcf\\x82', '\\xcf\\x80\\xcf\\x81\\xce\\xbf\\xcf\\x80\\xce\\xad\\xce\\xbc\\xcf\\x88\\xce\\xb1\\xcf\\x82', '\\xe1\\xbc\\x80\\xce\\xbe\\xce\\xaf\\xcf\\x89\\xcf\\x82', '\\xcf\\x84\\xce\\xbf\\xe1\\xbf\\xa6', '\\xce\\xb8\\xce\\xb5\\xce\\xbf\\xe1\\xbf\\xa6'], 7: ['\\xe1\\xbd\\x91\\xcf\\x80\\xce\\xad\\xcf\\x81', '\\xce\\xb3\\xce\\xac\\xcf\\x81', '\\xcf\\x84\\xce\\xbf\\xe1\\xbf\\xa6', '\\xe1\\xbd\\x80\\xce\\xbd\\xcf\\x8c\\xce\\xbc\\xce\\xb1\\xcf\\x84\\xce\\xbf\\xcf\\x82', '\\xe1\\xbc\\x90\\xce\\xbe\\xe1\\xbf\\x86\\xce\\xbb\\xce\\xb8\\xce\\xbf\\xce\\xbd', '\\xce\\xbc\\xce\\xb7\\xce\\xb4\\xce\\xad\\xce\\xbd', '\\xce\\xbb\\xce\\xb1\\xce\\xbc\\xce\\xb2\\xce\\xac\\xce\\xbd\\xce\\xbf\\xce\\xbd\\xcf\\x84\\xce\\xb5\\xcf\\x82', '\\xe1\\xbc\\x80\\xcf\\x80\\xcf\\x8c', '\\xcf\\x84\\xe1\\xbf\\xb6\\xce\\xbd', '\\xe1\\xbc\\x90\\xce\\xb8\\xce\\xbd\\xce\\xb9\\xce\\xba\\xe1\\xbf\\xb6\\xce\\xbd'], 8: ['\\xe1\\xbc\\xa1\\xce\\xbc\\xce\\xb5\\xe1\\xbf\\x96\\xcf\\x82', '\\xce\\xbf\\xe1\\xbd\\x96\\xce\\xbd', '\\xe1\\xbd\\x80\\xcf\\x86\\xce\\xb5\\xce\\xaf\\xce\\xbb\\xce\\xbf\\xce\\xbc\\xce\\xb5\\xce\\xbd', '\\xe1\\xbd\\x91\\xcf\\x80\\xce\\xbf\\xce\\xbb\\xce\\xb1\\xce\\xbc\\xce\\xb2\\xce\\xac\\xce\\xbd\\xce\\xb5\\xce\\xb9\\xce\\xbd', '\\xcf\\x84\\xce\\xbf\\xcf\\x8d\\xcf\\x82', '\\xcf\\x84\\xce\\xbf\\xce\\xb9\\xce\\xbf\\xcf\\x8d\\xcf\\x84\\xce\\xbf\\xcf\\x85\\xcf\\x82', '\\xe1\\xbc\\xb5\\xce\\xbd\\xce\\xb1', '\\xcf\\x83\\xcf\\x85\\xce\\xbd\\xce\\xb5\\xcf\\x81\\xce\\xb3\\xce\\xbf\\xce\\xaf', '\\xce\\xb3\\xce\\xb9\\xce\\xbd\\xcf\\x8e\\xce\\xbc\\xce\\xb5\\xce\\xb8\\xce\\xb1', '\\xcf\\x84\\xe1\\xbf\\x87', '\\xe1\\xbc\\x80\\xce\\xbb\\xce\\xb7\\xce\\xb8\\xce\\xb5\\xce\\xaf\\xe1\\xbe\\xb3'], 9: ['\\xe1\\xbc\\x94\\xce\\xb3\\xcf\\x81\\xce\\xb1\\xcf\\x88\\xce\\xb1', '\\xcf\\x84\\xce\\xb9', '\\xcf\\x84\\xe1\\xbf\\x87', '\\xe1\\xbc\\x90\\xce\\xba\\xce\\xba\\xce\\xbb\\xce\\xb7\\xcf\\x83\\xce\\xaf\\xe1\\xbe\\xb3', '\\xe1\\xbc\\x80\\xce\\xbb\\xce\\xbb\\xce\\xac', '\\xe1\\xbd\\x81', '\\xcf\\x86\\xce\\xb9\\xce\\xbb\\xce\\xbf\\xcf\\x80\\xcf\\x81\\xcf\\x89\\xcf\\x84\\xce\\xb5\\xcf\\x8d\\xcf\\x89\\xce\\xbd', '\\xce\\xb1\\xe1\\xbd\\x90\\xcf\\x84\\xe1\\xbf\\xb6\\xce\\xbd', '\\xce\\x94\\xce\\xb9\\xce\\xbf\\xcf\\x84\\xcf\\x81\\xce\\xad\\xcf\\x86\\xce\\xb7\\xcf\\x82', '\\xce\\xbf\\xe1\\xbd\\x90', '\\xe1\\xbc\\x90\\xcf\\x80\\xce\\xb9\\xce\\xb4\\xce\\xad\\xcf\\x87\\xce\\xb5\\xcf\\x84\\xce\\xb1\\xce\\xb9', '\\xe1\\xbc\\xa1\\xce\\xbc\\xe1\\xbe\\xb6\\xcf\\x82'], 10: ['\\xce\\xb4\\xce\\xb9\\xce\\xac', '\\xcf\\x84\\xce\\xbf\\xe1\\xbf\\xa6\\xcf\\x84\\xce\\xbf', '\\xe1\\xbc\\x90\\xce\\xac\\xce\\xbd', '\\xe1\\xbc\\x94\\xce\\xbb\\xce\\xb8\\xcf\\x89', '\\xe1\\xbd\\x91\\xcf\\x80\\xce\\xbf\\xce\\xbc\\xce\\xbd\\xce\\xae\\xcf\\x83\\xcf\\x89', '\\xce\\xb1\\xe1\\xbd\\x90\\xcf\\x84\\xce\\xbf\\xe1\\xbf\\xa6', '\\xcf\\x84\\xce\\xac', '\\xe1\\xbc\\x94\\xcf\\x81\\xce\\xb3\\xce\\xb1', '\\xe1\\xbc\\x85', '\\xcf\\x80\\xce\\xbf\\xce\\xb9\\xce\\xb5\\xe1\\xbf\\x96', '\\xce\\xbb\\xcf\\x8c\\xce\\xb3\\xce\\xbf\\xce\\xb9\\xcf\\x82', '\\xcf\\x80\\xce\\xbf\\xce\\xbd\\xce\\xb7\\xcf\\x81\\xce\\xbf\\xe1\\xbf\\x96\\xcf\\x82', '\\xcf\\x86\\xce\\xbb\\xcf\\x85\\xce\\xb1\\xcf\\x81\\xe1\\xbf\\xb6\\xce\\xbd', '\\xe1\\xbc\\xa1\\xce\\xbc\\xe1\\xbe\\xb6\\xcf\\x82', '\\xce\\xba\\xce\\xb1\\xce\\xaf', '\\xce\\xbc\\xce\\xae', '\\xe1\\xbc\\x80\\xcf\\x81\\xce\\xba\\xce\\xbf\\xcf\\x8d\\xce\\xbc\\xce\\xb5\\xce\\xbd\\xce\\xbf\\xcf\\x82', '\\xe1\\xbc\\x90\\xcf\\x80\\xce\\xaf', '\\xcf\\x84\\xce\\xbf\\xcf\\x8d\\xcf\\x84\\xce\\xbf\\xce\\xb9\\xcf\\x82', '\\xce\\xbf\\xe1\\xbd\\x94\\xcf\\x84\\xce\\xb5', '\\xce\\xb1\\xe1\\xbd\\x90\\xcf\\x84\\xcf\\x8c\\xcf\\x82', '\\xe1\\xbc\\x90\\xcf\\x80\\xce\\xb9\\xce\\xb4\\xce\\xad\\xcf\\x87\\xce\\xb5\\xcf\\x84\\xce\\xb1\\xce\\xb9', '\\xcf\\x84\\xce\\xbf\\xcf\\x8d\\xcf\\x82', '\\xe1\\xbc\\x80\\xce\\xb4\\xce\\xb5\\xce\\xbb\\xcf\\x86\\xce\\xbf\\xcf\\x8d\\xcf\\x82', '\\xce\\xba\\xce\\xb1\\xce\\xaf', '\\xcf\\x84\\xce\\xbf\\xcf\\x8d\\xcf\\x82', '\\xce\\xb2\\xce\\xbf\\xcf\\x85\\xce\\xbb\\xce\\xbf\\xce\\xbc\\xce\\xad\\xce\\xbd\\xce\\xbf\\xcf\\x85\\xcf\\x82', '\\xce\\xba\\xcf\\x89\\xce\\xbb\\xcf\\x8d\\xce\\xb5\\xce\\xb9', '\\xce\\xba\\xce\\xb1\\xce\\xaf', '\\xe1\\xbc\\x90\\xce\\xba', '\\xcf\\x84\\xe1\\xbf\\x86\\xcf\\x82', '\\xe1\\xbc\\x90\\xce\\xba\\xce\\xba\\xce\\xbb\\xce\\xb7\\xcf\\x83\\xce\\xaf\\xce\\xb1\\xcf\\x82', '\\xe1\\xbc\\x90\\xce\\xba\\xce\\xb2\\xce\\xac\\xce\\xbb\\xce\\xbb\\xce\\xb5\\xce\\xb9'], 11: ['\\xe1\\xbc\\x80\\xce\\xb3\\xce\\xb1\\xcf\\x80\\xce\\xb7\\xcf\\x84\\xce\\xad', '\\xce\\xbc\\xce\\xae', '\\xce\\xbc\\xce\\xb9\\xce\\xbc\\xce\\xbf\\xe1\\xbf\\xa6', '\\xcf\\x84\\xcf\\x8c', '\\xce\\xba\\xce\\xb1\\xce\\xba\\xcf\\x8c\\xce\\xbd', '\\xe1\\xbc\\x80\\xce\\xbb\\xce\\xbb\\xce\\xac', '\\xcf\\x84\\xcf\\x8c', '\\xe1\\xbc\\x80\\xce\\xb3\\xce\\xb1\\xce\\xb8\\xcf\\x8c\\xce\\xbd', '\\xe1\\xbd\\x81', '\\xe1\\xbc\\x80\\xce\\xb3\\xce\\xb1\\xce\\xb8\\xce\\xbf\\xcf\\x80\\xce\\xbf\\xce\\xb9\\xe1\\xbf\\xb6\\xce\\xbd', '\\xe1\\xbc\\x90\\xce\\xba', '\\xcf\\x84\\xce\\xbf\\xe1\\xbf\\xa6', '\\xce\\xb8\\xce\\xb5\\xce\\xbf\\xe1\\xbf\\xa6', '\\xe1\\xbc\\x90\\xcf\\x83\\xcf\\x84\\xce\\xaf(\\xce\\xbd)', '\\xe1\\xbd\\x81', '\\xce\\xba\\xce\\xb1\\xce\\xba\\xce\\xbf\\xcf\\x80\\xce\\xbf\\xce\\xb9\\xe1\\xbf\\xb6\\xce\\xbd', '\\xce\\xbf\\xe1\\xbd\\x90', '\\xe1\\xbc\\x91\\xcf\\x8e\\xcf\\x81\\xce\\xb1\\xce\\xba\\xce\\xb5(\\xce\\xbd)', '\\xcf\\x84\\xcf\\x8c\\xce\\xbd', '\\xce\\xb8\\xce\\xb5\\xcf\\x8c\\xce\\xbd'], 12: ['\\xce\\x94\\xce\\xb7\\xce\\xbc\\xce\\xb7\\xcf\\x84\\xcf\\x81\\xce\\xaf\\xe1\\xbf\\xb3', '\\xce\\xbc\\xce\\xb5\\xce\\xbc\\xce\\xb1\\xcf\\x81\\xcf\\x84\\xcf\\x8d\\xcf\\x81\\xce\\xb7\\xcf\\x84\\xce\\xb1\\xce\\xb9', '\\xe1\\xbd\\x91\\xcf\\x80\\xcf\\x8c', '\\xcf\\x80\\xce\\xac\\xce\\xbd\\xcf\\x84\\xcf\\x89\\xce\\xbd', '\\xce\\xba\\xce\\xb1\\xce\\xaf', '\\xe1\\xbd\\x91\\xcf\\x80\\xcf\\x8c', '\\xce\\xb1\\xe1\\xbd\\x90\\xcf\\x84\\xe1\\xbf\\x86\\xcf\\x82', '\\xcf\\x84\\xe1\\xbf\\x86\\xcf\\x82', '\\xe1\\xbc\\x80\\xce\\xbb\\xce\\xb7\\xce\\xb8\\xce\\xb5\\xce\\xaf\\xce\\xb1\\xcf\\x82', '\\xce\\xba\\xce\\xb1\\xce\\xaf', '\\xe1\\xbc\\xa1\\xce\\xbc\\xce\\xb5\\xe1\\xbf\\x96\\xcf\\x82', '\\xce\\xb4\\xce\\xad', '\\xce\\xbc\\xce\\xb1\\xcf\\x81\\xcf\\x84\\xcf\\x85\\xcf\\x81\\xce\\xbf\\xe1\\xbf\\xa6\\xce\\xbc\\xce\\xb5\\xce\\xbd', '\\xce\\xba\\xce\\xb1\\xce\\xaf', '\\xce\\xbf\\xe1\\xbc\\xb6\\xce\\xb4\\xce\\xb1\\xcf\\x82', '\\xe1\\xbd\\x85\\xcf\\x84\\xce\\xb9', '\\xe1\\xbc\\xa1', '\\xce\\xbc\\xce\\xb1\\xcf\\x81\\xcf\\x84\\xcf\\x85\\xcf\\x81\\xce\\xaf\\xce\\xb1', '\\xe1\\xbc\\xa1\\xce\\xbc\\xe1\\xbf\\xb6\\xce\\xbd', '\\xe1\\xbc\\x80\\xce\\xbb\\xce\\xb7\\xce\\xb8\\xce\\xae\\xcf\\x82', '\\xe1\\xbc\\x90\\xcf\\x83\\xcf\\x84\\xce\\xaf(\\xce\\xbd)'], 13: ['\\xcf\\x80\\xce\\xbf\\xce\\xbb\\xce\\xbb\\xce\\xac', '\\xce\\xb5\\xe1\\xbc\\xb6\\xcf\\x87\\xce\\xbf\\xce\\xbd', '\\xce\\xb3\\xcf\\x81\\xce\\xac\\xcf\\x88\\xce\\xb1\\xce\\xb9', '\\xcf\\x83\\xce\\xbf\\xce\\xb9', '\\xe1\\xbc\\x80\\xce\\xbb\\xce\\xbb\\xce\\xac', '\\xce\\xbf\\xe1\\xbd\\x90', '\\xce\\xb8\\xce\\xad\\xce\\xbb\\xcf\\x89', '\\xce\\xb4\\xce\\xb9\\xce\\xac', '\\xce\\xbc\\xce\\xad\\xce\\xbb\\xce\\xb1\\xce\\xbd\\xce\\xbf\\xcf\\x82', '\\xce\\xba\\xce\\xb1\\xce\\xaf', '\\xce\\xba\\xce\\xb1\\xce\\xbb\\xce\\xac\\xce\\xbc\\xce\\xbf\\xcf\\x85', '\\xcf\\x83\\xce\\xbf\\xce\\xb9', '\\xce\\xb3\\xcf\\x81\\xce\\xac\\xcf\\x86\\xce\\xb5\\xce\\xb9\\xce\\xbd'], 14: ['\\xe1\\xbc\\x90\\xce\\xbb\\xcf\\x80\\xce\\xaf\\xce\\xb6\\xcf\\x89', '\\xce\\xb4\\xce\\xad', '\\xce\\xb5\\xe1\\xbd\\x90\\xce\\xb8\\xce\\xad\\xcf\\x89\\xcf\\x82', '\\xcf\\x83\\xce\\xb5', '\\xe1\\xbc\\xb0\\xce\\xb4\\xce\\xb5\\xe1\\xbf\\x96\\xce\\xbd', '\\xce\\xba\\xce\\xb1\\xce\\xaf', '\\xcf\\x83\\xcf\\x84\\xcf\\x8c\\xce\\xbc\\xce\\xb1', '\\xcf\\x80\\xcf\\x81\\xcf\\x8c\\xcf\\x82', '\\xcf\\x83\\xcf\\x84\\xcf\\x8c\\xce\\xbc\\xce\\xb1', '\\xce\\xbb\\xce\\xb1\\xce\\xbb\\xce\\xae\\xcf\\x83\\xce\\xbf\\xce\\xbc\\xce\\xb5\\xce\\xbd'], 15: ['\\xce\\xb5\\xe1\\xbc\\xb0\\xcf\\x81\\xce\\xae\\xce\\xbd\\xce\\xb7', '\\xcf\\x83\\xce\\xbf\\xce\\xb9', '\\xe1\\xbc\\x80\\xcf\\x83\\xcf\\x80\\xce\\xac\\xce\\xb6\\xce\\xbf\\xce\\xbd\\xcf\\x84\\xce\\xb1\\xce\\xb9', '\\xcf\\x83\\xce\\xb5', '\\xce\\xbf\\xe1\\xbc\\xb1', '\\xcf\\x86\\xce\\xaf\\xce\\xbb\\xce\\xbf\\xce\\xb9', '\\xe1\\xbc\\x80\\xcf\\x83\\xcf\\x80\\xce\\xac\\xce\\xb6\\xce\\xbf\\xcf\\x85', '\\xcf\\x84\\xce\\xbf\\xcf\\x8d\\xcf\\x82', '\\xcf\\x86\\xce\\xaf\\xce\\xbb\\xce\\xbf\\xcf\\x85\\xcf\\x82', '\\xce\\xba\\xce\\xb1\\xcf\\x84\\xce\\xac', '\\xe1\\xbd\\x84\\xce\\xbd\\xce\\xbf\\xce\\xbc\\xce\\xb1']}}\n"
       ]
      }
     ],
     "prompt_number": 38
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So now we have a great, really useful dictionary of the New Testament.  And if you wanted to, e.g., build a dictionary only of word lemmata, you could do this very easily just by changing the word form that you extract from each line.\n",
      "\n",
      "But right now, we want to figure out how to save this so that we don't have to rebuild it every time.  We learned above about saving text files, and we could do that here as well.  But then, if we wanted to reuse it in Python, we would have to figure out how to rebuild the dictionary from the text file.  And that seems like too much trouble.\n",
      "\n",
      "Luckily, Python (and many other programming languages) have a way of dealing with this.  It is called _serialization_ and it allows you to save Python objects as Python objects so that you could, e.g., save a dictionary as a dictionary or a list as a list, etc.  Python's serialization mechanism is called `pickle` and it is quite easy to use.  To save an object as a file, you use `pickle.dump(object, file)`.  To load a pickled object back into Python, you use `pickle.load(file)`.\n",
      "\n",
      "Let's take a look at a short example."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#First import the pickle library\n",
      "import pickle\n",
      "#Then create an object, here a dictionary.\n",
      "d = {'\u1f41': 'the', '\u03b8\u03b5\u03cc\u03c2': 'God'}\n",
      "#Then open a file object.\n",
      "pickle_file = open('pickled_dict.pickle', mode='wb')\n",
      "#Then dump our dictionary into the open file object\n",
      "pickle.dump(d, pickle_file)\n",
      "#And finally, as always, close the file object.\n",
      "pickle_file.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 39
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Take a closer look at this line in our code:\n",
      "\n",
      "    pickle_file = open('pickled_dict.pickle', mode='wb')\n",
      "    \n",
      "Notice that we opened the file in `mode='wb'`.  We should recognize the 'w': it means to open the file in write mode.  The 'b' means to open it as a bytes object instead of a text object.  This is the necessary mode for pickled files.  If you get an error that reads `TypeError: must be str, not bytes` while you are trying to pickle an object, you probably didn't open your file in bytes mode.  Check that first.\n",
      "\n",
      "And now, let's `load` the dictionary back into memory."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#First, we need to open our file in 'rb' mode.\n",
      "pickled_file = open('pickled_dict.pickle', mode='rb')\n",
      "#No, we load that file into a variable with a different name.\n",
      "d_1 = pickle.load(pickled_file)\n",
      "#Now, we can compare it to our original dictionary object.\n",
      "print(d==d_1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "True\n"
       ]
      }
     ],
     "prompt_number": 40
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Just for your own information, try taking the `mode='rb'` argument from the `open` function and take a look at the error you get.\n",
      "\n",
      "Now you know what the error will look like if you forget your `mode` argument.\n",
      "\n",
      "Now, write your own code below to `pickle.dump` our NT_dict object to some file that you will recognize.  Save the file in the `data/NT/Text/` folder so you'll know where to find it later."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#insert your code here"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 41
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Exploratory data analysis"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As a first exploratory data analysis, we are going to compute for each biblical book how many chapters, verses, and words it contains. It is quite easy to count the number of chapters per book, since each book is represented by dictionary whose keys are the chapter numbers."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "chapters_per_book = {}\n",
      "for book, chapters in NT_dict.items():\n",
      "    chapters_per_book[book] = len(chapters)\n",
      "print(chapters_per_book['Lk'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "24\n"
       ]
      }
     ],
     "prompt_number": 42
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Using the function `max` we can find out what the highest number of chapters is:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "max(chapters_per_book.items())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 43,
       "text": [
        "('Tit', 3)"
       ]
      }
     ],
     "prompt_number": 43
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "That can't be right!  Titus does not have the most chapters.  What happened?  Think about  it.  Can you figure it out?\n",
      "\n",
      "...think\n",
      "\n",
      "...think\n",
      "\n",
      "...think\n",
      "\n",
      "The `max` function goes through whatever iterable it is given, in this case the `items` of `chapters_per_book`, and, if each element is an iterable, as they are here, then it gives the max of the _first_ element of that iterable, in this case the book name.  And since the book names are strings, it returns the maximum value judged by alphabetical order, in this case 'Tit'.\n",
      "\n",
      "If we want the `max` to look at a different element of each iterable, we have to tell it to do this with the `key` argument.  Check out the code below."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from operator import itemgetter\n",
      "max(chapters_per_book.items(), key=itemgetter(1))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 44,
       "text": [
        "('Mt', 28)"
       ]
      }
     ],
     "prompt_number": 44
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "OK, but that `itemgettr` thing there still needs explanation!  `itemgetter` is a way to get a different element of an iterable than the first one.  What it does here is that it takes each element that the `max` function gives it (in our case each of the `chapters_per_book.items()` key-value tuples) and returns the index 1 element of that element, i.e., the second element of each element.  As we can see, this now returns the value we were expecting.  \n",
      "\n",
      "We can also use the same `key` argument with the `sorted` function to get a sorted list of the books by number of chapters:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sorted(chapters_per_book.items(), key=itemgetter(1))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 45,
       "text": [
        "[('3Jn', 1),\n",
        " ('Phm', 1),\n",
        " ('Jud', 1),\n",
        " ('2Jn', 1),\n",
        " ('Tit', 3),\n",
        " ('2Th', 3),\n",
        " ('2Pe', 3),\n",
        " ('2Ti', 4),\n",
        " ('Php', 4),\n",
        " ('Col', 4),\n",
        " ('1Pe', 5),\n",
        " ('1Th', 5),\n",
        " ('Jas', 5),\n",
        " ('1Jn', 5),\n",
        " ('1Ti', 6),\n",
        " ('Eph', 6),\n",
        " ('Ga', 6),\n",
        " ('Heb', 13),\n",
        " ('2Co', 13),\n",
        " ('Ro', 16),\n",
        " ('Mk', 16),\n",
        " ('1Co', 16),\n",
        " ('Jn', 21),\n",
        " ('Re', 22),\n",
        " ('Lk', 24),\n",
        " ('Mt', 28),\n",
        " ('Ac', 28)]"
       ]
      }
     ],
     "prompt_number": 45
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "OK, can you figure out how to write a similar piece of code as above to discover the number of verses per book?\n",
      "\n",
      "__Hint:__ You need to go one level deeper into the NT_dict.  And make sure to count the number of verses in every chapter of every book"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Insert your code here.\n",
      "verses_per_book = {}\n",
      "for book, chapters in NT_dict.items():\n",
      "    n_verses = 0\n",
      "    for chapter, verses in chapters.items():\n",
      "        n_verses += len(verses)\n",
      "    verses_per_book[book] = n_verses\n",
      "\n",
      "\n",
      "print(verses_per_book['Ro'] == 430)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "True\n"
       ]
      }
     ],
     "prompt_number": 46
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Quiz!"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The function `sum` takes a list of numbers as input and returns the sum:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(sum([1, 3, 3, 4]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "11\n"
       ]
      }
     ],
     "prompt_number": 47
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Use this function to compute the _average_ number of verses per book. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# insert your code here\n",
      "sum(verses_per_book.values())/len(verses_per_book)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 48,
       "text": [
        "293"
       ]
      }
     ],
     "prompt_number": 48
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Given our data structure of a dictionary with three levels of keys and finally a list of words as the values for the third level, it is a little trickier to count for each book how many words it contains. One possible way is the following:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "words_per_book = {}\n",
      "for book, chapters in NT_dict.items():\n",
      "    n_words = 0\n",
      "    for chapter, verses in chapters.items():\n",
      "        for verse, words in verses.items():\n",
      "            n_words += len(words)\n",
      "    words_per_book[book] = n_words"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 49
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(words_per_book['Ro'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "7055\n"
       ]
      }
     ],
     "prompt_number": 50
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Make sure you really understand these lines of code as you will need them in the next quiz. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The New Testament was written to be heard by groups of early Christians, not to be read silently by individual believers.  Whether the letters of Paul or the Gospel of Mark, the biblical books were read aloud in house churches and larger gatherings.  So how long would one have to sit if one were, for instance, a Christian in Rome hearing Paul's letter to the Romans read for the first time.\n",
      "\n",
      "I am not aware of any exact numbers about how many words people speak per minute. Averages seem to fluctuate between 100 and 200 words per minute. Narrators are advised to use approximately 150 words per minute in audiobooks. I suspect that this number is a little lower for live reading of a document one has never seen and assume it lies around 130 words per minute (including pauses). Using this information, we can compute the time it takes to read a particular book as follows:\n",
      "\n",
      "$$\\textrm{reading time}(\\textrm{text}) = \\frac{\\textrm{number of words in text}}{\\textrm{number of words per minute}}$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Quiz!"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**1)** Write a function called `reading_time` that takes as input the name of a biblical book and the number of words in that book. Given a speed of 130 words per minute, compute how long it takes to read that text and return a dictionar with the name of the book as the key and the number of minutes it takes to read the book as the value."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def reading_time(book, n_words):\n",
      "    # insert your code here\n",
      "    return {book: n_words/130}\n",
      "\n",
      "# these tests should return True if your code is correct\n",
      "print(reading_time(\"Mt\", 130) == {\"Mt\": 1.0})\n",
      "print(reading_time(\"Ro\", 390) == {\"Ro\": 3.0})"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "True\n",
        "True\n"
       ]
      }
     ],
     "prompt_number": 51
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**2)** Compute the reading_time for each book in the NT. Assign the result to the dictionary `reading_time_per_book`.\n",
      "\n",
      "__Hint:__ You can add the key value pairs from one dictionary object to another dictionary thus:\n",
      "\n",
      "    dict1.update(dict2)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "reading_time_per_book = {}\n",
      "# insert your code here\n",
      "[reading_time_per_book.update(reading_time(book, words)) \n",
      " for book, words in words_per_book.items()]\n",
      "print(reading_time_per_book['Ro'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "54\n"
       ]
      }
     ],
     "prompt_number": 52
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**3**) Compute the average, minimum and maximum book reading time."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# insert your code here\n",
      "print('Average = ', sum(reading_time_per_book.values())/len(reading_time_per_book))\n",
      "print('Minimum = ', min(reading_time_per_book.items(), key=itemgetter(1)))\n",
      "print('Maximum = ', max(reading_time_per_book.items(), key=itemgetter(1)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "('Average = ', 38)\n",
        "('Minimum = ', ('3Jn', 1))\n",
        "('Maximum = ', ('Lk', 149))\n"
       ]
      }
     ],
     "prompt_number": 53
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Visualizing general statistics"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now that we have computed a range of general statistics for our corpus, it would be nice to visualize them. Python's plotting library *matplotlib* (see [here](http://matplotlib.org)) allows us to produce all kinds of graphs. We could for example, plot for each book, how many verses it contains:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import matplotlib.pyplot as plt\n",
      "plt.plot(list(verses_per_book.values())) #the data for the plot\n",
      "plt.xticks(range(len(verses_per_book)), list(verses_per_book.keys())) #ticks and labels on x-axis\n",
      "plt.show() #this will show the plot"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 54
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Quiz!"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**1)** Can you do the same for `words_per_book`?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# insert your code here\n",
      "plt.plot(list(words_per_book.values()))\n",
      "plt.xticks(range(len(words_per_book)), list(words_per_book.keys()))\n",
      "plt.show() #this will show the plot"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 55
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**2)** And can you do the same for `reading_time_per_book`?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# insert your code here\n",
      "plt.plot(list(reading_time_per_book.values()))\n",
      "plt.xticks(range(len(reading_time_per_book)), list(reading_time_per_book.keys()))\n",
      "plt.show() #this will show the plot"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 56
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**3)** In this final exercise we will put everything together what we have learnt so far. We want you to write a function `verses_with` that takes as input a word and the dictionary you want to search and returns the verses in the NT in which a given word occurs. We are not worried about the number of occurrences, only whether it occurs.  The result for each word should be a list with each element being another list that has three elements, the book name, chapter, and verse in which the word occurs, e.g., `[['Mt', 1, 1], ['Mt', 1, 2]]`.  Use that function to find all occurences of the name \u1f38\u03b7\u03c3\u03bf\u1fe6\u03c2 and store the corresponding results in the variable `verses_with_Jesus`. Do the same thing for \u03b8\u03b5\u03cc\u03c2. Store the result in `verses_with_God`. Finally, find all occurences of \u03c0\u03bd\u03b5\u1fe6\u03bc\u03b1 and store the indexes in `verses_with_Spirit`. Tip: (1) remember that we built our NT_dict using inflected forms of the words (2) remember that indexes start at 0."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def verses_with(word, d):\n",
      "    #insert your code here\n",
      "    verses_with_word = []\n",
      "    for book, chapters in d.items():\n",
      "        for chapter, verses in chapters.items():\n",
      "            for verse, words in verses.items():\n",
      "                if word in words:\n",
      "                    verses_with_word.append([book, chapter, verse])\n",
      "    return verses_with_word\n",
      "\n",
      "verses_with_Jesus = verses_with(\"\u1f38\u03b7\u03c3\u03bf\u1fe6\u03c2\", NT_dict)\n",
      "verses_with_God = verses_with(\"\u03b8\u03b5\u03cc\u03c2\", NT_dict)\n",
      "verses_with_Spirit = verses_with(\"\u03c0\u03bd\u03b5\u1fe6\u03bc\u03b1\", NT_dict)\n",
      "print(verses_with_Jesus[:10])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[['Lk', 2, 21], ['Lk', 2, 43], ['Lk', 2, 52], ['Lk', 3, 23], ['Lk', 4, 1], ['Lk', 4, 4], ['Lk', 4, 8], ['Lk', 4, 12], ['Lk', 4, 14], ['Lk', 4, 35]]\n"
       ]
      }
     ],
     "prompt_number": 57
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now, in order to graph our results, we first need a list of the numerical positions of all of the verses in the NT so that we can assign actual location numbers to all of the occurrences for each word.  So, first, write some code below that will extract all of the book names, chapter numbers, and verse numbers for every verse in the NT.  It will be quite similar to the function you wrote above."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "NT_verses = []\n",
      "#insert your code here\n",
      "for book, chapters in NT_dict.items():\n",
      "    for chapter, verses in chapters.items():\n",
      "        for verse in verses:\n",
      "            NT_verses.append([book, chapter, verse])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 58
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now, in order to sort these verses according to the order of their occurrence in the NT, we do the following:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "NT_book_order = ['Mt', 'Mk', 'Lk', 'Jn', 'Ac', 'Ro', '1Co', '2Co', 'Ga', 'Eph', 'Php', \n",
      "                 'Col', '1Th', '2Th', '1Ti', '2Ti', 'Tit', 'Phm', 'Heb', 'Jas', '1Pe', \n",
      "                 '2Pe', '1Jn', '2Jn', '3Jn', 'Jud', 'Re']\n",
      "sorted_verses = sorted(NT_verses, key=lambda x: NT_book_order.index(x[0]))\n",
      "print(sorted_verses[:10])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[['Mt', 1, 1], ['Mt', 1, 2], ['Mt', 1, 3], ['Mt', 1, 4], ['Mt', 1, 5], ['Mt', 1, 6], ['Mt', 1, 7], ['Mt', 1, 8], ['Mt', 1, 9], ['Mt', 1, 10]]\n"
       ]
      }
     ],
     "prompt_number": 60
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "OK, we have seen the `key` argument on the `sorted` function, but what is that `lambda` thing?  `lambda` is essentially Python's way of building mini functions.  It essentially says, \"For a value we shall call _x_, do the following thing with it.\"  In this case, what we do is use the value of _x_ to look up a value in the list `NT_book_order`.  You need to remember that `sorted` goes through every member of the list `NT_verses` and passes the value of that member to the `key` argument.  `lambda` picks up the value that is passed to `key` as _x_ and, in this case, uses the first member of the list _x_ to figure out how it should order the members of `NT_verses`.  \n",
      "\n",
      "If `lambda` statements are still a mystery to you, don't worry.  They are mysterious things.  But them more you experiment with them and use them, the more familiar with them you will become.  Consider this your introduction to `lambda` statements!\n",
      "\n",
      "Now, the last step is to produce a list of the numerical positions of every occurrence of each of our words.  To do this, you should write a function below that takes as input a list of the book, chapter, and verses for the occurrence of a word and another list with the books, chapters, and verses for the whole NT and will return a list that has the relative positions for that word in the whole NT represented as a float."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def positions_of(word_list, NT_list):\n",
      "    #insert your code here\n",
      "    word_positions = []\n",
      "    for verse in word_list:\n",
      "        word_positions.append(float(NT_list.index(verse)))\n",
      "    return word_positions\n",
      "\n",
      "positions_of_Jesus = positions_of(verses_with_Jesus, sorted_verses)\n",
      "positions_of_God = positions_of(verses_with_God, sorted_verses)\n",
      "positions_of_Spirit = positions_of(verses_with_Spirit, sorted_verses)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 61
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If everything went well, the following lines of code should produce a nice dispersion plot of all verse occurences of \u1f38\u03b7\u03c3\u03bf\u1fe6\u03c2, \u03b8\u03b5\u03cc\u03c2, \u03c0\u03bd\u03b5\u1fe6\u03bc\u03b1 in the NT."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "book_tick_locs = []\n",
      "for book in NT_book_order:\n",
      "    book_tick_locs.append(float(sorted_verses.index([book, 1, 1])))\n",
      "plt.figure(figsize=(20, 8))\n",
      "words = [u\"\u1f38\u03b7\u03c3\u03bf\u1fe6\u03c2\", u\"\u03b8\u03b5\u03cc\u03c2\", u\"\u03c0\u03bd\u03b5\u1fe6\u03bc\u03b1\"]\n",
      "plt.plot(positions_of_Jesus, [1]*len(positions_of_Jesus), \"|\", markersize=100)\n",
      "plt.plot(positions_of_God, [2]*len(positions_of_God), \"|\", markersize=100)\n",
      "plt.plot(positions_of_Spirit, [0]*len(positions_of_Spirit), \"|\", markersize=100)\n",
      "plt.yticks(range(len(words)), words)\n",
      "plt.ylim(-1, 3)\n",
      "plt.xticks(book_tick_locs, NT_book_order)\n",
      "plt.xlim(0, len(sorted_verses))\n",
      "plt.axes().grid(axis='x')\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 64
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "> Then Shahrazad reached the morning, and fell silent in the telling of her tale\u2026"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Ignore the following, it's just here to make the page pretty:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from IPython.core.display import HTML\n",
      "def css_styling():\n",
      "    styles = open(\"styles/custom.css\", \"r\").read()\n",
      "    return HTML(styles)\n",
      "css_styling()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "/*\n",
        "Placeholder for custom user CSS\n",
        "\n",
        "mainly to be overridden in profile/static/custom/custom.css\n",
        "\n",
        "This will always be an empty file in IPython\n",
        "*/\n",
        "<style>\n",
        "    @import url(http://fonts.googleapis.com/css?family=Roboto:400,300,300italic,400italic,700,700italic);\n",
        "\n",
        "    div.cell{\n",
        "        font-family:'roboto','helvetica','sans';\n",
        "        color:#444;\n",
        "        width:800px;\n",
        "        margin-left:16% !important;\n",
        "        margin-right:auto;\n",
        "    }\n",
        "\n",
        "    div.text_cell_render{\n",
        "        font-family: 'roboto','helvetica','sans';\n",
        "        line-height: 145%;\n",
        "        font-size: 120%;\n",
        "        color:#444;\n",
        "        width:800px;\n",
        "        margin-left:auto;\n",
        "        margin-right:auto;\n",
        "    }\n",
        "    .CodeMirror{\n",
        "            font-family: \"Menlo\", source-code-pro,Consolas, monospace;\n",
        "    }\n",
        "    .prompt{\n",
        "        display: None;\n",
        "    }    \n",
        "    .warning{\n",
        "        color: rgb( 240, 20, 20 )\n",
        "        }  \n",
        "</style>\n",
        "<script>\n",
        "    MathJax.Hub.Config({\n",
        "                        TeX: {\n",
        "                           extensions: [\"AMSmath.js\"]\n",
        "                           },\n",
        "                tex2jax: {\n",
        "                    inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ],\n",
        "                    displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ]\n",
        "                },\n",
        "                displayAlign: 'center', // Change this to 'center' to center equations.\n",
        "                \"HTML-CSS\": {\n",
        "                    styles: {'.MathJax_Display': {\"margin\": 4}}\n",
        "                }\n",
        "        });\n",
        "</script>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 11,
       "text": [
        "<IPython.core.display.HTML at 0x7f92346c5550>"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p><small><a rel=\"license\" href=\"http://creativecommons.org/licenses/by-sa/4.0/\"><img alt=\"Creative Commons License\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by-sa/4.0/88x31.png\" /></a><br /><span xmlns:dct=\"http://purl.org/dc/terms/\" property=\"dct:title\">Python Programming for the Humanities</span> by <a xmlns:cc=\"http://creativecommons.org/ns#\" href=\"http://fbkarsdorp.github.io/python-course\" property=\"cc:attributionName\" rel=\"cc:attributionURL\">http://fbkarsdorp.github.io/python-course</a> is licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-sa/4.0/\">Creative Commons Attribution-ShareAlike 4.0 International License</a>. Based on a work at <a xmlns:dct=\"http://purl.org/dc/terms/\" href=\"https://github.com/fbkarsdorp/python-course\" rel=\"dct:source\">https://github.com/fbkarsdorp/python-course</a>.</small></p>"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}