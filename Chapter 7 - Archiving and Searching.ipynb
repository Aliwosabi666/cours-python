{
 "metadata": {
  "name": "",
  "signature": "sha256:d3aa102c0028256ad87eb57cd2564c5383ab4edbcc90ced1486a59cf5bc197b3"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Chapter 7 - Archiving and Searching"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "-- *A Python Course for the Humanities*"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Imagine you have collected a large number of documents for your research. You have, for example, collected the complete volume of a particular newspaper for the 20th century. A typical newspaper contains about a billion words per year. How are you going to store this collection to encertain its sustainability? And, how are you going to search through this enormous collection? \n",
      "\n",
      "In this chapter we will explore some of the important techniques for archiving and searching through large collections of text. We will develop several archiving and search tools that allow you to efficiently and reliably extract information from corpora. Along the way we will discuss quite some new and important programming techniques and constructs. We will also further our knowledge about the framework of Object Oriented Programming or OOP.\n",
      "\n",
      "By the end of the chapter you will know how to index and search through large collections of texts, how to implement different document ranking metrics and how to make your searching system available to end-users in a user friendly way. We have quite some ground to cover, so let's get started."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Indexing Collections of Text"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "> Information Retrieval (IR) is finding material (usually documents) of an unstructured nature (usually text) that satisfies an information need from within large collections (usually stored on computers. (Manning, NLP course, coursera)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The field of Information Retrieval (IR) deals with developing techniques that allow users to efficiently search through large collections (of text) to find documents that are relevant to the question a particular user has. These days the we almost immediately thingk of *web search*, but there are many other use case scenarios, such as E-mail search, searching for music (e.g. Spotify) or searching for content on your laptop. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "![caption](files/images/IR-schema.png)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Most search models in Information Retrieval are similar to the schema above. We have a collection of items, say text documents that are indexed using an indexer with a particular indexing schema. Given a user query, this index is used to retrieve $k$ documents which after scoring and sorting are presented to the user. This might seem all a little abstract, but it will all become clear once we start implementing each component.\n",
      "\n",
      "Coming back to our newspaper collection, say we would like to find all documents that mention [Albert Einstein](https://en.wikipedia.org/wiki/Albert_einstein) and [Edwin Hubble](https://en.wikipedia.org/wiki/Edwin_Hubble) but not [Enrico Fermi](https://en.wikipedia.org/wiki/Enrico_Fermi). Those of you familiar with the unix search tool [grep](https://en.wikipedia.org/wiki/Grep) might argue that we could simply use grep to search for all documents that contain both Einstein and Hubble and then filter out those documents that mention Fermi. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Quiz!"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Come up with at least two reasons why searching with grep is not a good option."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "*Double click this cell and write down your answer.*"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To efficiently provide an answer to such queries we need to come up with another representation of our collection instead of plain text. Have a look at the following table.\n",
      "\n",
      "<table border=\"1\" class=\"dataframe\">\n",
      "  <thead>\n",
      "    <tr style=\"text-align: right;\">\n",
      "      <th></th>\n",
      "      <th>12-11-1928</th>\n",
      "      <th>04-04-1946</th>\n",
      "      <th>03-11-1983</th>\n",
      "      <th>19-01-1999</th>\n",
      "    </tr>\n",
      "  </thead>\n",
      "  <tbody>\n",
      "    <tr>\n",
      "      <th>Einstein</th>\n",
      "      <td> 1</td>\n",
      "      <td> 1</td>\n",
      "      <td> 0</td>\n",
      "      <td> 0</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <th>Hubble</th>\n",
      "      <td> 1</td>\n",
      "      <td> 1</td>\n",
      "      <td> 1</td>\n",
      "      <td> 0</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <th>Fermi</th>\n",
      "      <td> 1</td>\n",
      "      <td> 0</td>\n",
      "      <td> 0</td>\n",
      "      <td> 0</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <th>Winfrey</th>\n",
      "      <td> 0</td>\n",
      "      <td> 0</td>\n",
      "      <td> 0</td>\n",
      "      <td> 1</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <th>Dylan</th>\n",
      "      <td> 0</td>\n",
      "      <td> 0</td>\n",
      "      <td> 1</td>\n",
      "      <td> 1</td>\n",
      "    </tr>\n",
      "  </tbody>\n",
      "</table>\n",
      "\n",
      "This table is called a *Term Document incidence matrix* where for each term (rows) we write down a 1 if a particular document (columns) contains that term and 0 otherwise. This representation provides us with so-called incidence vectors for each term. If we now would like to extract all document that mention Albert Einstein, we can use the binary vector `1100` of Einstein to quickly extract all documents mentioning Einstein. Now, to find al documents containing both Einstein and Hubble we take the complement of the vectors of Einstein (1100) and Hubble (1110):\n",
      "\n",
      "`1100 AND`   \n",
      "`1110 =`    \n",
      "`1100`   "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Quiz!"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**a)** Given the incidence vectors of Einstein (1100), Hubble (1110) and Fermi (1000), what is the binary representation corresponding to the query `Einstein AND Hubble AND NOT Fermi`?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "*Double click this cell and write down your answer.*"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**b)** Can you implement the function `AND` that takes as argument two incidence vectors and returns the complement of the two?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def AND(vector_a, vector_b):\n",
      "    # insert your code here\n",
      "    return [1 if a and b else 0 for a, b in zip(vector_a, vector_b)]\n",
      "    \n",
      "# these tests should return True if your code is correct\n",
      "print(AND([1, 1, 0, 0], [1, 1, 1, 0]) == [1, 1, 0, 0])\n",
      "print(AND([1, 0, 0, 1, 0, 0, 1], [1, 1, 1, 0, 1, 0, 1]) == [1, 0, 0, 0, 0, 0, 1])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "True\n",
        "True\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**c)** Rewrite the function AND to allow it to take an arbitary number of incidence vectors."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def AND(*vectors):\n",
      "    # insert your code here\n",
      "    return [1 if all(row) else 0 for row in zip(*vectors)]    \n",
      "\n",
      "# these tests should return True if your code is correct\n",
      "print(AND([1, 1, 0, 0], [1, 1, 1, 0], [1, 0, 0, 0]) == [1, 0, 0, 0])\n",
      "print(AND([1, 1, 1, 0, 1], [1, 0, 0, 1, 0], [0, 1, 1, 0, 1]) == [0, 0, 0, 0, 0])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "True\n",
        "True\n"
       ]
      }
     ],
     "prompt_number": 29
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**d)** Can you implement the function `NOT` that takes as argument an incidence vector and return a representation that can be used in combination with other AND queries?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def NOT(vector):\n",
      "    # insert your code here\n",
      "    return [1 if not elt else 0 for elt in vector]\n",
      "\n",
      "# these tests should return True if your code is correct\n",
      "print(AND([1, 1, 0, 0], [1, 1, 1, 0], NOT([1, 0, 0, 0])) == [0, 1, 0, 0])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "True\n"
       ]
      }
     ],
     "prompt_number": 30
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The binary representation allows us to efficiently search for documents containing particular terms of a search query. There are, however, still some problems that need to be overcome. Consider a document collecion of $N=1.000.000$ documents where each document is about a 1000 words long. On average such a collection contains approximately 500k unique terms. This means that if we try to build an incidence matrix we would have to construct a matrix containing $500,000 \\times 1,000,000 =$ half a trillion 0's and 1's. Such a large matrix cannot be stored on a simple laptop. The matrix contains mnuch redundant information, because it can only contain $1,000 \\times 1,000,000 =$ 1 billion 1's."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Quiz!"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "What would be a better representation of our matrix?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "*Double click this cell and write down your answer.*"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The *Inverted Index* is the key data structure in modern Information Retrieval. An inverted index is a structure in which for each unique term $t$ in our collection we store a list of all document ID's that contain $t$. If we represent our small collection of newspapers in this way, it looks like this:\n",
      "\n",
      "`Einstein: [12-11-1928, 04-04-1946]`   \n",
      "`Hubble:   [12-11-1928, 04-04-1946, 03-11-1983]`   \n",
      "`Fermi:    [12-11-1928]`   \n",
      "`Winfrey:  [19-01-1999]`   \n",
      "`Dylan:    [03-11-1983, 19-01-1999]`"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Quiz!"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**a)** What would be a convenient and efficient data structure in Python to represent an inverted index?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "*Double click this cell and write down your answer.*"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**b)** We will implement a first version of an IR system. I choose to implement it as the class `IRSystem`. Implement the methiod `index_document`. It takes as argument a document ID and a list of words. The function should update the variable `tdf` in such a way that for each term it stores a set of all document IDs in which that term occurs."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import glob, os, re\n",
      "from collections import defaultdict\n",
      "\n",
      "\n",
      "def tokenize(text, lowercase=True):\n",
      "    text = text.lower() if lowercase else text\n",
      "    for match in re.finditer(r\"\\w+(\\.?\\w+)*\", text):\n",
      "        yield match.group()\n",
      "\n",
      "        \n",
      "class IRSystem:\n",
      "    \"\"\"A very simple Information Retrieval System. The constructor \n",
      "    s = IRSystem() builds an empty system. Next, index several documents \n",
      "    with s.index_document(ID, text).\n",
      "    \"\"\"\n",
      "    \n",
      "    def __init__(self):\n",
      "        \"Initialize an IR Sytem.\"\n",
      "        self.tdf = defaultdict(set)\n",
      "        self.doc_ids = []\n",
      "                \n",
      "    def index_document(self, doc_id, words):\n",
      "        \"Add a new unindexed document to the system.\"\n",
      "        self.doc_ids.append(doc_id)\n",
      "        # insert your code here\n",
      "        for word in words:\n",
      "            self.tdf[word].add(doc_id)\n",
      "        \n",
      "    def index_collection(self, filenames):\n",
      "        \"Index a collection of documents.\"\n",
      "        for filename in filenames:\n",
      "            self.index_document(os.path.basename(filename), \n",
      "                                tokenize(open(filename).read()))\n",
      "    \n",
      "# these tests should return True if your code is correct\n",
      "s = IRSystem()\n",
      "s.index_collection(glob.glob('data/haggard/*.txt'))\n",
      "\n",
      "print('The Ghost Kings 8184.txt' in s.tdf['master'])\n",
      "print('Cleopatra 2769.txt' in s.tdf['children'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "True\n",
        "True\n"
       ]
      }
     ],
     "prompt_number": 164
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now that we have an efficient, sparse data structure to represent our collection, how can we use that stucture to efficiently process user queries? Our index is now represented as a Python dictionary which allows us to easily query the index for single terms, using\n",
      "\n",
      "    s = IRSystem()\n",
      "    s.tdf[term]\n",
      "\n",
      "which will return a set of all documents in which that term occurs. But how do we search for documents that contain two or more terms? Python's data structure `set` defines a convenient method called `intersection` with which we can extract all items common to two or more sets:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "a = {'a', 'b', 'c', 'd'}\n",
      "b = {'c', 'a', 'e', 'f'}\n",
      "print(a.intersection(b))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "{'a', 'c'}\n"
       ]
      }
     ],
     "prompt_number": 45
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can make use of this method to implement a function `query` that takes as argument an arbitrary number of query terms and returns all documents in which those terms occur."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Quiz!"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Implement the method `IRSystem.query(*terms)`. The method should return an iterable containing all ID's of the documents in which all query terms occur."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class IRSystem:\n",
      "    \"\"\"A very simple Information Retrieval System. The constructor \n",
      "    s = IRSystem() builds an empty system. Next, index several documents \n",
      "    with s.index_document(ID, text). Then ask queries with \n",
      "    s.query('term1', 'term2') to retrieve the matching documents.\"\"\"\n",
      "    \n",
      "    def __init__(self):\n",
      "        \"Initialize an IR Sytem.\"\n",
      "        self.tdf = defaultdict(set)\n",
      "        self.doc_ids = []\n",
      "                \n",
      "    def index_document(self, doc_id, words):\n",
      "        \"Add a new unindexed document to the system.\"\n",
      "        self.doc_ids.append(doc_id)\n",
      "        for word in words:\n",
      "            self.tdf[word].add(doc_id)\n",
      "\n",
      "    def index_collection(self, filenames):\n",
      "        \"Index a collection of documents.\"\n",
      "        for filename in filenames:\n",
      "            self.index_document(os.path.basename(filename), \n",
      "                                tokenize(open(filename).read()))\n",
      "            \n",
      "    def query(self, *terms):\n",
      "        \"Query the system for documents in which all terms occur.\"\n",
      "        # insert your code here\n",
      "        return set.intersection(*map(self.tdf.get, terms))\n",
      "    \n",
      "# these tests should return True if your code is correct\n",
      "s = IRSystem()\n",
      "s.index_collection(glob.glob('data/haggard/*.txt'))\n",
      "\n",
      "print('Beatrice 3096.txt' in s.query(\"master\", \"children\"))\n",
      "print('Fair Margaret 9780.txt' in s.query(\"eye\", \"father\", \"work\"))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "True\n",
        "True\n"
       ]
      }
     ],
     "prompt_number": 165
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Our Information Retrieval system is starts to look quite good. We have written functions to tokenize documents, index documents and query the index for documents. In the query method above, we made the simplifying assumption that as long as two documents contain a particular search term, they are equally relevant. However, our intuition says that documents that contain more instances of a particular term are more relevant than documents with less instances. To account for this intuition we need a way to sort and score our search results.\n",
      "\n",
      "Let's start with a very naive and simple scoring function: document frequency. This scoring function simply sums the document frequencies of each search term in a particular document:\n",
      "\n",
      "$$\\textrm{score}(q_1, q_2, \\ldots, q_n) = \\sum^n_{i=1} df(q_i)$$\n",
      "\n",
      "where $n$ is the number of search terms and $df$ the document frequency of term $q_i$ in a document.\n",
      "\n",
      "In order to compute this formula we need to have information about the frequency of words in each document. We can extract this information in the method `index_document`. For each term we store for each document how often that term occurs in that document. Note that the variable `tdf` currently is represented by a dictionary with sets of document ID's as values. We will adapt this data structure to a structure that allows us to store the document frequencies:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from collections import Counter\n",
      "\n",
      "class IRSystem:\n",
      "    \"\"\"A very simple Information Retrieval System. The constructor \n",
      "    s = IRSystem() builds an empty system. Next, index several documents \n",
      "    with s.index_document(ID, text). Then ask queries with \n",
      "    s.query('term1', 'term2') to retrieve the matching documents.\"\"\"\n",
      "    \n",
      "    def __init__(self):\n",
      "        \"Initialize an IR Sytem.\"\n",
      "        self.tdf = defaultdict(Counter) # changed!\n",
      "        self.doc_ids = []\n",
      "                \n",
      "    def index_document(self, doc_id, words):\n",
      "        \"Add a new unindexed document to the system.\"\n",
      "        self.doc_ids.append(doc_id)\n",
      "        for word in words:\n",
      "            self.tdf[word][doc_id] += 1 # changed!\n",
      "\n",
      "    def index_collection(self, filenames):\n",
      "        \"Index a collection of documents.\"\n",
      "        for filename in filenames:\n",
      "            self.index_document(os.path.basename(filename), \n",
      "                                tokenize(open(filename).read()))            \n",
      "            \n",
      "    def query(self, *terms):\n",
      "        \"Query the system for documents in which all terms occur.\"\n",
      "        return set.intersection(*map(self.tdf.get, terms))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 166
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "I represent the `tdf` variable as a dictionary with [Counter](https://docs.python.org/dev/library/collections.html#collections.Counter) objects as default values. The `Counter` object is a very convenient structure for counting hashable objects. As you can see, we only had to adjust two lines in our original system. Before we continue, make sure you understand what is happening here. We now reinitialize our IR system:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "s = IRSystem()\n",
      "s.index_collection(glob.glob('data/haggard/*.txt'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 167
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's inspect the new data structure of the term-document frequency matrix:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "s.tdf['master'].most_common(n=10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 168,
       "text": [
        "[('The Ancient Allan 5746.txt', 161),\n",
        " ('Red Eve 3094.txt', 125),\n",
        " ('Lysbeth, a Tale of the Dutch 5754.txt', 116),\n",
        " ('The Virgin of the Sun 3153.txt', 90),\n",
        " ('The Brethren 2762.txt', 65),\n",
        " ('Fair Margaret 9780.txt', 61),\n",
        " ('The Lady of Blossholme 3813.txt', 60),\n",
        " (\"Wisdom's Daughter (1923) 0200181.txt\", 58),\n",
        " ('Pearl-Maiden 5175.txt', 57),\n",
        " ('Finished 1724.txt', 37)]"
       ]
      }
     ],
     "prompt_number": 168
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The class `Counter` features a method called `most_common`. It returns the $n$ most common items in a Counter object. Now that we have a data structure that stores the information about the frequency of words in documents, we need to adapt our query function in such a way that it returns a ranked list of documents where each document is sorted on the basis of equation above."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Quiz!"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**a)** The method query in the class definition below calls the method `score`. This method should take as argument a document ID and an arbitrary number of terms and returns the sum of the frequencies of these terms in this document. Implement the method `score`."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class IRSystem:\n",
      "    \"\"\"A very simple Information Retrieval System. The constructor \n",
      "    s = IRSystem() builds an empty system. Next, index several \n",
      "    documents with s.index_document(ID, text). Then ask queries \n",
      "    with s.query('term1', 'term2') to retrieve the top n matching \n",
      "    documents.\"\"\"\n",
      "    \n",
      "    def __init__(self):\n",
      "        \"Initialize an IR Sytem.\"\n",
      "        self.tdf = defaultdict(Counter)\n",
      "        self.doc_ids = []\n",
      "                \n",
      "    def index_document(self, doc_id, words):\n",
      "        \"Add a new unindexed document to the system.\"\n",
      "        self.doc_ids.append(doc_id)\n",
      "        for word in words:\n",
      "            self.tdf[word][doc_id] += 1\n",
      "\n",
      "    def index_collection(self, filenames):\n",
      "        \"Index a collection of documents.\"\n",
      "        for filename in filenames:\n",
      "            self.index_document(os.path.basename(filename), \n",
      "                                tokenize(open(filename).read()))\n",
      "                \n",
      "    def score(self, doc_id, *terms):\n",
      "        \"Score a document for a particular query using the sum of the term frequencies.\"\n",
      "        # insert your code here\n",
      "        return sum(self.tdf[term][doc_id] for term in terms)\n",
      "            \n",
      "    def query(self, *terms, n=10):\n",
      "        \"\"\"Query the system for documents in which all terms occur. Returns\n",
      "        the top n matching documents.\"\"\"\n",
      "        scores = {doc_id: self.score(doc_id, *terms) for doc_id in self.doc_ids}\n",
      "        return sorted(scores, key=scores.get, reverse=True)[:n]\n",
      "\n",
      "\n",
      "# these tests should return True if your code is correct\n",
      "s = IRSystem()\n",
      "s.index_collection(glob.glob('data/haggard/*.txt'))\n",
      "\n",
      "print(s.query(\"master\")[0] == 'The Ancient Allan 5746.txt')\n",
      "print(s.query(\"egg\", \"shell\")[0] == 'Dawn 10892.txt')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "True\n",
        "True\n"
       ]
      }
     ],
     "prompt_number": 169
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**b)** Give at least two reasons why this way of scoring and sorting our documents is probably not such a good idea."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "*Double click this cell and write down your answer.*"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Our scoring method contains multiple flaws. Most worrying probably, is the fact that it does not control for the lengths of our documents. Needless to say, this could greatly influence our final lists. We need to think about what makes it that a search term is more or less relevant for a particular document? \n",
      "\n",
      "The frequency with which terms occur in a document functions as an important cue to the importance of a document. A particular term is even more important when it is both frequent in a document and occurs in only a limited number of other documents. In Information Retrieval a ranking metric that attempts to capture this intuition is [Okapi BM25](https://en.wikipedia.org/wiki/Okapi_BM25). The metric has proven itself to be one of the most successful ranking functions. In one of its many versions it is computed as follows:\n",
      "\n",
      "$$score(q_1, q_2, \\ldots, q_n) = \\sum^n_{i=1} IDF(q_i) \\cdot \\frac{df(q_i, D) \\cdot (k_1 + 1)}{df(q_i, D) + k_1 \\cdot (1 - b + b \\cdot \\frac{|D|}{\\textrm{avgdl}})}$$\n",
      "\n",
      "where $Q$ represents a query and $df(q_i, D)$ is the frequency of the i'th term in $Q$ in document $D$. $|D|$ is the length of document $D$ in number of word tokens and avgdl is the average document length. The parameters $b$ and $k_1$ are set often to $0.75$ and $1.2$, respectively. We compute the I(nverse) D(ocument) F(requency) weight using:\n",
      "\n",
      "$$IDF(q_i) = \\log \\frac{N - n(q_i) + 0.5}{n(q_i) + 0.5}$$\n",
      "\n",
      "where $N$ is the number of documents in the corpus and $n(q_i)$ the number of documents that contain $q_i$.\n",
      "\n",
      "We will implement this formula in our `score` method. Before we will do that it is important to make a list of all the pieces of information we need to compute the formula:\n",
      "\n",
      "1. the frequency of a term $q_i$ in document $D$;\n",
      "2. the length of document $D$;\n",
      "3. the average length of all documents in the collection;\n",
      "4. the IDF weight of a term $q_i$.\n",
      "\n",
      "Our implementation already provides information about the document frequency of terms. We need three pieces of addtional information to be able to compute the formula: (1) the length of each document, (2) the average document length and (3) the IDF weight of each unique term."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Quiz!"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**a)** We will start with adding the length of each document in our collection to the IR system. Reimplement the `index_document` method. Besides updating the term-document frequencies of each term, it should update the Counter object `lengths` in such a way that for each document ID it stores the length of the document being indexed."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class IRSystem:\n",
      "    \"\"\"A very simple Information Retrieval System. The constructor \n",
      "    s = IRSystem() builds an empty system. Next, index several \n",
      "    documents with s.index_document(ID, text). Then ask queries \n",
      "    with s.query('term1', 'term2') to retrieve the top n matching \n",
      "    documents.\"\"\"\n",
      "    \n",
      "    def __init__(self):\n",
      "        \"Initialize an IR Sytem.\"\n",
      "        self.tdf = defaultdict(Counter)\n",
      "        self.lengths = Counter()\n",
      "        self.doc_ids = []\n",
      "                \n",
      "    def index_document(self, doc_id, words):\n",
      "        \"Add a new unindexed document to the system.\"\n",
      "        self.doc_ids.append(doc_id)\n",
      "        # insert your code here\n",
      "        for word in words:\n",
      "            self.tdf[word][doc_id] += 1\n",
      "            self.lengths[doc_id] += 1\n",
      "\n",
      "    def index_collection(self, filenames):\n",
      "        \"Index a collection of documents.\"\n",
      "        for filename in filenames:\n",
      "            self.index_document(os.path.basename(filename), \n",
      "                                tokenize(open(filename).read()))\n",
      "            \n",
      "    def score(self, doc_id, *terms):\n",
      "        \"Score a document for a particular query using the sum of the term frequencies.\"\n",
      "        return sum(self.tdf[term][doc_id] for term in terms)\n",
      "            \n",
      "    def query(self, *terms, n=10):\n",
      "        \"\"\"Query the system for documents in which all terms occur. Returns\n",
      "        the top n matching documents.\"\"\"\n",
      "        scores = {doc_id: self.score(doc_id, *terms) for doc_id in self.doc_ids}\n",
      "        return sorted(scores, key=scores.get, reverse=True)[:n]\n",
      "\n",
      "\n",
      "# these tests should return True if your code is correct\n",
      "s = IRSystem()\n",
      "s.index_collection(glob.glob('data/haggard/*.txt'))\n",
      "\n",
      "print(s.lengths['Dawn 10892.txt'] == 192299)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "True\n"
       ]
      }
     ],
     "prompt_number": 170
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**b)** Once we have computed the document length for each document, the average document length is trivial to compute. In this exercize we will turn to the computation of the IDF weights. To compute the IDF weight for a particular term we need to know (1) how many documents there are in our collection, and (2) in how many documents that term occurs. We will implement a helper method called `_document_frequency`. It should return a dictionary in which for each we term we store the number of documents in which that term occurs. You will also need to adapt the `index_document` method in such a way that the variable `N` represents the number of documents that have been indexed."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class IRSystem:\n",
      "    \"\"\"A very simple Information Retrieval System. The constructor \n",
      "    s = IRSystem() builds an empty system. Next, index several \n",
      "    documents with s.index_document(ID, text). Then ask queries \n",
      "    with s.query('term1', 'term2') to retrieve the top n matching \n",
      "    documents.\"\"\"\n",
      "    \n",
      "    def __init__(self):\n",
      "        \"Initialize an IR Sytem.\"\n",
      "        self.tdf = defaultdict(Counter)\n",
      "        self.lengths = Counter()\n",
      "        self.doc_ids = []\n",
      "        self.N = 0\n",
      "                \n",
      "    def index_document(self, doc_id, words):\n",
      "        \"Add a new unindexed document to the system.\"\n",
      "        # insert you code here\n",
      "        self.doc_ids.append(doc_id)\n",
      "        for word in words:\n",
      "            self.tdf[word][doc_id] += 1\n",
      "            self.lengths[doc_id] += 1\n",
      "    \n",
      "    def index_collection(self, filenames):\n",
      "        \"Index a collection of documents.\"\n",
      "        for filename in filenames:\n",
      "            self.index_document(os.path.basename(filename), \n",
      "                                tokenize(open(filename).read()))\n",
      "\n",
      "    def _document_frequency(self):\n",
      "        \"Return the document frequency for each term in self.tdf.\"\n",
      "        # insert your code here\n",
      "        return {term: len(documents) for term, documents in self.tdf.items()}\n",
      "    \n",
      "    def score(self, doc_id, *terms):\n",
      "        \"Score a document for a particular query using the sum of the term frequencies.\"\n",
      "        return sum(self.tdf[term][doc_id] for term in terms)\n",
      "            \n",
      "    def query(self, *terms, n=10):\n",
      "        \"\"\"Query the system for documents in which all terms occur. Returns\n",
      "        the top n matching documents.\"\"\"\n",
      "        scores = {doc_id: self.score(doc_id, *terms) for doc_id in self.doc_ids}\n",
      "        return sorted(scores, key=scores.get, reverse=True)[:n]\n",
      "\n",
      "\n",
      "# these tests should return True if your code is correct\n",
      "s = IRSystem()\n",
      "s.index_collection(glob.glob('data/haggard/*.txt'))\n",
      "\n",
      "print(s._document_frequency()['children'] == 59)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "True\n"
       ]
      }
     ],
     "prompt_number": 172
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We now have all the ingredients to compute the BM25 score. Lets put everything together and implement a complete version of our IR system:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import glob, os\n",
      "\n",
      "class IRSystem:\n",
      "    \"\"\"A very simple Information Retrieval System. The constructor \n",
      "    s = IRSystem() builds an empty system. Next, index several documents \n",
      "    with s.index_document(text, url). Then ask queries with \n",
      "    s.query('term1', 'term2', n=10) to retrieve the top n \n",
      "    matching documents.\"\"\"\n",
      "    \n",
      "    def __init__(self, b=0.75, k1=1.2):\n",
      "        \"Initialize an IR Sytem.\"\n",
      "        self.N = 0\n",
      "        self.lengths = Counter()\n",
      "        self.tdf = defaultdict(Counter)\n",
      "        self.doc_ids = []\n",
      "        self.b = b\n",
      "        self.k1 = k1\n",
      "        self._all_set = False\n",
      "        \n",
      "    def __repr__(self):\n",
      "        return '<IRSystem(b={self.b}, k1={self.k1}, N={self.N})>'.format(self=self)\n",
      "        \n",
      "    def index_document(self, doc_id, words):\n",
      "        \"Add a new unindexed document to the system.\"\n",
      "        self.N += 1\n",
      "        self.doc_ids.append(doc_id)\n",
      "        for word in words:\n",
      "            self.tdf[word][doc_id] += 1\n",
      "            self.lengths[doc_id] += 1\n",
      "        self._all_set = False\n",
      "        \n",
      "    def index_collection(self, filenames):\n",
      "        \"Index a collection of documents.\"\n",
      "        for filename in filenames:\n",
      "            self.index_document(os.path.basename(filename), \n",
      "                                tokenize(open(filename).read()))\n",
      "    \n",
      "    def _document_frequency(self):\n",
      "        \"Return the document frequency for each term in self.tdf.\"\n",
      "        return {term: len(documents) for term, documents in self.tdf.items()}\n",
      "    \n",
      "    def score(self, doc_id, *query):\n",
      "        \"Score a document for a particular query using Okapi BM25.\"\n",
      "        score = 0\n",
      "        length = self.lengths[doc_id]\n",
      "        for term in query:\n",
      "            tf = self.tdf[term][doc_id]\n",
      "            df = self.df.get(term, 0)\n",
      "            idf = log((self.N - df + 0.5) / (df + 0.5))\n",
      "            score += (idf * (tf * (self.k1 + 1)) / \n",
      "                          (tf + self.k1 * (1 - self.b + (self.b * length / self.avg_len))))\n",
      "        return score\n",
      "    \n",
      "    def query(self, *query, n=10):\n",
      "        \"\"\"Query an indexed collection. Returns a ranked list of doc ID's sorted by\n",
      "        the computation of Okapi BM25.\"\"\"\n",
      "        if not self._all_set:\n",
      "            self.df = self._document_frequency()\n",
      "            self.avg_len = sum(self.lengths.values()) / self.N\n",
      "            self._all_set = True\n",
      "            \n",
      "        scores = {doc_id: self.score(doc_id, *query) for doc_id in self.doc_ids}\n",
      "        return sorted(scores.items(), key=lambda i: i[1], reverse=True)[:n]\n",
      "    \n",
      "    def present(self, results):\n",
      "        \"Present the query results as a list.\"\n",
      "        for doc_id, score in results:\n",
      "            print(\"%5.2f | %s\" % (100 * score, doc_id))\n",
      "            \n",
      "    def present_results(self, *query):\n",
      "        \"Query the collection and present the results.\"\n",
      "        return self.present(self.query(*query))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 217
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "I have taken the liberty to make some minor adjustments to the class and add some methods to present the results of a query. The method `score` implements the Okapi BM25 quite straightforwardly. Note that I added an attribute called `all_set`. This attribute tells the system whether all the pieces of information have been collected in order to compute the BM25 scores. In the method `query` we ask whether all is set. If not, we first compute the document frequencies of each unique term in our collection as well as the average document length. The reason I do not compute this earlier is that we need to be certain that after a document has been added to the index, we need to recompute all these values (note that we set `all_set` in `index_document` to `False`). The methods `present` and `present_results` are two helper methods to more conveniently print the results of a query.\n",
      "\n",
      "Let's test out IR System!"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "s = IRSystem()\n",
      "s.index_collection(glob.glob('data/haggard/*.txt'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 218
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "s.present_results(\"regeneration\", \"pharao\", \"odds\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "460.36 | Regeneration 13434.txt\n",
        "250.53 | The Ancient Allan 5746.txt\n",
        "242.67 | Smith and the Pharaohs, and other Tales 6073.txt\n",
        "223.03 | Ayesha, the Return of She 5228.txt\n",
        "209.67 | A Winter Pilgrimage (1901) 0600121.txt\n",
        "25.79 | Long Odds 1918.txt\n",
        "22.96 | Marie An Episode in The Life of the late Allan Quatermain 1690.txt\n",
        "22.93 | Stories by English Authors: Africa (Selected by Scribners) 1980.txt\n",
        "22.82 | Eric Brighteyes 2721.txt\n",
        "21.13 | Colonel Quaritch, V.C. A Tale of Country Life 11882.txt\n"
       ]
      }
     ],
     "prompt_number": 222
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The biggest advantage and greatest use of Object Oriented Programming is the ability to subclass objects. We could for example make a specialized IRSystem for searching through particular directories on your own laptop. To finish this section, let's implement a subclass of our IRSystem specialized for the Haggard collection:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os\n",
      "import subprocess\n",
      "\n",
      "class HaggardSearcher(IRSystem):\n",
      "    \"\"\"A trivial IR system over a collection of novels.\"\"\"\n",
      "    \n",
      "    def __init__(self, b=0.75, k1=1.2):\n",
      "        super(HaggardSearcher, self).__init__(b=b, k1=k1)\n",
      "        self.index_collection()\n",
      "        \n",
      "    def index_collection(self):\n",
      "        for filename in glob.glob(\"data/haggard/*.txt\"):\n",
      "            self.index_document(os.path.basename(filename), \n",
      "                                tokenize(open(filename).read()))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 223
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "haggard = HaggardSearcher()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 224
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "haggard.present_results(\"regeneration\", \"pharao\", \"disguise\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "444.83 | Regeneration 13434.txt\n",
        "270.99 | The Ancient Allan 5746.txt\n",
        "253.73 | Ayesha, the Return of She 5228.txt\n",
        "242.67 | Smith and the Pharaohs, and other Tales 6073.txt\n",
        "209.67 | A Winter Pilgrimage (1901) 0600121.txt\n",
        "53.06 | The World's Desire 2763.txt\n",
        "50.81 | Fair Margaret 9780.txt\n",
        "50.70 | Queen of the Dawn (1925) 0200381.txt\n",
        "46.82 | Morning Star 2722.txt\n",
        "45.77 | A Yellow God: an Idol of Africa 2857.txt\n"
       ]
      }
     ],
     "prompt_number": 235
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Practical: Searching your own PDF library"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "You've reached the end of the chapter. Ignore the code below, it's just here to make the page pretty:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from IPython.core.display import HTML\n",
      "def css_styling():\n",
      "    styles = open(\"styles/custom.css\", \"r\").read()\n",
      "    return HTML(styles)\n",
      "css_styling()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "/*\n",
        "Placeholder for custom user CSS\n",
        "\n",
        "mainly to be overridden in profile/static/custom/custom.css\n",
        "\n",
        "This will always be an empty file in IPython\n",
        "*/\n",
        "<style>\n",
        "    @import url(http://fonts.googleapis.com/css?family=Roboto:400,300,300italic,400italic,700,700italic);\n",
        "\n",
        "    div.cell{\n",
        "        font-family:'roboto','helvetica','sans';\n",
        "        color:#444;\n",
        "        width:800px;\n",
        "        margin-left:16% !important;\n",
        "        margin-right:auto;\n",
        "    }\n",
        "\n",
        "    div.text_cell_render{\n",
        "        font-family: 'roboto','helvetica','sans';\n",
        "        line-height: 145%;\n",
        "        font-size: 120%;\n",
        "        color:#444;\n",
        "        width:800px;\n",
        "        margin-left:auto;\n",
        "        margin-right:auto;\n",
        "    }\n",
        "    .CodeMirror{\n",
        "            font-family: \"Menlo\", source-code-pro,Consolas, monospace;\n",
        "    }\n",
        "    .prompt{\n",
        "        display: None;\n",
        "    }    \n",
        "    .warning{\n",
        "        color: rgb( 240, 20, 20 )\n",
        "        }  \n",
        "</style>\n",
        "<script>\n",
        "    MathJax.Hub.Config({\n",
        "                        TeX: {\n",
        "                           extensions: [\"AMSmath.js\"]\n",
        "                           },\n",
        "                tex2jax: {\n",
        "                    inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ],\n",
        "                    displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ]\n",
        "                },\n",
        "                displayAlign: 'center', // Change this to 'center' to center equations.\n",
        "                \"HTML-CSS\": {\n",
        "                    styles: {'.MathJax_Display': {\"margin\": 4}}\n",
        "                }\n",
        "        });\n",
        "</script>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 1,
       "text": [
        "<IPython.core.display.HTML at 0x105424c90>"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "by Folgert Karsdorp (Meertens Instituut) en Maarten van Gompel (Radboud University Nijmegen)\n",
      "\n",
      "Licensed under the [GNU Free Documentation License](http://www.gnu.org/copyleft/fdl.html)"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}