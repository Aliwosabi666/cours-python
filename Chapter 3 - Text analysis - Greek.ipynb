{
 "metadata": {
  "name": "",
  "signature": "sha256:ddc12d05bf810e52ea424d39564ff01bbd1f179fedb7b70415838facf804c461"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Chapter 3: Text Analysis"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "-- *A Python Course for the Humanities by Folgert Karsdorp and Maarten van Gompel*"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In this chapter we will introduce you to the task of text analysis in Python. You will learn how to read an entire corpus into Python, clean it and how to perform certain data analyses on those texts. We will also briefly introduce you to using Python's plotting library *matplotlib*, with which you can visualize your data.\n",
      "\n",
      "Before we delve into the main subject of this chapter, text analysis, we will first write a couple of utility functions that build upon the things you learnt in the previous chapter. Often we don't work with a single text file stored at our computer, but with multiple text files or entire corpora. We would like to have a way to load a corpus into Python.\n",
      "\n",
      "Remember how to read files? Each time we had to open a file, read the contents and then close the file. Since this is a series of steps we will often need to do, we can write a single function that does all that for us. We write a small utility function `read_file(filename)` that reads the specified file and simply returns all contents as a single string."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def read_file(filename):\n",
      "    \"Read the contents of FILENAME and return as a string.\"\n",
      "    infile = open(filename) # windows users should add the encoding='utf-8' option\n",
      "    contents = infile.read()\n",
      "    infile.close()\n",
      "    return contents"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now, instead of having to open a file, read the contents and close the file, we can just call the function `read_file` to do all that:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "text = read_file(\"data/romans_1:14-23_gk.txt\")\n",
      "print(text)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \u1f1d\u03bb\u03bb\u03b7\u03c3\u03af\u03bd \u03c4\u03b5 \u03ba\u03b1\u1f76 \u03b2\u03b1\u03c1\u03b2\u03ac\u03c1\u03bf\u03b9\u03c2, \u03c3\u03bf\u03c6\u03bf\u1fd6\u03c2 \u03c4\u03b5 \u03ba\u03b1\u1f76 \u1f00\u03bd\u03bf\u03ae\u03c4\u03bf\u03b9\u03c2 \u1f40\u03c6\u03b5\u03b9\u03bb\u03ad\u03c4\u03b7\u03c2 \u03b5\u1f30\u03bc\u03af\u00b7 \u03bf\u1f55\u03c4\u03c9\u03c2 \u03c4\u1f78 \u03ba\u03b1\u03c4\u2019 \u1f10\u03bc\u1f72 \u03c0\u03c1\u03cc\u03b8\u03c5\u03bc\u03bf\u03bd \u03ba\u03b1\u1f76 \u1f51\u03bc\u1fd6\u03bd \u03c4\u03bf\u1fd6\u03c2 \u1f10\u03bd \u1fec\u03ce\u03bc\u1fc3 \u03b5\u1f50\u03b1\u03b3\u03b3\u03b5\u03bb\u03af\u03c3\u03b1\u03c3\u03b8\u03b1\u03b9. \u039f\u1f50 \u03b3\u1f70\u03c1 \u1f10\u03c0\u03b1\u03b9\u03c3\u03c7\u03cd\u03bd\u03bf\u03bc\u03b1\u03b9 \u03c4\u1f78 \u2e00\u03b5\u1f50\u03b1\u03b3\u03b3\u03ad\u03bb\u03b9\u03bf\u03bd, \u03b4\u03cd\u03bd\u03b1\u03bc\u03b9\u03c2 \u03b3\u1f70\u03c1 \u03b8\u03b5\u03bf\u1fe6 \u1f10\u03c3\u03c4\u03b9\u03bd \u03b5\u1f30\u03c2 \u03c3\u03c9\u03c4\u03b7\u03c1\u03af\u03b1\u03bd \u03c0\u03b1\u03bd\u03c4\u1f76 \u03c4\u1ff7 \u03c0\u03b9\u03c3\u03c4\u03b5\u03cd\u03bf\u03bd\u03c4\u03b9, \u1f38\u03bf\u03c5\u03b4\u03b1\u03af\u1ff3 \u03c4\u03b5 \u03c0\u03c1\u1ff6\u03c4\u03bf\u03bd \u03ba\u03b1\u1f76 \u1f1d\u03bb\u03bb\u03b7\u03bd\u03b9\u00b7 \u03b4\u03b9\u03ba\u03b1\u03b9\u03bf\u03c3\u03cd\u03bd\u03b7 \u03b3\u1f70\u03c1 \u03b8\u03b5\u03bf\u1fe6 \u1f10\u03bd \u03b1\u1f50\u03c4\u1ff7 \u1f00\u03c0\u03bf\u03ba\u03b1\u03bb\u03cd\u03c0\u03c4\u03b5\u03c4\u03b1\u03b9 \u1f10\u03ba \u03c0\u03af\u03c3\u03c4\u03b5\u03c9\u03c2 \u03b5\u1f30\u03c2 \u03c0\u03af\u03c3\u03c4\u03b9\u03bd, \u03ba\u03b1\u03b8\u1f7c\u03c2 \u03b3\u03ad\u03b3\u03c1\u03b1\u03c0\u03c4\u03b1\u03b9\u00b7 \u1f49 \u03b4\u1f72 \u03b4\u03af\u03ba\u03b1\u03b9\u03bf\u03c2 \u1f10\u03ba \u03c0\u03af\u03c3\u03c4\u03b5\u03c9\u03c2 \u03b6\u03ae\u03c3\u03b5\u03c4\u03b1\u03b9. \u1f08\u03c0\u03bf\u03ba\u03b1\u03bb\u03cd\u03c0\u03c4\u03b5\u03c4\u03b1\u03b9 \u03b3\u1f70\u03c1 \u1f40\u03c1\u03b3\u1f74 \u03b8\u03b5\u03bf\u1fe6 \u1f00\u03c0\u2019 \u03bf\u1f50\u03c1\u03b1\u03bd\u03bf\u1fe6 \u1f10\u03c0\u1f76 \u03c0\u1fb6\u03c3\u03b1\u03bd \u1f00\u03c3\u03ad\u03b2\u03b5\u03b9\u03b1\u03bd \u03ba\u03b1\u1f76 \u1f00\u03b4\u03b9\u03ba\u03af\u03b1\u03bd \u1f00\u03bd\u03b8\u03c1\u03ce\u03c0\u03c9\u03bd \u03c4\u1ff6\u03bd \u03c4\u1f74\u03bd \u1f00\u03bb\u03ae\u03b8\u03b5\u03b9\u03b1\u03bd \u1f10\u03bd \u1f00\u03b4\u03b9\u03ba\u03af\u1fb3 \u03ba\u03b1\u03c4\u03b5\u03c7\u03cc\u03bd\u03c4\u03c9\u03bd, \u03b4\u03b9\u03cc\u03c4\u03b9 \u03c4\u1f78 \u03b3\u03bd\u03c9\u03c3\u03c4\u1f78\u03bd \u03c4\u03bf\u1fe6 \u03b8\u03b5\u03bf\u1fe6 \u03c6\u03b1\u03bd\u03b5\u03c1\u03cc\u03bd \u1f10\u03c3\u03c4\u03b9\u03bd \u1f10\u03bd \u03b1\u1f50\u03c4\u03bf\u1fd6\u03c2, \u1f41 \u2e02\u03b8\u03b5\u1f78\u03c2 \u03b3\u1f70\u03c1\u2e03 \u03b1\u1f50\u03c4\u03bf\u1fd6\u03c2 \u1f10\u03c6\u03b1\u03bd\u03ad\u03c1\u03c9\u03c3\u03b5\u03bd. \u03c4\u1f70 \u03b3\u1f70\u03c1 \u1f00\u03cc\u03c1\u03b1\u03c4\u03b1 \u03b1\u1f50\u03c4\u03bf\u1fe6 \u1f00\u03c0\u1f78 \u03ba\u03c4\u03af\u03c3\u03b5\u03c9\u03c2 \u03ba\u03cc\u03c3\u03bc\u03bf\u03c5 \u03c4\u03bf\u1fd6\u03c2 \u03c0\u03bf\u03b9\u03ae\u03bc\u03b1\u03c3\u03b9\u03bd \u03bd\u03bf\u03bf\u03cd\u03bc\u03b5\u03bd\u03b1 \u03ba\u03b1\u03b8\u03bf\u03c1\u1fb6\u03c4\u03b1\u03b9, \u1f25 \u03c4\u03b5 \u1f00\u0390\u03b4\u03b9\u03bf\u03c2 \u03b1\u1f50\u03c4\u03bf\u1fe6 \u03b4\u03cd\u03bd\u03b1\u03bc\u03b9\u03c2 \u03ba\u03b1\u1f76 \u03b8\u03b5\u03b9\u03cc\u03c4\u03b7\u03c2, \u03b5\u1f30\u03c2 \u03c4\u1f78 \u03b5\u1f36\u03bd\u03b1\u03b9 \u03b1\u1f50\u03c4\u03bf\u1f7a\u03c2 \u1f00\u03bd\u03b1\u03c0\u03bf\u03bb\u03bf\u03b3\u03ae\u03c4\u03bf\u03c5\u03c2, \u03b4\u03b9\u03cc\u03c4\u03b9 \u03b3\u03bd\u03cc\u03bd\u03c4\u03b5\u03c2 \u03c4\u1f78\u03bd \u03b8\u03b5\u1f78\u03bd \u03bf\u1f50\u03c7 \u1f61\u03c2 \u03b8\u03b5\u1f78\u03bd \u1f10\u03b4\u03cc\u03be\u03b1\u03c3\u03b1\u03bd \u1f22 \u03b7\u1f50\u03c7\u03b1\u03c1\u03af\u03c3\u03c4\u03b7\u03c3\u03b1\u03bd, \u1f00\u03bb\u03bb\u1f70 \u1f10\u03bc\u03b1\u03c4\u03b1\u03b9\u03ce\u03b8\u03b7\u03c3\u03b1\u03bd \u1f10\u03bd \u03c4\u03bf\u1fd6\u03c2 \u03b4\u03b9\u03b1\u03bb\u03bf\u03b3\u03b9\u03c3\u03bc\u03bf\u1fd6\u03c2 \u03b1\u1f50\u03c4\u1ff6\u03bd \u03ba\u03b1\u1f76 \u1f10\u03c3\u03ba\u03bf\u03c4\u03af\u03c3\u03b8\u03b7 \u1f21 \u1f00\u03c3\u03cd\u03bd\u03b5\u03c4\u03bf\u03c2 \u03b1\u1f50\u03c4\u1ff6\u03bd \u03ba\u03b1\u03c1\u03b4\u03af\u03b1\u00b7 \u03c6\u03ac\u03c3\u03ba\u03bf\u03bd\u03c4\u03b5\u03c2 \u03b5\u1f36\u03bd\u03b1\u03b9 \u03c3\u03bf\u03c6\u03bf\u1f76 \u1f10\u03bc\u03c9\u03c1\u03ac\u03bd\u03b8\u03b7\u03c3\u03b1\u03bd, \u03ba\u03b1\u1f76 \u1f24\u03bb\u03bb\u03b1\u03be\u03b1\u03bd \u03c4\u1f74\u03bd \u03b4\u03cc\u03be\u03b1\u03bd \u03c4\u03bf\u1fe6 \u1f00\u03c6\u03b8\u03ac\u03c1\u03c4\u03bf\u03c5 \u03b8\u03b5\u03bf\u1fe6 \u1f10\u03bd \u1f41\u03bc\u03bf\u03b9\u03ce\u03bc\u03b1\u03c4\u03b9 \u03b5\u1f30\u03ba\u03cc\u03bd\u03bf\u03c2 \u03c6\u03b8\u03b1\u03c1\u03c4\u03bf\u1fe6 \u1f00\u03bd\u03b8\u03c1\u03ce\u03c0\u03bf\u03c5 \u03ba\u03b1\u1f76 \u03c0\u03b5\u03c4\u03b5\u03b9\u03bd\u1ff6\u03bd \u03ba\u03b1\u1f76 \u03c4\u03b5\u03c4\u03c1\u03b1\u03c0\u03cc\u03b4\u03c9\u03bd \u03ba\u03b1\u1f76 \u1f11\u03c1\u03c0\u03b5\u03c4\u1ff6\u03bd.\n"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In the directory `data/NT/Raw` we have a corpus consisting of multiple files with the extension `.txt`. This corpus contains all the books of the New Testament in the raw text format from https://github.com/morphgnt/sblgnt. We want to iterate over all these files. You can do this using the `listdir` function from the `os` module. We import this function as follows:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from os import listdir"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "After that, the `listdir` function is available to use. This function takes as argument the path to a directory and returns all the files and subdirectories present in that directory:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "listdir(\"data\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 18,
       "text": [
        "['arabian_nights',\n",
        " 'austen-emma-excerpt-tokenised.txt',\n",
        " 'austen-emma-excerpt.txt',\n",
        " 'austen-emma.txt',\n",
        " 'british-novels',\n",
        " 'gutenberg',\n",
        " 'haggard',\n",
        " 'names',\n",
        " 'NT',\n",
        " 'romans_1:14-23_gk.txt',\n",
        " 'romans_1_gk.txt',\n",
        " 'romans_gk.txt',\n",
        " 'romans_greeting_gk.txt',\n",
        " 'romans_punc.txt',\n",
        " 'twitter.txt']"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Notice that `listdir` returns a list and we can iterate over that list. Now, consider the following function:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def list_textfiles(directory):\n",
      "    \"Return a list of filenames ending in '.txt' in DIRECTORY.\"\n",
      "    textfiles = []\n",
      "    for filename in listdir(directory):\n",
      "        if filename.endswith(\".txt\"):\n",
      "            textfiles.append(directory + \"/\" + filename)\n",
      "    return textfiles"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 19
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The function `listdir` takes as argument the name of a directory and lists all filenames in that directory. We iterate over this list and append each filename that ends with the extension, `.txt` to a new list of `textfiles`. Using the `list_textfiles` function, the following code will read all text files in the directory `data/NT/Raw` and outputs the length (in characters) of each:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for filepath in list_textfiles(\"data/NT/Raw\"):\n",
      "    text = read_file(filepath)\n",
      "    print(filepath +  \" has \" + str(len(text)) + \" characters.\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "data/NT/Raw/61-Mt-morphgnt.txt has 776473 characters.\n",
        "data/NT/Raw/62-Mk-morphgnt.txt has 483361 characters.\n",
        "data/NT/Raw/63-Lk-morphgnt.txt has 826160 characters."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "data/NT/Raw/64-Jn-morphgnt.txt has 634936 characters.\n",
        "data/NT/Raw/65-Ac-morphgnt.txt has 798556 characters.\n",
        "data/NT/Raw/66-Ro-morphgnt.txt has 296673 characters.\n",
        "data/NT/Raw/67-1Co-morphgnt.txt has 285775 characters."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "data/NT/Raw/68-2Co-morphgnt.txt has 190250 characters.\n",
        "data/NT/Raw/69-Ga-morphgnt.txt has 94866 characters.\n",
        "data/NT/Raw/70-Eph-morphgnt.txt has 102302 characters.\n",
        "data/NT/Raw/71-Php-morphgnt.txt has 68870 characters.\n",
        "data/NT/Raw/72-Col-morphgnt.txt has 67096 characters.\n",
        "data/NT/Raw/73-1Th-morphgnt.txt has 62673 characters.\n",
        "data/NT/Raw/74-2Th-morphgnt.txt has 34673 characters.\n",
        "data/NT/Raw/75-1Ti-morphgnt.txt has 71476 characters."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "data/NT/Raw/76-2Ti-morphgnt.txt has 54223 characters."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "data/NT/Raw/77-Tit-morphgnt.txt has 29831 characters.\n",
        "data/NT/Raw/78-Phm-morphgnt.txt has 13877 characters.\n",
        "data/NT/Raw/79-Heb-morphgnt.txt has 216463 characters.\n",
        "data/NT/Raw/80-Jas-morphgnt.txt has 74787 characters.\n",
        "data/NT/Raw/81-1Pe-morphgnt.txt has 73918 characters.\n",
        "data/NT/Raw/82-2Pe-morphgnt.txt has 48948 characters.\n",
        "data/NT/Raw/83-1Jn-morphgnt.txt has 86316 characters.\n",
        "data/NT/Raw/84-2Jn-morphgnt.txt has 10084 characters.\n",
        "data/NT/Raw/85-3Jn-morphgnt.txt has 9409 characters.\n",
        "data/NT/Raw/86-Jud-morphgnt.txt has 20569 characters.\n",
        "data/NT/Raw/87-Re-morphgnt.txt has 406512 characters."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Sentence tokenization"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In the previous chapter we wrote a function to tokenize or split a text string into a list of words. However, using this function we lose information about where sentences end and start in the text. We will develop a function `split_sentences` that performs some very simple sentence splitting when passed a text string. Each sentence will be represented as a new string, so the function as a whole returns a list of sentence strings. We assume that any occurrence of either `.` or `!` or `?` marks the end of a sentence. In reality, this is more ambiguous of course. Consider for example the use of periods as end-of-sentence marker as well as in abbreviations and initials!\n",
      "\n",
      "How should we tackle this problem? Have a look at the following picture:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "![caption](files/images/indexing.png)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The first sentence *Hello there!* spans from index 0 to index 11. The second sentence from 13 to 26. If we come up with a way to extract those indexes, we could slice the text into separate sentences. First we define a utility function `end_of_sentence` that takes as argument a character and returns `True` if it is an end-of-sentence marker, otherwise it returns `False`. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Quiz!"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Write the function `end_of_sentence_marker` below:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def end_of_sentence_marker(character):\n",
      "    # insert your code here\n",
      "\n",
      "# these tests should return True if your code is correct\n",
      "print(end_of_sentence_marker(\"?\") == True)\n",
      "print(end_of_sentence_marker(\"a\") == False)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "IndentationError",
       "evalue": "expected an indented block (<ipython-input-21-1d084bae50a6>, line 5)",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-21-1d084bae50a6>\"\u001b[1;36m, line \u001b[1;32m5\u001b[0m\n\u001b[1;33m    print(end_of_sentence_marker(\"?\") == True)\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block\n"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "An important function we will use is the built in `enumerate`. `enumerate` takes as argument any iterable (a string a list etc.). Let's see it in action:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for element in enumerate(\"Python\"):\n",
      "    print(element)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As you can see, enumerate allows you to iterate over an iterable and for each element in that iterable, it gives you its corresponding index. A slightly more convenient way of iterating over `enumerate` is the following:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for index, character in enumerate(\"Python\"):\n",
      "    print(index)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This way, we have easy access to both the index and the original item in the iterable. Now we know enough to write our `split_sentences` function. We will walk you through it, step by step, but first try to read the function and think about what it possibly does at each step:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def split_sentences(text):\n",
      "    \"Split a text string into a list of sentences.\"\n",
      "    sentences = []\n",
      "    start = 0\n",
      "    for end, character in enumerate(text):\n",
      "        if end_of_sentence_marker(character):\n",
      "            sentence = text[start: end + 1]\n",
      "            sentences.append(sentence)\n",
      "            start = end + 1\n",
      "    return sentences"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The function `split_sentences` takes as argument a text represented by a simple string. Within the function we define a variable `sentences` in which we will store the individual sentences. We need to extract both the start position and the end position of each sentence. We know that the first sentence will always start at position 0. Therefore we define a variable start and set it to zero.\n",
      "\n",
      "Next we will use `enumerate` to loop over all individual characters in the text. Remember that enumerate returns pairs of indexes and their corresponding elements (here characters). For each character we check whether it is an end-of-sentence marker. If it is, the variable `end` marks the position in `text` where a sentence ends. We can now slice the text from the starting position to the end position and obtain our sentence. Notice that we add 1 to the end position. Why would that be? This is because, as you might remember from the first chapter, slices are non-inclusive, so `text[start:end]` would return the text starting at `start` and ending one position before `end`. Since we have reached the end of a sentence, we know that the next sentence will start at least one position later than our last end point. Therefore, we update the start variable to `end + 1`. Let's check whether our function works as promised:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(split_sentences(\"This is a sentence. Should we seperate it from this one?\"))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "It does! "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Quiz!"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To conclude this section, you will write a wrapper function `tokenize`, that takes as input a text represented by a string and tokenizes this string into sentences. After that, we clean each sentence, by lowercasing all words and removing punctuation. The final step is to tokenize each sentence into a list of words. The file `preprocessing.py` contains a function called `clean_gk` which removes all punctuation from a Greek text and turns all characters to lowercase. We import that function using the following line:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from pyhum.preprocessing import clean_gk"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "First, you will need to rewrite your `end_of_sentence_marker` function to recognize the two end-of-sentence markers in our Greek New Testament text: `.` and `;`."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def gk_end_of_sentence_marker(character):\n",
      "    # insert your code here\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def tokenize(text):\n",
      "    \"\"\"Transform TEXT into a list of sentences. Lowercase \n",
      "    each sentence and remove all punctuation. Finally split each\n",
      "    sentence into a list of words.\"\"\"\n",
      "    # insert your code here\n",
      "\n",
      "# these tests should return True if your code is correct\n",
      "print(tokenize(\"\u03a4\u03af \u03b4\u1f72 \u1f51\u03bc\u1fd6\u03bd \u03b4\u03bf\u03ba\u03b5\u1fd6; \u1f04\u03bd\u03b8\u03c1\u03c9\u03c0\u03bf\u03c2 \u03b5\u1f36\u03c7\u03b5\u03bd \u03c4\u03ad\u03ba\u03bd\u03b1 \u03b4\u03cd\u03bf.\") == \n",
      "      [['\u03c4\u03af', '\u03b4\u1f72', '\u1f51\u03bc\u1fd6\u03bd', '\u03b4\u03bf\u03ba\u03b5\u1fd6'], ['\u1f04\u03bd\u03b8\u03c1\u03c9\u03c0\u03bf\u03c2', '\u03b5\u1f36\u03c7\u03b5\u03bd', '\u03c4\u03ad\u03ba\u03bd\u03b1', '\u03b4\u03cd\u03bf']])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "General Text Statistics"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "> \u1f10\u03ac\u03bd \u03c4\u03b9\u03c2 \u1f10\u03c0\u03b9\u03b8\u1fc7 \u1f10\u03c0\u2019 \u03b1\u1f50\u03c4\u03ac, \u1f10\u03c0\u03b9\u03b8\u03ae\u03c3\u03b5\u03b9 \u1f41 \u03b8\u03b5\u1f78\u03c2 \u1f10\u03c0\u2019 \u03b1\u1f50\u03c4\u1f78\u03bd \u03c4\u1f70\u03c2 \u03c0\u03bb\u03b7\u03b3\u1f70\u03c2 \u03c4\u1f70\u03c2 \u03b3\u03b5\u03b3\u03c1\u03b1\u03bc\u03bc\u03ad\u03bd\u03b1\u03c2 \u1f10\u03bd \u03c4\u1ff7 \u03b2\u03b9\u03b2\u03bb\u03af\u1ff3 \u03c4\u03bf\u03cd\u03c4\u1ff3\u00b7"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "One of the great advantages of being a student of the Bible in the digital age is the great amount of information that is available, one might even say that has been _added_, to the plain text of the Bible.  Besides the innumerable amateur, professional, and semi-professional commentaries on words, passages, and whole books, there are also many very useful sites that contain more information about the very words themselves, whether this information is lexical, syntactic, or textual.  Besides well-known sites such as <a href='http://www.perseus.tufts.edu/hopper/'>Perseus</a> or the German Bible Society's <a href='http://www.academic-bible.com/en/home/'>Academic Bible Portal</a>, there are also older or lesser known projects, like the <a href='http://ccat.sas.upenn.edu/rak/catss.html'>Computer Assisted Tools for Septuagint/Scriptural Study</a>.\n",
      "\n",
      "The texts that we are going to work with for the rest of this lesson come from a very current project to analyze every word in the New Testament, the <a href='http://morphgnt.org/'>MorphGNT</a> project, which provides \"linguistic databases and Python tools for the Greek New Testament.\"  (If you're interested in the Hebrew Bible, check out <a href='https://github.com/openscriptures/morphhb'>MorphHB</a>.)  If we take a look at one line of one of their files, we will see what a wealth of information they have.\n",
      "\n",
      "> 010405 V- 3AAI-S-- \u2e00\u1f14\u03c3\u03c4\u03b7\u03c3\u03b5\u03bd \u1f14\u03c3\u03c4\u03b7\u03c3\u03b5\u03bd \u1f14\u03c3\u03c4\u03b7\u03c3\u03b5(\u03bd) \u1f35\u03c3\u03c4\u03b7\u03bc\u03b9\n",
      "\n",
      "As you can see, each line is divided by spaces into 7 parts separated.  The first part, the six digits, consists of three 2-digit groups that tell us the book, chapter and verse in which this word occurs, in this case Matthew (book 01) 4:5.  The second part represents the part of speech, here a verb.  The next part is an 8-part parsing code.  This one tells us that the verb is a `3`rd person `A`orist `A`ctive `I`ndicative `S`ingular verb form.  The final four parts represent the text of the word in different states: 1) exactly as it appears in the text, 2) without punctuation, 3) normalized, and 4) lemma.  And every word in the New Testament is represented by such a line.  Perhaps you can imagine some of the possibilities!"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Quiz!"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We want to recreate the biblical text of the SBLGNT, the text used by MorphGNT.  We could use the `read_file` function above but, as we just saw, the MorphGNT files are not set up as a running text but, instead, as lines that each represent a single word.  So the first thing we should do is to rewrite this function to split a text file into lines instead of reading it in as one string, as we did above.  Take a look at our new function below."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def read_file_lines(filename):\n",
      "    \"Read the contents of FILENAME and return as a string.\"\n",
      "    infile = open(filename) # windows users should add the encoding='utf-8' option\n",
      "    contents = infile.read()\n",
      "    infile.close()\n",
      "    contents = contents.split('\\n') # \\n is the line separator\n",
      "    return contents"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The only difference between the two functions (besides the name) is that we have added the line\n",
      "\n",
      "    contents = contents.split('\\n')\n",
      "    \n",
      "If we pass the string `'\\n'` to the `split` method, it will split the string on the line separator '\\n' and return a list where each member is a separate line in the original file."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lines = read_file_lines('data/NT/Raw/85-3Jn-morphgnt.txt')\n",
      "lines[:5]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Our list `lines` now has the information for every word in 3 John separated into one line per word.  Your task is to write code below that will loop through each line of the `lines` list, extract the word as it appears in the text from each line, and add that word to a string object (called `Third_Jn`) that will then be the whole text of 3 John as it appears in the SLBGNT."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Third_Jn = ''\n",
      "# insert your code here"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We now have the whole book 3 John in the string variable `Third_Jn`.  Let's see how it looks."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(Third_Jn)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "How does it look?  If you see any problems, go back to your code and correct them.  Now that we have the text of the book, it would be a shame to lose it.  We could recreate it easily, but we could just save it to a text file and wouldn't need to.  Plus, saving it will allow us to use the .txt file with other programs on our system.\n",
      "\n",
      "But before we just blunder into giving the file any name we want, let's take a look at how the files from MorphGNT are named."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "list_textfiles(\"data/NT/Raw\")[:10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 24,
       "text": [
        "['data/NT/Raw/61-Mt-morphgnt.txt',\n",
        " 'data/NT/Raw/62-Mk-morphgnt.txt',\n",
        " 'data/NT/Raw/63-Lk-morphgnt.txt',\n",
        " 'data/NT/Raw/64-Jn-morphgnt.txt',\n",
        " 'data/NT/Raw/65-Ac-morphgnt.txt',\n",
        " 'data/NT/Raw/66-Ro-morphgnt.txt',\n",
        " 'data/NT/Raw/67-1Co-morphgnt.txt',\n",
        " 'data/NT/Raw/68-2Co-morphgnt.txt',\n",
        " 'data/NT/Raw/69-Ga-morphgnt.txt',\n",
        " 'data/NT/Raw/70-Eph-morphgnt.txt']"
       ]
      }
     ],
     "prompt_number": 24
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As you can see, the files are sorted in the order in which they appear in the NT by the number that begins their file name (61, 62, 63, etc.).  It would be a shame to lose this information, so we would like to keep this number at the beginning of all of the files.  We could simply load each file and give each one its own name.  But we are learning to program to save ourselves the trouble of having to do such things.  So what we will do below is build a series of functions that will allow us to load each file, read its text into a string, and then save the string with a filename appropriate for that book of the Bible."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Quiz!"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**1)** We want to build our `save as` filenames using the original filenames as a guide.  To do this, the first thing we need to do is to remove the file extension from the filename, in our case `.txt`.  Write a function `remove_ext` that takes as argument a string. It should return the string without the file extension. Tip: use the function `splitext` from the `os.path` module. Look up the documentation [here](https://docs.python.org/3.4/library/os.path.html#os.path.splitext)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from os.path import splitext\n",
      "\n",
      "def remove_ext(filename):\n",
      "    # insert your code here\n",
      "    \n",
      "# these tests should return True if your code is correct\n",
      "print(remove_ext(\"data/NT/Raw/61-Mt-morphgnt.txt\") == \"data/NT/Raw/61-Mt-morphgnt\")\n",
      "print(remove_ext(\"ridiculous_selfie.jpg\") == \"ridiculous_selfie\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**2)** Next, we need to remove the directory from the front of the filename.  Write a function `remove_dir` that takes as argument a filepath and removes the directory from a filepath. Tip: use the function `basename` from the `os.path` module. Look up the document [here](http://docs.python.org/3.4/library/os.path.html#os.path.basename)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from os.path import basename\n",
      "\n",
      "def remove_dir(filepath):\n",
      "    # insert your code here\n",
      "    \n",
      "# these tests should return True if your code is correct\n",
      "print(remove_dir(\"data/NT/Raw/61-Mt-morphgnt.txt\") == \"61-Mt-morphgnt.txt\")\n",
      "print(remove_dir(\"/a/kind/of/funny/filepath/to/file.txt\") == \"file.txt\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**3)** Combine the two functions `remove_ext` and `remove_dir` into one function `get_filename`. This function takes as argument a filepath and returns the name (without the extensions) of the file."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_filename(filepath):\n",
      "    # insert your code here\n",
      "    \n",
      "# these tests should return True if your code is correct\n",
      "print(get_filename(\"data/NT/Raw/61-Mt-morphgnt.txt\") == '61-Mt-morphgnt')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The final step is to strip the part of the filename that we don't want in our `save as` filename, i.e., the '-morphgnt' part.  Below we will present two different ways to do this.  \n",
      "\n",
      "The first way is useful if you want to strip the same sequence of characters from the beginning or the end of the filename, or any string.  It takes advantage of the `strip`, `lstrip`, and `rstrip` methods on strings.  Let's take a look at how they work.\n",
      "\n",
      "As the names suggest, they 'strip' something from a string, either on both sides (`strip`), the left side (`lstrip`), or the right side (`rstrip`).  Take a look."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "string = '...wo.rd...'\n",
      "print(string.strip('.'))\n",
      "print(string.lstrip('.'))\n",
      "print(string.rstrip('.'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "wo.rd\n",
        "wo.rd...\n",
        "...wo.rd\n"
       ]
      }
     ],
     "prompt_number": 26
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Notice that it strips __all__ occurrences of the string from the left and/or right side.  Below, right a short bit of code using `strip` that gives us what we want."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "name = '61-Mt-morphgnt'\n",
      "new_name = #insert your code here\n",
      "\n",
      "print(new_name == '61-Mt')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "That method is simple and fairly straightforward, but it only works if your filename begins and ends with the same characters that you want to strip off.  The second method is a bit more generalizable.  It uses the `split` method that we have already seen.  Try to figure out below how to use `split` in the function below to give us what we want."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "name = '61-Mt-morphgnt'\n",
      "def build_filename(f_name):\n",
      "    #insert your code here\n",
      "\n",
      "print(build_filename(name) == '61-Mt.txt')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Not quite as easy but, trust me, knowing how to split and reconstitute filenames, and strings in general, will be of enormous use.  And now we have a function that will do it for us."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Quiz!"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now, finally, combine the functions `get_filename` and `build_filename` into the function `saveas_filename` to obtain the integer corresponding to a night."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def saveas_filename(filepath):\n",
      "    # insert your code here\n",
      "\n",
      "# these tests should return True if your code is correct\n",
      "print(saveas_filename(\"data/NT/Raw/61-Mt-morphgnt.txt\") == \"61-Mt.txt\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "OK, so we now have the tools we need read the text of every NT book into a file of its own.  The problem is that we are losing a lot of information when we do this.  What we want to do below is to construct a function that will capture some of this information for us: the chapter and verse numbers for the text.  And while we could simply insert these into the text at the correct place, instead we will use a dictionary to organize this information in a way that is easily retrievable later.\n",
      "\n",
      "We learned about dictionaries in Chapter 1.  Remember dictionaries are made up of key:value pairs that allow us to look up the values very easily: `dict(key)` will return the value of that key in the dictionary.  But the `values` in a dictionary do not have to be strings, integers, etc.  They can also be other Python objects, like lists or even other dictionaries!  Take a look."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "d = {}\n",
      "d['list'] = [1, 2, 3, 4, 5]\n",
      "d['dict'] = {'\u1f41': 'the', '\u03b8\u03b5\u03cc\u03c2': 'God'}\n",
      "d['Mt'] = {1: {1: ['\u0392\u03af\u03b2\u03bb\u03bf\u03c2', '\u03b3\u03b5\u03bd\u03ad\u03c3\u03b5\u03c9\u03c2', '\u1f38\u03b7\u03c3\u03bf\u1fe6', '\u03c7\u03c1\u03b9\u03c3\u03c4\u03bf\u1fe6', '\u03c5\u1f31\u03bf\u1fe6', '\u0394\u03b1\u03c5\u1f76\u03b4', '\u03c5\u1f31\u03bf\u1fe6', '\u1f08\u03b2\u03c1\u03b1\u03ac\u03bc']}}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 29
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Do you see what is going on with `d['Matthew']`?  This is what we want to end up with.  And look what we can do with it then."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(d['Mt'][1][1]) #print out the words in Matthew chapter 1, verse 1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['\u0392\u03af\u03b2\u03bb\u03bf\u03c2', '\u03b3\u03b5\u03bd\u03ad\u03c3\u03b5\u03c9\u03c2', '\u1f38\u03b7\u03c3\u03bf\u1fe6', '\u03c7\u03c1\u03b9\u03c3\u03c4\u03bf\u1fe6', '\u03c5\u1f31\u03bf\u1fe6', '\u0394\u03b1\u03c5\u1f76\u03b4', '\u03c5\u1f31\u03bf\u1fe6', '\u1f08\u03b2\u03c1\u03b1\u03ac\u03bc']\n"
       ]
      }
     ],
     "prompt_number": 30
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This may appear to be a difficult task at first, but if we break it down into its component steps, we should be able to accomplish it without too much trouble.\n",
      "\n",
      "The best way to go about it is to build the dictionary from the inside out.  That is, first we will build the list of words in each verse, then put together each verse for each chapter, and then, finally, put together all the chapters in the book.  First, let's take another look at what the lines look like in the original files.\n",
      "\n",
      "> 010405 V- 3AAI-S-- \u2e00\u1f14\u03c3\u03c4\u03b7\u03c3\u03b5\u03bd \u1f14\u03c3\u03c4\u03b7\u03c3\u03b5\u03bd \u1f14\u03c3\u03c4\u03b7\u03c3\u03b5(\u03bd) \u1f35\u03c3\u03c4\u03b7\u03bc\u03b9\n",
      "\n",
      "Our first task will be to write a function that takes such a line as input and extracts the elements that we want, in this case the first element that representst the book, chapter, and verse for each word, and fourth element, which represents the word as it is in the text."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def bcv_word_extract(line):\n",
      "    #insert your code here\n",
      "    \n",
      "    return bcv, word\n",
      "\n",
      "print(bcv_word_extract('010405 V- 3AAI-S-- \u2e00\u1f14\u03c3\u03c4\u03b7\u03c3\u03b5\u03bd \u1f14\u03c3\u03c4\u03b7\u03c3\u03b5\u03bd \u1f14\u03c3\u03c4\u03b7\u03c3\u03b5(\u03bd) \u1f35\u03c3\u03c4\u03b7\u03bc\u03b9') == ('010405', '\u2e00\u1f14\u03c3\u03c4\u03b7\u03c3\u03b5\u03bd'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we need to "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Our next task is to write a function that takes as input this first element in every line and returns the chapter and verse numbers for each word.  Remember the first two digits represent the book, the second two the chapter, and the third two the verse.\n",
      "\n",
      "N.B.: Make sure to return chapter and verse numbers as `int` objects, not `str`."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def chapter_verse(element):\n",
      "    #insert your code here\n",
      "    \n",
      "    return c, v\n",
      "\n",
      "print(chapter_verse('010101') == (1, 1))\n",
      "print(chapter_verse('012025') == (20, 25))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now, we need to write a piece of code to detect when the verse or chapter changes from one line to another.  One trick for doing this is demonstrated below."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "numbers = (1,1,1,1,2,2,2,3,3,3,4,4,4)\n",
      "old_number = 1\n",
      "for number in numbers:\n",
      "    if number != old_number:\n",
      "        print(number, ' != ', old_number)\n",
      "        old_number = number"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "2  !=  1\n",
        "3  !=  2\n",
        "4  !=  3\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Notice that the `if` statement is only executed when the new number changes from what it was before.  And if this is the case, then we need to change `old_number` so that it is the same as the new value of `number`."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def build_book(file):\n",
      "    lines = read_file_lines(file)\n",
      "    old_chap_num = 1\n",
      "    old_verse_num = 1\n",
      "    book_dict = {}\n",
      "    chap_dict = {}\n",
      "    words = []\n",
      "    for line in lines:\n",
      "        b_c_v, word = line.split()[0], line.split()[3]\n",
      "        chap_num, verse_num = int(b_c_v[2:4]), int(b_c_v[4:])\n",
      "        if verse_num != old_verse_num:\n",
      "            if chap_num != old_chap_num:\n",
      "                book_dict[old_chap_num] = chap_dict\n",
      "                chap_dict = {}\n",
      "                old_chap_num = chap_num\n",
      "                old_verse_num = verse_num\n",
      "            else:\n",
      "                chap_dict[old_verse_num] = words\n",
      "                words = []\n",
      "                old_verse_num = verse_num\n",
      "        words.append(word)\n",
      "    chap_dict[verse_num] = words\n",
      "    book_dict[chap_num] = chap_dict\n",
      "    return book_dict"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 27
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "d = build_book('data/NT/Raw/85-3Jn-morphgnt.txt')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 28
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "d"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 29,
       "text": [
        "{1: {1: ['\u1f49',\n",
        "   '\u03c0\u03c1\u03b5\u03c3\u03b2\u03cd\u03c4\u03b5\u03c1\u03bf\u03c2',\n",
        "   '\u0393\u03b1\u0390\u1ff3',\n",
        "   '\u03c4\u1ff7',\n",
        "   '\u1f00\u03b3\u03b1\u03c0\u03b7\u03c4\u1ff7,',\n",
        "   '\u1f43\u03bd',\n",
        "   '\u1f10\u03b3\u1f7c',\n",
        "   '\u1f00\u03b3\u03b1\u03c0\u1ff6',\n",
        "   '\u1f10\u03bd',\n",
        "   '\u1f00\u03bb\u03b7\u03b8\u03b5\u03af\u1fb3.'],\n",
        "  2: ['\u1f08\u03b3\u03b1\u03c0\u03b7\u03c4\u03ad,',\n",
        "   '\u03c0\u03b5\u03c1\u1f76',\n",
        "   '\u03c0\u03ac\u03bd\u03c4\u03c9\u03bd',\n",
        "   '\u03b5\u1f54\u03c7\u03bf\u03bc\u03b1\u03af',\n",
        "   '\u03c3\u03b5',\n",
        "   '\u03b5\u1f50\u03bf\u03b4\u03bf\u1fe6\u03c3\u03b8\u03b1\u03b9',\n",
        "   '\u03ba\u03b1\u1f76',\n",
        "   '\u1f51\u03b3\u03b9\u03b1\u03af\u03bd\u03b5\u03b9\u03bd,',\n",
        "   '\u03ba\u03b1\u03b8\u1f7c\u03c2',\n",
        "   '\u03b5\u1f50\u03bf\u03b4\u03bf\u1fe6\u03c4\u03b1\u03af',\n",
        "   '\u03c3\u03bf\u03c5',\n",
        "   '\u1f21',\n",
        "   '\u03c8\u03c5\u03c7\u03ae.'],\n",
        "  3: ['\u1f10\u03c7\u03ac\u03c1\u03b7\u03bd',\n",
        "   '\u03b3\u1f70\u03c1',\n",
        "   '\u03bb\u03af\u03b1\u03bd',\n",
        "   '\u1f10\u03c1\u03c7\u03bf\u03bc\u03ad\u03bd\u03c9\u03bd',\n",
        "   '\u1f00\u03b4\u03b5\u03bb\u03c6\u1ff6\u03bd',\n",
        "   '\u03ba\u03b1\u1f76',\n",
        "   '\u03bc\u03b1\u03c1\u03c4\u03c5\u03c1\u03bf\u03cd\u03bd\u03c4\u03c9\u03bd',\n",
        "   '\u03c3\u03bf\u03c5',\n",
        "   '\u03c4\u1fc7',\n",
        "   '\u1f00\u03bb\u03b7\u03b8\u03b5\u03af\u1fb3,',\n",
        "   '\u03ba\u03b1\u03b8\u1f7c\u03c2',\n",
        "   '\u03c3\u1f7a',\n",
        "   '\u1f10\u03bd',\n",
        "   '\u1f00\u03bb\u03b7\u03b8\u03b5\u03af\u1fb3',\n",
        "   '\u03c0\u03b5\u03c1\u03b9\u03c0\u03b1\u03c4\u03b5\u1fd6\u03c2.'],\n",
        "  4: ['\u03bc\u03b5\u03b9\u03b6\u03bf\u03c4\u03ad\u03c1\u03b1\u03bd',\n",
        "   '\u03c4\u03bf\u03cd\u03c4\u03c9\u03bd',\n",
        "   '\u03bf\u1f50\u03ba',\n",
        "   '\u1f14\u03c7\u03c9',\n",
        "   '\u2e00\u03c7\u03b1\u03c1\u03ac\u03bd,',\n",
        "   '\u1f35\u03bd\u03b1',\n",
        "   '\u1f00\u03ba\u03bf\u03cd\u03c9',\n",
        "   '\u03c4\u1f70',\n",
        "   '\u1f10\u03bc\u1f70',\n",
        "   '\u03c4\u03ad\u03ba\u03bd\u03b1',\n",
        "   '\u1f10\u03bd',\n",
        "   '\u2e00\u03c4\u1fc7',\n",
        "   '\u1f00\u03bb\u03b7\u03b8\u03b5\u03af\u1fb3',\n",
        "   '\u03c0\u03b5\u03c1\u03b9\u03c0\u03b1\u03c4\u03bf\u1fe6\u03bd\u03c4\u03b1.'],\n",
        "  5: ['\u1f08\u03b3\u03b1\u03c0\u03b7\u03c4\u03ad,',\n",
        "   '\u03c0\u03b9\u03c3\u03c4\u1f78\u03bd',\n",
        "   '\u03c0\u03bf\u03b9\u03b5\u1fd6\u03c2',\n",
        "   '\u1f43',\n",
        "   '\u1f10\u1f70\u03bd',\n",
        "   '\u1f10\u03c1\u03b3\u03ac\u03c3\u1fc3',\n",
        "   '\u03b5\u1f30\u03c2',\n",
        "   '\u03c4\u03bf\u1f7a\u03c2',\n",
        "   '\u1f00\u03b4\u03b5\u03bb\u03c6\u03bf\u1f7a\u03c2',\n",
        "   '\u03ba\u03b1\u1f76',\n",
        "   '\u2e00\u03c4\u03bf\u1fe6\u03c4\u03bf',\n",
        "   '\u03be\u03ad\u03bd\u03bf\u03c5\u03c2,'],\n",
        "  6: ['\u03bf\u1f33',\n",
        "   '\u1f10\u03bc\u03b1\u03c1\u03c4\u03cd\u03c1\u03b7\u03c3\u03ac\u03bd',\n",
        "   '\u03c3\u03bf\u03c5',\n",
        "   '\u03c4\u1fc7',\n",
        "   '\u1f00\u03b3\u03ac\u03c0\u1fc3',\n",
        "   '\u1f10\u03bd\u03ce\u03c0\u03b9\u03bf\u03bd',\n",
        "   '\u1f10\u03ba\u03ba\u03bb\u03b7\u03c3\u03af\u03b1\u03c2,',\n",
        "   '\u03bf\u1f53\u03c2',\n",
        "   '\u03ba\u03b1\u03bb\u1ff6\u03c2',\n",
        "   '\u03c0\u03bf\u03b9\u03ae\u03c3\u03b5\u03b9\u03c2',\n",
        "   '\u03c0\u03c1\u03bf\u03c0\u03ad\u03bc\u03c8\u03b1\u03c2',\n",
        "   '\u1f00\u03be\u03af\u03c9\u03c2',\n",
        "   '\u03c4\u03bf\u1fe6',\n",
        "   '\u03b8\u03b5\u03bf\u1fe6\u00b7'],\n",
        "  7: ['\u1f51\u03c0\u1f72\u03c1',\n",
        "   '\u03b3\u1f70\u03c1',\n",
        "   '\u03c4\u03bf\u1fe6',\n",
        "   '\u1f40\u03bd\u03cc\u03bc\u03b1\u03c4\u03bf\u03c2',\n",
        "   '\u1f10\u03be\u1fc6\u03bb\u03b8\u03bf\u03bd',\n",
        "   '\u03bc\u03b7\u03b4\u1f72\u03bd',\n",
        "   '\u03bb\u03b1\u03bc\u03b2\u03ac\u03bd\u03bf\u03bd\u03c4\u03b5\u03c2',\n",
        "   '\u1f00\u03c0\u1f78',\n",
        "   '\u03c4\u1ff6\u03bd',\n",
        "   '\u2e00\u1f10\u03b8\u03bd\u03b9\u03ba\u1ff6\u03bd.'],\n",
        "  8: ['\u1f21\u03bc\u03b5\u1fd6\u03c2',\n",
        "   '\u03bf\u1f56\u03bd',\n",
        "   '\u1f40\u03c6\u03b5\u03af\u03bb\u03bf\u03bc\u03b5\u03bd',\n",
        "   '\u2e00\u1f51\u03c0\u03bf\u03bb\u03b1\u03bc\u03b2\u03ac\u03bd\u03b5\u03b9\u03bd',\n",
        "   '\u03c4\u03bf\u1f7a\u03c2',\n",
        "   '\u03c4\u03bf\u03b9\u03bf\u03cd\u03c4\u03bf\u03c5\u03c2,',\n",
        "   '\u1f35\u03bd\u03b1',\n",
        "   '\u03c3\u03c5\u03bd\u03b5\u03c1\u03b3\u03bf\u1f76',\n",
        "   '\u03b3\u03b9\u03bd\u03ce\u03bc\u03b5\u03b8\u03b1',\n",
        "   '\u03c4\u1fc7',\n",
        "   '\u1f00\u03bb\u03b7\u03b8\u03b5\u03af\u1fb3.'],\n",
        "  9: ['\u1f1c\u03b3\u03c1\u03b1\u03c8\u03ac',\n",
        "   '\u2e00\u03c4\u03b9',\n",
        "   '\u03c4\u1fc7',\n",
        "   '\u1f10\u03ba\u03ba\u03bb\u03b7\u03c3\u03af\u1fb3\u00b7',\n",
        "   '\u1f00\u03bb\u03bb\u2019',\n",
        "   '\u1f41',\n",
        "   '\u03c6\u03b9\u03bb\u03bf\u03c0\u03c1\u03c9\u03c4\u03b5\u03cd\u03c9\u03bd',\n",
        "   '\u03b1\u1f50\u03c4\u1ff6\u03bd',\n",
        "   '\u0394\u03b9\u03bf\u03c4\u03c1\u03ad\u03c6\u03b7\u03c2',\n",
        "   '\u03bf\u1f50\u03ba',\n",
        "   '\u1f10\u03c0\u03b9\u03b4\u03ad\u03c7\u03b5\u03c4\u03b1\u03b9',\n",
        "   '\u1f21\u03bc\u1fb6\u03c2.'],\n",
        "  10: ['\u03b4\u03b9\u1f70',\n",
        "   '\u03c4\u03bf\u1fe6\u03c4\u03bf,',\n",
        "   '\u1f10\u1f70\u03bd',\n",
        "   '\u1f14\u03bb\u03b8\u03c9,',\n",
        "   '\u1f51\u03c0\u03bf\u03bc\u03bd\u03ae\u03c3\u03c9',\n",
        "   '\u03b1\u1f50\u03c4\u03bf\u1fe6',\n",
        "   '\u03c4\u1f70',\n",
        "   '\u1f14\u03c1\u03b3\u03b1',\n",
        "   '\u1f03',\n",
        "   '\u03c0\u03bf\u03b9\u03b5\u1fd6,',\n",
        "   '\u03bb\u03cc\u03b3\u03bf\u03b9\u03c2',\n",
        "   '\u03c0\u03bf\u03bd\u03b7\u03c1\u03bf\u1fd6\u03c2',\n",
        "   '\u03c6\u03bb\u03c5\u03b1\u03c1\u1ff6\u03bd',\n",
        "   '\u1f21\u03bc\u1fb6\u03c2,',\n",
        "   '\u03ba\u03b1\u1f76',\n",
        "   '\u03bc\u1f74',\n",
        "   '\u1f00\u03c1\u03ba\u03bf\u03cd\u03bc\u03b5\u03bd\u03bf\u03c2',\n",
        "   '\u1f10\u03c0\u1f76',\n",
        "   '\u03c4\u03bf\u03cd\u03c4\u03bf\u03b9\u03c2',\n",
        "   '\u03bf\u1f54\u03c4\u03b5',\n",
        "   '\u03b1\u1f50\u03c4\u1f78\u03c2',\n",
        "   '\u1f10\u03c0\u03b9\u03b4\u03ad\u03c7\u03b5\u03c4\u03b1\u03b9',\n",
        "   '\u03c4\u03bf\u1f7a\u03c2',\n",
        "   '\u1f00\u03b4\u03b5\u03bb\u03c6\u03bf\u1f7a\u03c2',\n",
        "   '\u03ba\u03b1\u1f76',\n",
        "   '\u03c4\u03bf\u1f7a\u03c2',\n",
        "   '\u03b2\u03bf\u03c5\u03bb\u03bf\u03bc\u03ad\u03bd\u03bf\u03c5\u03c2',\n",
        "   '\u03ba\u03c9\u03bb\u03cd\u03b5\u03b9',\n",
        "   '\u03ba\u03b1\u1f76',\n",
        "   '\u1f10\u03ba',\n",
        "   '\u03c4\u1fc6\u03c2',\n",
        "   '\u1f10\u03ba\u03ba\u03bb\u03b7\u03c3\u03af\u03b1\u03c2',\n",
        "   '\u1f10\u03ba\u03b2\u03ac\u03bb\u03bb\u03b5\u03b9.'],\n",
        "  11: ['\u1f08\u03b3\u03b1\u03c0\u03b7\u03c4\u03ad,',\n",
        "   '\u03bc\u1f74',\n",
        "   '\u03bc\u03b9\u03bc\u03bf\u1fe6',\n",
        "   '\u03c4\u1f78',\n",
        "   '\u03ba\u03b1\u03ba\u1f78\u03bd',\n",
        "   '\u1f00\u03bb\u03bb\u1f70',\n",
        "   '\u03c4\u1f78',\n",
        "   '\u1f00\u03b3\u03b1\u03b8\u03cc\u03bd.',\n",
        "   '\u1f41',\n",
        "   '\u1f00\u03b3\u03b1\u03b8\u03bf\u03c0\u03bf\u03b9\u1ff6\u03bd',\n",
        "   '\u1f10\u03ba',\n",
        "   '\u03c4\u03bf\u1fe6',\n",
        "   '\u03b8\u03b5\u03bf\u1fe6',\n",
        "   '\u1f10\u03c3\u03c4\u03b9\u03bd\u00b7',\n",
        "   '\u1f41',\n",
        "   '\u03ba\u03b1\u03ba\u03bf\u03c0\u03bf\u03b9\u1ff6\u03bd',\n",
        "   '\u03bf\u1f50\u03c7',\n",
        "   '\u1f11\u03ce\u03c1\u03b1\u03ba\u03b5\u03bd',\n",
        "   '\u03c4\u1f78\u03bd',\n",
        "   '\u03b8\u03b5\u03cc\u03bd.'],\n",
        "  12: ['\u0394\u03b7\u03bc\u03b7\u03c4\u03c1\u03af\u1ff3',\n",
        "   '\u03bc\u03b5\u03bc\u03b1\u03c1\u03c4\u03cd\u03c1\u03b7\u03c4\u03b1\u03b9',\n",
        "   '\u1f51\u03c0\u1f78',\n",
        "   '\u03c0\u03ac\u03bd\u03c4\u03c9\u03bd',\n",
        "   '\u03ba\u03b1\u1f76',\n",
        "   '\u1f51\u03c0\u1f78',\n",
        "   '\u03b1\u1f50\u03c4\u1fc6\u03c2',\n",
        "   '\u03c4\u1fc6\u03c2',\n",
        "   '\u1f00\u03bb\u03b7\u03b8\u03b5\u03af\u03b1\u03c2\u00b7',\n",
        "   '\u03ba\u03b1\u1f76',\n",
        "   '\u1f21\u03bc\u03b5\u1fd6\u03c2',\n",
        "   '\u03b4\u1f72',\n",
        "   '\u03bc\u03b1\u03c1\u03c4\u03c5\u03c1\u03bf\u1fe6\u03bc\u03b5\u03bd,',\n",
        "   '\u03ba\u03b1\u1f76',\n",
        "   '\u2e00\u03bf\u1f36\u03b4\u03b1\u03c2',\n",
        "   '\u1f45\u03c4\u03b9',\n",
        "   '\u1f21',\n",
        "   '\u03bc\u03b1\u03c1\u03c4\u03c5\u03c1\u03af\u03b1',\n",
        "   '\u1f21\u03bc\u1ff6\u03bd',\n",
        "   '\u1f00\u03bb\u03b7\u03b8\u03ae\u03c2',\n",
        "   '\u1f10\u03c3\u03c4\u03b9\u03bd.'],\n",
        "  13: ['\u03a0\u03bf\u03bb\u03bb\u1f70',\n",
        "   '\u03b5\u1f36\u03c7\u03bf\u03bd',\n",
        "   '\u2e02\u03b3\u03c1\u03ac\u03c8\u03b1\u03b9',\n",
        "   '\u03c3\u03bf\u03b9\u2e03,',\n",
        "   '\u1f00\u03bb\u03bb\u2019',\n",
        "   '\u03bf\u1f50',\n",
        "   '\u03b8\u03ad\u03bb\u03c9',\n",
        "   '\u03b4\u03b9\u1f70',\n",
        "   '\u03bc\u03ad\u03bb\u03b1\u03bd\u03bf\u03c2',\n",
        "   '\u03ba\u03b1\u1f76',\n",
        "   '\u03ba\u03b1\u03bb\u03ac\u03bc\u03bf\u03c5',\n",
        "   '\u03c3\u03bf\u03b9',\n",
        "   '\u2e00\u03b3\u03c1\u03ac\u03c6\u03b5\u03b9\u03bd\u00b7'],\n",
        "  14: ['\u1f10\u03bb\u03c0\u03af\u03b6\u03c9',\n",
        "   '\u03b4\u1f72',\n",
        "   '\u03b5\u1f50\u03b8\u03ad\u03c9\u03c2',\n",
        "   '\u2e02\u03c3\u03b5',\n",
        "   '\u1f30\u03b4\u03b5\u1fd6\u03bd\u2e03,',\n",
        "   '\u03ba\u03b1\u1f76',\n",
        "   '\u03c3\u03c4\u03cc\u03bc\u03b1',\n",
        "   '\u03c0\u03c1\u1f78\u03c2',\n",
        "   '\u03c3\u03c4\u03cc\u03bc\u03b1',\n",
        "   '\u03bb\u03b1\u03bb\u03ae\u03c3\u03bf\u03bc\u03b5\u03bd.'],\n",
        "  15: ['\u0395\u1f30\u03c1\u03ae\u03bd\u03b7',\n",
        "   '\u03c3\u03bf\u03b9.',\n",
        "   '\u1f00\u03c3\u03c0\u03ac\u03b6\u03bf\u03bd\u03c4\u03b1\u03af',\n",
        "   '\u03c3\u03b5',\n",
        "   '\u03bf\u1f31',\n",
        "   '\u03c6\u03af\u03bb\u03bf\u03b9.',\n",
        "   '\u1f00\u03c3\u03c0\u03ac\u03b6\u03bf\u03c5',\n",
        "   '\u03c4\u03bf\u1f7a\u03c2',\n",
        "   '\u03c6\u03af\u03bb\u03bf\u03c5\u03c2',\n",
        "   '\u03ba\u03b1\u03c4\u2019',\n",
        "   '\u1f44\u03bd\u03bf\u03bc\u03b1.']}}"
       ]
      }
     ],
     "prompt_number": 29
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Exploratory data analysis"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As a first exploratory data analysis, we are going to compute for each night how many sentences it contains and how many words. It is quite easy to count the number of sentences per night, since each night is represented by a list of sentences."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sentences_per_night = []\n",
      "for night in corpus:\n",
      "    sentences_per_night.append(len(night))\n",
      "print(sentences_per_night[:10])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Using the function `max` we can find out what the highest number of sentences is:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "max(sentences_per_night)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Similarly, if we would like to now what the lowest number of sentences is, we use the function `min`:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "min(sentences_per_night)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Quiz!"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The function `sum` takes a list of numbers as input and returns the sum:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(sum([1, 3, 3, 4]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Use this function to compute the average number of sentences per night. Note if you use Python 2.7, you will need to convert the result of sum, which will be an integer to a `float`, using `float(some_number)`. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# if you use Python 3.x, both print statements will return \n",
      "# the same thing and you don't need to worry.\n",
      "number = 1\n",
      "print(number)\n",
      "number = float(number)\n",
      "print(number)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# insert your code here"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Given our data structure of a list of sentences which are themselves lists of words, it is a little trickier to count for each night how many words it contains. One possible way is the following:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "words_per_night = []\n",
      "for night in corpus:\n",
      "    n_words = 0\n",
      "    for sentence in night:\n",
      "        n_words += len(sentence)\n",
      "    words_per_night.append(n_words)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Make sure you really understand these lines of code as you will need them in the next quiz. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The suspense created by Shahrazad\u2019s story-telling skills is intriguing, especially the \u201ccliff-hanger\u201d ending each night which she uses to avert her own execution (and possibly that of womanhood). Every night she tells the Sultan a story only to stop at dawn and she picks up the thread the next night. But does it really take the whole night to tell a particular story?\n",
      "\n",
      "I am not aware of any exact numbers about how many words people speak per minute. Averages seem to fluctuate between 100 and 200 words per minute. Narrators are advised to use approximately 150 words per minute in audiobooks. I suspect that this number is a little lower for live storytelling and assume it lies around 130 words per minute (including pauses). Using this information, we can compute the time it takes to tell a particular story as follows:\n",
      "\n",
      "$$\\textrm{story time}(\\textrm{text}) = \\frac{\\textrm{number of words in text}}{\\textrm{number of words per minute}}$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Quiz!"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**1)** Write a function called `story_time` that takes as input a text. Given a speed of 130 words per minute, compute how long it takes to tell that text."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def story_time(text):\n",
      "    # insert your code here\n",
      "\n",
      "# these tests should return True if your code is correct\n",
      "print(story_time([[\"story\", \"story\"]]) * 130 == 2.0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**2)** Compute the story_time for each night in our corpus. Assign the result to the variable `story_time_per_night`."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "story_time_per_night = []\n",
      "# insert your code here\n",
      "print(story_time_per_night[:10])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**3**) Compute the average, minimum and maximum story telling time."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# insert your code here"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Visualizing general statistics"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now that we have computed a range of general statistics for our corpus, it would be nice to visualize them. Python's plotting library *matplotlib* (see [here](http://matplotlib.org)) allows us to produce all kinds of graphs. We could for example, plot for each story, how many sentences it contains:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import matplotlib.pyplot as plt\n",
      "\n",
      "plt.plot(sentences_per_night)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Quiz!"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**1)** Can you do the same for `words_per_night`?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# insert your code here"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**2)** And can you do the same for `story_time_per_night`?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# insert your code here"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**3)** In this final exercise we will put everything together what we have learnt so far. We want you to write a function `positions_of` that returns for a given word all sentence positions in the *Arabian Nights* where that word occurs. We are not interested in the positions relative to a particular night, but only to the corpus as a whole. Use that function to find all occurences of the name Sharahzad and store the corresponding indexes in the variable `positions_of_shahrazad`. Do the same thing for the name *Ali*. Store the result in `positions_of_ali`. Finally, find all occurences of *Egypt* and store the indexes in `positions_of_egypt`. Tip: (1) remember that we lowercased the entire corpus! (2) remember that indexes start at 0."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def positions_of(word):\n",
      "    #insert your code here\n",
      "\n",
      "positions_of_shahrazad = positions_of(\"shahrazad\")\n",
      "positions_of_ali = positions_of(\"ali\")\n",
      "positions_of_egypt = positions_of(\"egypt\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If everything went well, the following lines of code should produce a nice dispersion plot of all sentence occurences of Shahrazad, Ali and Egypt in the corpus."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.figure(figsize=(20, 8))\n",
      "names = [\"Shahrazad\", \"Ali\", \"Egypt\"]\n",
      "plt.plot(positions_of_shahrazad, [1]*len(positions_of_shahrazad), \"|\", markersize=100)\n",
      "plt.plot(positions_of_ali, [2]*len(positions_of_ali), \"|\", markersize=100)\n",
      "plt.plot(positions_of_egypt, [0]*len(positions_of_egypt), \"|\", markersize=100)\n",
      "plt.yticks(range(len(names)), names)\n",
      "_ = plt.ylim(-1, 3)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "> Then Shahrazad reached the morning, and fell silent in the telling of her tale\u2026"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Ignore the following, it's just here to make the page pretty:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from IPython.core.display import HTML\n",
      "def css_styling():\n",
      "    styles = open(\"styles/custom.css\", \"r\").read()\n",
      "    return HTML(styles)\n",
      "css_styling()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "/*\n",
        "Placeholder for custom user CSS\n",
        "\n",
        "mainly to be overridden in profile/static/custom/custom.css\n",
        "\n",
        "This will always be an empty file in IPython\n",
        "*/\n",
        "<style>\n",
        "    @import url(http://fonts.googleapis.com/css?family=Roboto:400,300,300italic,400italic,700,700italic);\n",
        "\n",
        "    div.cell{\n",
        "        font-family:'roboto','helvetica','sans';\n",
        "        color:#444;\n",
        "        width:800px;\n",
        "        margin-left:16% !important;\n",
        "        margin-right:auto;\n",
        "    }\n",
        "\n",
        "    div.text_cell_render{\n",
        "        font-family: 'roboto','helvetica','sans';\n",
        "        line-height: 145%;\n",
        "        font-size: 120%;\n",
        "        color:#444;\n",
        "        width:800px;\n",
        "        margin-left:auto;\n",
        "        margin-right:auto;\n",
        "    }\n",
        "    .CodeMirror{\n",
        "            font-family: \"Menlo\", source-code-pro,Consolas, monospace;\n",
        "    }\n",
        "    .prompt{\n",
        "        display: None;\n",
        "    }    \n",
        "    .warning{\n",
        "        color: rgb( 240, 20, 20 )\n",
        "        }  \n",
        "</style>\n",
        "<script>\n",
        "    MathJax.Hub.Config({\n",
        "                        TeX: {\n",
        "                           extensions: [\"AMSmath.js\"]\n",
        "                           },\n",
        "                tex2jax: {\n",
        "                    inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ],\n",
        "                    displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ]\n",
        "                },\n",
        "                displayAlign: 'center', // Change this to 'center' to center equations.\n",
        "                \"HTML-CSS\": {\n",
        "                    styles: {'.MathJax_Display': {\"margin\": 4}}\n",
        "                }\n",
        "        });\n",
        "</script>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 11,
       "text": [
        "<IPython.core.display.HTML at 0x7f92346c5550>"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p><small><a rel=\"license\" href=\"http://creativecommons.org/licenses/by-sa/4.0/\"><img alt=\"Creative Commons License\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by-sa/4.0/88x31.png\" /></a><br /><span xmlns:dct=\"http://purl.org/dc/terms/\" property=\"dct:title\">Python Programming for the Humanities</span> by <a xmlns:cc=\"http://creativecommons.org/ns#\" href=\"http://fbkarsdorp.github.io/python-course\" property=\"cc:attributionName\" rel=\"cc:attributionURL\">http://fbkarsdorp.github.io/python-course</a> is licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-sa/4.0/\">Creative Commons Attribution-ShareAlike 4.0 International License</a>. Based on a work at <a xmlns:dct=\"http://purl.org/dc/terms/\" href=\"https://github.com/fbkarsdorp/python-course\" rel=\"dct:source\">https://github.com/fbkarsdorp/python-course</a>.</small></p>"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}